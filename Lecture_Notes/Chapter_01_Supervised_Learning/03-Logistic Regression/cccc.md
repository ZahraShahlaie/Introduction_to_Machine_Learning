
**مقدمه‌ای بر رگرسیون لجستیک (Logistic Regression)**


**مدرس:** علی شریفی-زارچی
**دپارتمان:** مهندسی کامپیوتر
**دانشگاه:** صنعتی شریف
**تاریخ:** 5 اکتبر 2024
**درس:** یادگیری ماشین (CE 40717)

---
**رگرسیون لجستیک: مباحث اولیه و پیش‌پردازش داده**

در جلسه گذشته، مروری بر تکنیک‌های دسته‌بندی (classification) انجام شد و برای اولین بار با مسئله دسته‌بندی آشنا شدیم. [cite_start]برخلاف مسئله رگرسیون که هدف، پیش‌بینی یک مقدار پیوسته بود، در دسته‌بندی هدف پیش‌بینی مجموعه‌ای از کلاس‌ها است[cite: 49]. این کلاس‌ها می‌توانند حالات مختلفی داشته باشند، مانند بیمار/سالم، سگ/گربه، فروش رفتن/نرفتن خانه، خوش‌حساب/بدحساب بودن مشتری، پرداخت وام/عدم پرداخت وام، اسپم بودن/نبودن ایمیل، یا حاوی محتوای توهین‌آمیز بودن/نبودن یک متن، و بسیاری موارد دیگر.

در جلسات قبلی، در ساده‌ترین مدل‌های دسته‌بندی، برچسب کلاس‌ها را منفی یک و یک در نظر می‌گرفتیم. [cite_start]اما در این جلسه، به خاطر سادگی و دلایل ریاضیاتی مدل، برچسب کلاس‌ها از این به بعد 0 و 1 در نظر گرفته می‌شود[cite: 57, 58, 59]. این یک تفاوت اولیه است که باید به آن توجه داشت.

**پرسش محوری:**
در جلسات قبل، با دسته‌بند "پرسپترون" آشنا شدیم که ساختار شبکه عصبی داشت. اما این مدل اشکالی داشت: خروجی آن شبیه یک تابع پله‌ای بود که از یک نقطه خاص (مثلاً صفر) به بعد، کلاس را 1 و قبل از آن، کلاس را منفی 1 در نظر می‌گرفت. [cite_start]عملاً از علامت (sign) استفاده می‌شد[cite: 81, 82]. واقعیت این است که در بسیاری از مواقع، شما یک "درجه اطمینان" دارید. به عنوان مثال، برای یک بیمار، ممکن است 100% مطمئن باشید که سرطان دارد، یا 100% مطمئن باشید که سالم است. اما اغلب اوقات، مقادیری بین این دو حالت وجود دارد. مثلاً، نه می‌توانید 100% مطمئن باشید که فرد سرطان دارد و نه 100% مطمئن باشید که سالم است؛ شاید 70% احتمال می‌دهید سرطان داشته باشد یا 80% احتمال می‌دهید سالم باشد.

**سوال این است که چگونه می‌توانیم این موضوع را مدل‌سازی کنیم؟** چگونه الگوریتم خود را توسعه دهیم تا بتواند یک نوع "احتمال" (مانند احتمال بیمار بودن یا سالم بودن) را به دست آورد؟ این رویکرد، بسیار متفاوت از تابع پله‌ای است که در جلسه قبل دیدیم. ما به دنبال تابعی هستیم که خروجی آن بین 0 و 1 باشد، به گونه‌ای که اگر ورودی در یک ناحیه خاص قرار گیرد، خروجی به 0 نزدیک باشد (مثلاً 100% سالم)، و اگر در ناحیه دیگر باشد، خروجی به 1 نزدیک باشد (مثلاً 100% بیمار). اما در نقاط میانی، خروجی باید مقادیر بین 0 و 1 را بگیرد (مثلاً 0.4 یا 0.6).

![image1](https://github.com/ZahraShahlaie/Introduction_to_Machine_Learning/raw/main/Lecture_Notes/Chapter_01_Supervised_Learning/03-Logistic%20Regression/images/01.png)

**مشکل رگرسیون خطی برای مسائل دسته‌بندی:**
یک راه حل ساده این است که به سراغ رگرسیون خطی برویم، شبیه همان مدلی که قبلاً داشتیم. اگر داده‌هایی با برچسب 0 و 1 داشته باشیم، می‌توانیم یک خط به آنها برازش (fit) دهیم تا حداقل خطای مربعات را داشته باشد. اما مشکل اینجاست که اگر یک نقطه جدید (outlier) به داده‌ها اضافه شود، خط برازش یافته به شدت تغییر می‌کند. این تغییر می‌تواند باعث شود که بسیاری از نقاطی که قبلاً به درستی دسته‌بندی شده بودند، اکنون به اشتباه دسته‌بندی شوند (مثلاً همه نقاط را 1 یا 0 برچسب‌گذاری کند). این یک نمونه از اینکه چرا رگرسیون خطی برای مسائل دسته‌بندی مناسب نیست.
![image2](https://github.com/ZahraShahlaie/Introduction_to_Machine_Learning/raw/main/Lecture_Notes/Chapter_01_Supervised_Learning/03-Logistic%20Regression/images/02.png)


**رگرسیون لجستیک:**
الگوریتمی معرفی خواهد شد که یکی از پرکاربردترین الگوریتم‌های دسته‌بندی است. این الگوریتم، رگرسیون لجستیک (Logistic Regression) نام دارد.

**قالب‌بندی داده‌ها (Data Format):**
فرض کنید یک جدول داده (table) داریم که شامل تعدادی "ویژگی" (features) است. تلاش می‌کنیم همه این ویژگی‌ها را به مقادیر عددی تبدیل کنیم. به عنوان مثال، اگر ستونی برای جنسیت (male/female) داریم، می‌توانیم آن را به 0 و 1 تبدیل کنیم که کاملاً قراردادی است.

**پیش‌پردازش داده (Data Preprocessing):**
ویژگی‌های عددی ممکن است مقیاس‌های (scales) بسیار متفاوتی داشته باشند. مثلاً، یک ویژگی ممکن است بین 0 تا 1 باشد (مثل جنسیت)، در حالی که دیگری ممکن است بین 150 تا 230 باشد (مثل قد). این دامنه‌های متفاوت باعث می‌شوند که ضرایب (weights) مدل (مثلاً در یک معادله خط ساده) مقیاس‌های بسیار متفاوتی داشته باشند. بنابراین، "مقیاس‌بندی" (scaling) تک‌تک ویژگی‌ها مهم است.
* **استانداردسازی (Standardization):** یک تکنیک این است که میانگین هر ویژگی را صفر و واریانس آن را یک کنیم. این کار به "نرمال‌سازی" یا "استانداردسازی" داده‌ها معروف است.
* **مقیاس‌بندی Min-Max (Min-Max Scaling):** روش دیگر این است که حداقل مقدار هر ویژگی را به صفر و حداکثر آن را به یک برسانیم.

پیش‌پردازش داده‌ها یک مرحله بسیار مهم در یادگیری ماشین است که قبل از قرار دادن داده‌ها در مدل باید انجام شود. در این مرحله، کارهایی مانند حذف داده‌های پرت (outliers)، بصری‌سازی (visualization) داده‌ها، و بررسی توزیع ویژگی‌های مختلف انجام می‌شود تا از کیفیت داده‌ها اطمینان حاصل شود. ممکن است لازم باشد برخی از ردیف‌ها (samples) یا ستون‌های داده (features) را حذف کنید زیرا نتایج مدل را تحت تأثیر قرار می‌دهند. این کار به "پیش‌پردازش داده" معروف است و در عمل یادگیری ماشین، اهمیت زیادی دارد.

در نهایت، یک ستون "برچسب" (label) نیز وجود دارد، مثلاً "اضافه وزن" (overweight) بودن یا نبودن، که هدف ما پیش‌بینی آن از روی ویژگی‌ها است.


![image](https://github.com/user-attachments/assets/b2a2474e-2769-4857-9e6a-cca7561a04a3)




---

**مبانی رگرسیون لجستیک: تابع سیگموئید و ویژگی‌های آن**


**1. نیاز به تابع احتمالاتی:**
* در رگرسیون لجستیک، ما به دنبال یک "تابع احتمال" هستیم. این تابع به صورت اولیه "سیگما" نامیده می‌شود.
* وظیفه این تابع این است که مقادیر ورودی (که می‌توانند از منفی بی‌نهایت تا مثبت بی‌نهایت باشند) را به بازه بین صفر تا یک تبدیل کند.
* مقدار احتمال همیشه بین صفر و یک است، بنابراین به چنین تابعی نیاز داریم.
* اگر ضرایب $w$ را در بردار ویژگی $x$ (که شامل $x_0=1, x_1, \dots, x_d$ است) ضرب کنیم (یعنی $w^T x$)، نتیجه می‌تواند هر عددی بین منفی بی‌نهایت تا مثبت بی‌نهایت باشد.
* تابع "سیگما" این مقدار را به بازه صفر تا یک مقیاس‌بندی می‌کند.
* برخلاف تابع پله‌ای (که مشتق‌ناپذیر است و خروجی‌های چکشی 0 یا 1 می‌دهد)، تابع سیگما رفتاری نرم (smooth) و مشتق‌پذیر دارد و امکان نمایش "درجه اطمینان" را فراهم می‌کند.
* هدف این است که خروجی تابع، احتمال بیمار بودن یا سالم بودن را نشان دهد، نه فقط یک دسته‌بندی قطعی.
* $P(y=1|x, w) = \sigma(w^T x)$ نشان‌دهنده احتمال اینکه کلاس برابر با 1 باشد (مثلاً بیمار باشد) با توجه به ورودی $x$ و پارامترهای $w$ است.
* $P(y=0|x, w) = 1 - \sigma(w^T x)$ نشان‌دهنده احتمال اینکه کلاس برابر با 0 باشد (مثلاً سالم باشد).

  
---


**2. تابع سیگموئید (Sigmoid Function) یا تابع لجستیک (Logistic Function):**


   
   
   * این تابع، شکل مناسبی برای تابع احتمالاتی است و "تابع سیگموئید" یا "تابع لجستیک" نام دارد.
   * به همین دلیل، الگوریتم دسته‌بندی ما "رگرسیون لجستیک" نامیده می‌شود، با وجود اینکه یک الگوریتم دسته‌بندی است، نه رگرسیون.
   * فرمول تابع سیگموئید: $\sigma(z) = \frac{1}{1 + e^{-z}}$
   * 2.1. ویژگی‌های تابع سیگموئید:
   * اگر $z$ به سمت مثبت بی‌نهایت میل کند، $\sigma(z)$ به سمت 1 میل می‌کند.
   * اگر $z$ به سمت منفی بی‌نهایت میل کند، $\sigma(z)$ به سمت 0 میل می‌کند.
   * این رفتار تابع، مطابق با انتظارات ما برای یک تابع احتمال است.
   *  اگر $z = 0$ باشد، $\sigma(0) = 0.5$.
   *  خروجی آن همواره بین 0 و 1 است.
   * به صورت "هموار" (smoothly) بین 0 و 1 تغییر می‌کند.
   * "مشتق‌پذیر" (differentiable) است.

   

   ![image3](https://github.com/ZahraShahlaie/Introduction_to_Machine_Learning/raw/main/Lecture_Notes/Chapter_01_Supervised_Learning/03-Logistic%20Regression/images/03.png)

   * **مشتق تابع سیگموئید:**
      * مشتق $\sigma(z)$ نسبت به $z$， برابر است با $\sigma(z)(1 - \sigma(z))$.
       ![image](https://github.com/user-attachments/assets/714d9c2a-3e3a-42c6-b4ea-5d548bfeb7ab)

      * این ویژگی ریاضیاتی تابع سیگموئید، در محاسبات بعدی (مانند گرادیان) بسیار مفید خواهد بود و باعث سادگی محاسبات می‌شود.
      * نمودار مشتق سیگموئید نشان می‌دهد که مشتق در اطراف $z=0$ (که $\sigma(z)=0.5$ است) به حداکثر می‌رسد و در دورترین نقاط (نزدیک به 0 و 1) به 0 میل می‌کند.
    
      ![image4](https://github.com/ZahraShahlaie/Introduction_to_Machine_Learning/raw/main/Lecture_Notes/Chapter_01_Supervised_Learning/03-Logistic%20Regression/images/04.png)


   * **تفسیر خروجی رگرسیون لجستیک:**
      * $\sigma(w^T x)$ نشان‌دهنده احتمال اینکه $y=1$ باشد، با توجه به ورودی $x$ و پارامترهای $w$ است: $P(y=1|x, w) = \sigma(w^T x)$.
      * احتمال اینکه $y=0$ باشد، برابر است با: $P(y=0|x, w) = 1 - \sigma(w^T x)$.
      * $w$ یک بردار شامل ضرایب $w_0, w_1, ..., w_d$ است و $x$ یک بردار ویژگی شامل $x_0=1, x_1, ..., x_d$ است.
      * $w^T x$ همان ضرب داخلی بردار ویژگی‌ها در بردار وزن‌ها است.
      * **مثال:** اگر $\sigma(w^T x) = 0.7$ باشد، به این معنی است که 70 درصد شانس برد در بازی بسکتبال وجود دارد.
    





**سطح تصمیم (Decision Surface) در رگرسیون لجستیک**

در رگرسیون لجستیک، با مفهوم "سطح تصمیم" (Decision Surface) آشنا می‌شویم. قبل از این، در دسته‌بندهای خطی مانند پرسپترون که در جلسه قبل دیدیم، یک خط وجود داشت که نقاط دارای برچسب‌های مختلف (مانند -1 و 1) را از یکدیگر جدا می‌کرد. در رگرسیون لجستیک نیز می‌توان همین را تصور کرد: خطی که نقاط با برچسب 0 را از نقاط با برچسب 1 جدا می‌کند.

اما تفاوت در این است که برخلاف دسته‌بند خطی پرسپترون که تابع آن به صورت "پله‌ای تیز" (sharp step function) بود (یعنی به طور ناگهانی از یک مقدار به مقدار دیگر تغییر می‌کرد), در رگرسیون لجستیک، تابع "بسیار هموارتر" (smoother) است و به صورت پیوسته تغییر می‌کند.
![image5](https://github.com/ZahraShahlaie/Introduction_to_Machine_Learning/raw/main/Lecture_Notes/Chapter_01_Supervised_Learning/03-Logistic%20Regression/images/05.png)

**سطح تصمیم غیرخطی (Non-linear Decision Boundary) در رگرسیون لجستیک**

اگر نقاط داده به گونه‌ای باشند که با یک خط مستقیم قابل تفکیک نباشند و کلاس‌های 0 و 1 به صورت غیرخطی (مثلاً به شکل دایره‌ای) توزیع شده باشند، می‌توان از تکنیک "مهندسی ویژگی" (Feature Engineering) برای ایجاد یک مرز تصمیم غیرخطی استفاده کرد.

**روش کار:**
* **افزایش ابعاد ویژگی:** علاوه بر ویژگی‌های اصلی مانند $x_1$ و $x_2$， می‌توانیم ویژگی‌های مرتبه بالاتر مانند $x_1^2$ و $x_2^2$ را نیز به بردار ویژگی‌های ورودی اضافه کنیم.
* **ایجاد مرز تصمیم غیرخطی:** با اضافه کردن این ویژگی‌های جدید، رگرسیون لجستیک می‌تواند یک مرز تصمیم غیرخطی (مانند یک دایره) را یاد بگیرد که به بهترین شکل کلاس‌ها را از هم جدا کند.
* **مثال معادله مرز تصمیم دایره‌ای:** اگر مرز تصمیم یک دایره باشد، معادله آن می‌تواند به صورت $x_1^2 + x_2^2 - 1 = 0$ باشد. در این حالت، ضرایب (weights) مدل (بردار $w$) به گونه‌ای خواهند بود که برای $x_1^2$ و $x_2^2$ مقدار 1 و برای جمله ثابت (عرض از مبدأ) مقدار -1 را داشته باشند، و برای $x_1$ و $x_2$ مقادیر 0.
* **تفسیر خروجی:** در این مدل، رگرسیون لجستیک برای تمام نقاط داخل دایره، مقدار 0 و برای تمام نقاط خارج دایره، مقدار 1 را برمی‌گرداند. این شبیه به یک "چاله هموار" است که داخل آن 0 و بیرون آن 1 است.

**پارامترهای قابل یادگیری:**
تنها پارامترهایی که الگوریتم باید یاد بگیرد و به صورت خودکار محاسبه کند، همین مقادیر $w_0, w_1, w_2, w_3, w_4$ (شامل ضرایب ویژگی‌های اصلی، ویژگی‌های مرتبه بالاتر و عرض از مبدأ) هستند.
![image6](https://github.com/ZahraShahlaie/Introduction_to_Machine_Learning/raw/main/Lecture_Notes/Chapter_01_Supervised_Learning/03-Logistic%20Regression/images/06.png)

---
یاداوری: احتمال 
![image7](https://github.com/ZahraShahlaie/Introduction_to_Machine_Learning/raw/main/Lecture_Notes/Chapter_01_Supervised_Learning/03-Logistic%20Regression/images/07.png)


---






























* **تابع سیگموئید (Sigmoid Function) یا تابع لجستیک (Logistic Function):**
    * یک کاندید مناسب برای تابع فعال‌سازی است.
    * فرمول: $\sigma(z) = \frac{1}{1 + e^{-z}}$
    * **ویژگی‌ها:**

    * **مشتق تابع سیگموئید:**
 
* 

**2.2. سطح تصمیم (Decision Surface)**

* **تعریف:** سطح تصمیم یا مرز تصمیم (decision boundary)، ناحیه‌ای در فضای مسئله است که در آن، برچسب خروجی یک دسته‌بند مبهم است.
* **در دسته‌بندی باینری:** این سطح جایی است که احتمال تعلق یک نمونه به هر دو کلاس $y=0$ و $y=1$ برابر است.
* **معادله سطح تصمیم در رگرسیون لجستیک:**
    * $\sigma(w^T x) = 0.5$
    * از آنجایی که $\sigma(z) = 0.5$ فقط زمانی اتفاق می‌افتد که $z=0$، بنابراین مرز تصمیم جایی است که $w^T x = 0$.
    * این به این معنی است که سطوح تصمیم در رگرسیون لجستیک، "توابع خطی" از $x$ هستند.
    * **قانون پیش‌بینی:** اگر $\sigma(w^T x) \ge 0.5$， پیش‌بینی $y=1$ است؛ در غیر این صورت، پیش‌بینی $y=0$ است.
    * این معادل با این است که اگر $w^T x \ge 0$ باشد، $\hat{y}=1$ و در غیر این صورت $\hat{y}=0$.
* **ابعاد مرز تصمیم:** مرز تصمیم همیشه یک بعد کمتر از فضای ویژگی دارد.
* **مرز تصمیم غیرخطی (Non-linear Decision Boundary):**
    * می‌توان با اضافه کردن جملات مرتبه بالاتر (higher order terms) به بردار ویژگی‌ها (همانند مهندسی ویژگی - Feature Engineering)، مرزهای تصمیم پیچیده‌تر و غیرخطی را یاد گرفت.
    * **مثال:** با استفاده از ویژگی‌هایی مانند $x_1, x_2, x_1^2, x_2^2$， می‌توان یک مرز تصمیم دایره‌ای را ایجاد کرد.

**2.3. تخمین حداکثر درستنمایی (Maximum Likelihood Estimation - MLE)**

* **هدف:** در رگرسیون لجستیک، هدف پیدا کردن پارامترهای $w$ است که احتمال (شرطی) مشاهده داده‌های آموزشی را حداکثر کند.
* **احتمال پسین (Posterior Probability) یک نمونه:** $P(y^{(i)}|x^{(i)}, w)$
* **فرمول احتمال پسین برای دسته‌بندی باینری:**
    * از آنجایی که $y^{(i)}$ فقط می‌تواند $0$ یا $1$ باشد، می‌توان عبارت احتمال پسین را به صورت زیر ساده کرد:
    * $P(y^{(i)}|x^{(i)}, w) = \sigma(w^T x^{(i)})^{y^{(i)}} (1 - \sigma(w^T x^{(i)}))^{(1 - y^{(i)})}$
    * اگر $y^{(i)} = 1$ باشد، عبارت به $\sigma(w^T x^{(i)})$ تبدیل می‌شود.
    * اگر $y^{(i)} = 0$ باشد، عبارت به $(1 - \sigma(w^T x^{(i)}))$ تبدیل می‌شود.
* **تابع درستنمایی (Likelihood Function):** برای کل مجموعه داده، فرض می‌کنیم نمونه‌ها مستقل از هم هستند (Independent and Identically Distributed - IID). در این صورت، درستنمایی کل مجموعه داده، حاصل‌ضرب درستنمایی تک‌تک نمونه‌ها است:
    * $L(w) = \prod_{i=1}^{n} P(y^{(i)}|x^{(i)}, w)$
* **تابع لگاریتم درستنمایی (Log-Likelihood Function):** برای سادگی محاسبات، معمولاً لگاریتم تابع درستنمایی را به حداکثر می‌رسانیم (چون تابع لگاریتم یک تابع اکیداً صعودی است و نقطه ماکسیمم را تغییر نمی‌دهد).
    * $\hat{w} = \arg \max_{w} \log \prod_{i=1}^{n} P(y^{(i)}|x^{(i)}, w)$
    * با اعمال لگاریتم، حاصل‌ضرب به جمع تبدیل می‌شود:
    * $\log L(w) = \sum_{i=1}^{n} [y^{(i)} \log(\sigma(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(w^T x^{(i)}))]$

**2.4. تابع هزینه (Cost Function)**

* **ارتباط با لگاریتم درستنمایی:** تابع هزینه در رگرسیون لجستیک، منفی لگاریتم درستنمایی است، زیرا هدف ما "حداقل کردن" تابع هزینه (با استفاده از گرادیان کاهشی) است، در حالی که هدف MLE "حداکثر کردن" درستنمایی است.
    * $J(w) = -\sum_{i=1}^{n} \log P(y^{(i)}|x^{(i)}, w)$
    * $J(w) = \sum_{i=1}^{n} [-y^{(i)} \log(\sigma(w^T x^{(i)})) - (1 - y^{(i)}) \log(1 - \sigma(w^T x^{(i)}))]$
* **عدم وجود فرم بسته (No Closed-Form Solution):** برخلاف رگرسیون خطی، برای پیدا کردن $w$ با برابر قرار دادن گرادیان تابع هزینه به صفر ($\nabla_w J(w) = 0$)، راه حل فرم بسته وجود ندارد.
* **محدب بودن (Convexity):** با این حال، تابع هزینه $J(w)$ یک تابع "محدب" (convex) است.
    * این ویژگی تضمین می‌کند که الگوریتم‌های بهینه‌سازی مبتنی بر گرادیان (مانند گرادیان کاهشی) همواره به یک بهینه سراسری (global optimum) همگرا می‌شوند و در بهینه‌های محلی (local optima) گیر نمی‌کنند.
    * محدب بودن تابع هزینه از آنجا ناشی می‌شود که هر جمله در مجموع ( مربوط به یک نمونه) مشتق‌پذیر (دو بار) است و مشتق دوم آن همواره مثبت است، که نشان‌دهنده محدب بودن هر جمله و در نتیجه کل مجموع است.
* **نمایش بصری تابع هزینه (Binary Cross-Entropy Loss):**
    * این تابع هزینه به عنوان "Binary Cross-Entropy Loss" نیز شناخته می‌شود.
    * نمودار آن نشان می‌دهد که اگر مقدار پیش‌بینی شده (Predicted Value) نزدیک به مقدار واقعی (True Value) باشد، هزینه (Loss) کم است و اگر دور باشد، هزینه زیاد است.

**2.5. گرادیان کاهشی (Gradient Descent)**

* **هدف:** پیدا کردن پارامترهای $w$ که تابع هزینه $J(w)$ را حداقل کنند.
* **قانون به‌روزرسانی (Update Rule):**
    * $w^{t+1} = w^t - \eta \nabla_w J(w^t)$
    * که در آن $\eta$ نرخ یادگیری (learning rate) است.
* **گرادیان تابع هزینه رگرسیون لجستیک:**
    * $\nabla_w J(w) = \sum_{i=1}^{n} (\sigma(w^T x^{(i)}) - y^{(i)}) x^{(i)}$
* **مقایسه با رگرسیون خطی:**
    * گرادیان رگرسیون خطی: $\nabla_w J(w) = \sum_{i=1}^{n} (w^T x^{(i)} - y^{(i)}) x^{(i)}$
    * تنها تفاوت، وجود تابع سیگموئید ($\sigma$) در رگرسیون لجستیک است. این سادگی فرم گرادیان، یکی از دلایل انتخاب تابع سیگموئید است.
    * این فرم گرادیان نشان می‌دهد که "خطا" (تفاوت بین پیش‌بینی و مقدار واقعی) در ویژگی‌ها ضرب می‌شود تا جهت بهینه‌سازی پارامترها مشخص شود.

**2.6. رگرسیون لجستیک چندکلاسه (Multi-class Logistic Regression)**

* **مسئله:** زمانی که بیش از دو کلاس (K کلاس) وجود دارد و هر نمونه فقط به یک کلاس تعلق دارد.
* **تابع Softmax (Normalized Exponential):**
    * برای هر کلاس $k$، $\sigma_k(x; W)$ احتمال $P(y=k|x, W)$ را پیش‌بینی می‌کند.
    * مجموع احتمالات برای همه کلاس‌ها باید برابر با 1 باشد: $\sum_{k=1}^{K} P(y=k|x_0, W) = 1$.
    * **فرمول:** $\sigma_k(x, W) = P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_{j=1}^{K} \exp(w_j^T x)}$.
    * $W$ یک ماتریس وزن است که شامل بردارهای وزن $w_k$ برای هر کلاس است.
    * **ویژگی‌ها:**
        * مقادیر منفی را نیز به خوبی مدیریت می‌کند (به دلیل تابع نمایی).
        * مانند سیگموئید، تابع Softmax نیز هموار (smooth) و مشتق‌پذیر است.
        * مقدار بیشینه را به صورت هموار برجسته می‌کند (برخلاف تابع $\max(.)$ که گسسته و نامشتق‌پذیر است).
* **قانون پیش‌بینی:** برای یک ورودی جدید $x$， کلاسی را انتخاب می‌کنیم که $\sigma_k(x; W)$ آن را حداکثر کند:
    * $\alpha(x) = \arg \max_{k=1,...,K} \sigma_k(x; W)$
* **تابع هزینه (Cross-Entropy Loss برای Multi-class):**
    * تابع هزینه همچنان منفی لگاریتم درستنمایی است.
    * $J(W) = -\sum_{i=1}^{n} \sum_{k=1}^{K} y_k^{(i)} \log(\sigma_k(x^{(i)}; W))$
    * در اینجا $y_k^{(i)}$ نشان‌دهنده یک "کدگذاری یک از K" (1-of-K encoding) برای برچسب $y^{(i)}$ است (اگر نمونه $i$ به کلاس $k$ تعلق داشته باشد، $y_k^{(i)}=1$ و در غیر این صورت $0$).
* **گرادیان کاهشی برای Multi-class Logistic Regression:**
    * همچنان راه حل فرم بسته وجود ندارد.
    * قانون به‌روزرسانی گرادیان کاهشی: $w_j^{t+1} = w_j^t - \eta \nabla_{w_j} J(W^t)$
    * گرادیان نسبت به بردار وزن $w_j$ برای کلاس $j$:
        * $\nabla_{w_j} J(W) = \sum_{i=1}^{n} (\sigma_j(x^{(i)}; W) - y_j^{(i)}) x^{(i)}$
    * که در آن $w_j^t$ بردار وزن برای کلاس $j$ در تکرار $t$-ام است.

---

**3. جمع‌بندی رگرسیون لجستیک (Logistic Regression Summary)**

* **دسته‌بند خطی (Linear Classifier):** رگرسیون لجستیک یک دسته‌بند خطی است (مگر اینکه از ویژگی‌های غیرخطی - Feature Engineering - استفاده شود).
* **بهینه‌سازی با حداکثر درستنمایی (Maximum Likelihood Optimization):** مسئله بهینه‌سازی آن از طریق حداکثر درستنمایی به دست می‌آید.
* **عدم وجود فرم بسته (No Closed-Form Solution):** برای حل مسئله بهینه‌سازی آن راه حل فرم بسته وجود ندارد.
* **تابع هزینه محدب (Convex Cost Function):** تابع هزینه آن محدب است و می‌توان با استفاده از گرادیان صعودی (برای حداکثر کردن درستنمایی) یا گرادیان کاهشی (برای حداقل کردن تابع هزینه) به بهینه سراسری دست یافت.

---

**4. مطالعه بیشتر: دیدگاه احتمالی در دسته‌بندی (Probabilistic View in Classification)**

* **متغیرهای تصادفی:** در مسائل دسته‌بندی، هم ویژگی‌ها (مثل قد یک فرد) و هم برچسب کلاس (مثل اضافه وزن داشتن یا نداشتن) به عنوان متغیرهای تصادفی در نظر گرفته می‌شوند.
* **هدف:** مشاهده ویژگی‌ها ($x$) و پیدا کردن برچسب کلاس ($y$).

**4.1. تعاریف (Definitions)**

* **احتمال پسین (Posterior Probability):** احتمال یک برچسب کلاس $C_k$ با توجه به یک نمونه $x$.
    * $P(C_k|x)$
* **درستنمایی (Likelihood) یا احتمال شرطی کلاس (Class Conditional Probability):** تابع چگالی احتمال (PDF) بردار ویژگی $x$ برای نمونه‌های کلاس $C_k$.
    * $P(x|C_k)$
* **احتمال پیشین (Prior Probability):** احتمال وقوع کلاس $C_k$.
    * $P(C_k)$
* **تابع چگالی احتمال (PDF) بردار ویژگی $x$:**
    * $P(x) = \sum_{k=1}^{K} P(x|C_k) P(C_k)$

**4.2. دسته‌بندهای احتمالی (Probabilistic Classifiers)**

* **رویکردهای اصلی:**
    * **مدل‌های مولد (Generative Models):**
        * یادگیری توزیع احتمال توأم $P(x, y)$.
        * سپس از آن برای پیدا کردن $P(C_k|x)$ (با استفاده از قضیه بیز) استفاده می‌شود.
        * می‌توانند برای تولید جفت‌های $(x, y)$ نیز استفاده شوند.
    * **مدل‌های تمایزدهنده (Discriminative Models):**
        * مستقیماً توزیع احتمال شرطی $P(y|x)$ را تخمین می‌زنند.
        * **رگرسیون لجستیک یک رویکرد تمایزدهنده است.**
        * در رگرسیون لجستیک، ما مستقیماً می‌خواهیم برچسب کلاس را با $\sigma(w^T x)$ مشخص کنیم.

---

**5. مراجع (References)**

این اسلایدها توسط دانیال غریب تهیه شده‌اند.

---
