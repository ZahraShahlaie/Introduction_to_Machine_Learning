✅ جزوه کامل و ساخت‌یافته‌ی جلسه چهارم با موضوع **رگرسیون لجستیک (Logistic Regression)** آماده شد.

📄 دانلود PDF جزوه:
[📥 دریافت فایل PDF](sandbox:/mnt/data/ML_Session4_Logistic_Regression.pdf)

ساختار جزوه:

* آموزش مفهومی رگرسیون لجستیک
* تابع سیگموید، مرز تصمیم، MLE، تابع هزینه
* مدل چندکلاسه (Softmax)
* دیدگاه احتمالاتی تفکیک‌گر vs. گشتاورزا

اگر تأیید کنی، مجموعه ۱۰ الی ۲۰ سؤال مصاحبه‌ای تخصصی هم براساس همین جلسه برات آماده می‌کنم. ✅ آیا ادامه بدم؟


با کمال میل! در ادامه، **متن کامل جزوه‌ی آموزشی جلسه ۴** با موضوع **رگرسیون لجستیک (Logistic Regression)** رو در قالبی آموزشی، منظم و یادگیری‌محور برات می‌نویسم—درست همون‌طور که در PDF بود، فقط اینجا به‌صورت متنی کامل برای کپی یا ویرایش راحت:

---

## 📘 جزوه آموزشی جلسه ۴

### موضوع: رگرسیون لجستیک (Logistic Regression)

---

### ۱. مقدمه: چرا رگرسیون لجستیک؟

رگرسیون لجستیک مدلی برای حل مسائل **طبقه‌بندی دودویی** است. برخلاف رگرسیون خطی که مقدار پیوسته پیش‌بینی می‌کند، رگرسیون لجستیک می‌خواهد خروجی بین ۰ و ۱ باشد (یعنی احتمال تعلق به کلاس مثبت). بنابراین به تابعی نیاز داریم که خروجی‌اش در بازه \[0,1] باشد.

---

### ۲. چرا نمی‌توان از رگرسیون خطی برای طبقه‌بندی استفاده کرد؟

رگرسیون خطی ممکن است خروجی‌هایی خارج از بازه \[0,1] تولید کند، مثل 1.2 یا -0.3 که نمی‌توانند احتمال باشند. در رگرسیون لجستیک، تابع خروجی به‌گونه‌ای طراحی شده که خروجی همیشه بین صفر و یک باشد.

---

### ۳. تابع سیگموید (Sigmoid Function)

تابع سیگموید به شکل زیر تعریف می‌شود:

$$
σ(z) = \frac{1}{1 + e^{-z}}
$$

این تابع برای نگاشت مقادیر به بازه \[0,1] مناسب است. وقتی ورودی مثبت باشد، خروجی نزدیک به ۱ و وقتی منفی باشد، خروجی نزدیک به صفر خواهد بود. بنابراین مقدار خروجی آن را می‌توان به‌عنوان احتمال تفسیر کرد.

---

### ۴. تابع تصمیم در رگرسیون لجستیک

در رگرسیون لجستیک، تابع تصمیم ترکیب تابع سیگموید با ترکیب خطی ویژگی‌هاست:

$$
h(x) = σ(w^T x)
$$

اگر $h(x) ≥ 0.5$، نمونه به کلاس ۱ تعلق دارد و در غیر این‌صورت به کلاس ۰. بنابراین مرز تصمیم همانند مدل‌های خطی یک هایپرپلین خواهد بود.

---

### ۵. سطح تصمیم (Decision Boundary)

سطح تصمیم همان مکانی است که احتمال تعلق به دو کلاس برابر است، یعنی:

$$
σ(w^T x) = 0.5 \Rightarrow w^T x = 0
$$

بنابراین مرز تصمیم در فضای ویژگی‌ها یک **مرز خطی** است. اگر از نگاشت ویژگی‌های غیرخطی استفاده کنیم، مرز تصمیم می‌تواند غیرخطی نیز باشد.

---

### ۶. برآورد بیشینه درست‌نمایی (Maximum Likelihood Estimation)

برای یادگیری وزن‌ها در رگرسیون لجستیک از روش MLE استفاده می‌شود. احتمال هر نمونه به صورت زیر تعریف می‌شود:

$$
P(y|x,w) = σ(w^T x)^y \cdot (1 - σ(w^T x))^{1-y}
$$

با گرفتن لگاریتم از احتمال کل داده‌ها، تابع **log-likelihood** به‌دست می‌آید که راحت‌تر برای بهینه‌سازی استفاده می‌شود.

---

### ۷. تابع هزینه و مشتق آن

تابع هزینه همان منفی log-likelihood است و به شکل زیر نوشته می‌شود:

$$
J(w) = -\sum_i [ y^{(i)} \log(σ(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - σ(w^T x^{(i)})) ]
$$

این تابع **محدب (Convex)** است و گرادیان آن نیز به‌سادگی قابل محاسبه است، بنابراین می‌توان از گرادیان دیسنت برای بهینه‌سازی آن استفاده کرد.

---

### ۸. مقایسه با گرادیان رگرسیون خطی

در رگرسیون خطی، گرادیان تابع هزینه به صورت:

$$
\nabla J(w) = \sum (w^T x - y) x
$$

اما در رگرسیون لجستیک، تفاوت فقط در جایگزینی $w^T x$ با تابع سیگموید است.

$$
\nabla J(w) = \sum (σ(w^T x) - y) x
$$

بنابراین ساختار مشابه است اما رفتار یادگیری متفاوت، چون لجستیک خروجی را به‌عنوان احتمال مدل می‌کند.

---

### ۹. رگرسیون لجستیک چندکلاسه (Softmax Regression)

برای طبقه‌بندی چندکلاسه، از تعمیم رگرسیون لجستیک به **Softmax Regression** استفاده می‌شود. تابع احتمال برای کلاس k به صورت زیر است:

$$
P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_j \exp(w_j^T x)}
$$

در این مدل برای هر کلاس یک بردار وزن مجزا $w_k$ در نظر گرفته می‌شود و مجموع خروجی‌ها برابر ۱ خواهد بود.

---

### ۱۰. دیدگاه احتمالاتی: Generative vs Discriminative

مدل‌های **Generative** مانند Naive Bayes ابتدا توزیع $P(x|y)$ و $P(y)$ را مدل می‌کنند و با استفاده از قانون بیز، احتمال $P(y|x)$ را محاسبه می‌کنند. اما مدل‌های **Discriminative** مانند Logistic Regression مستقیماً $P(y|x)$ را مدل می‌کنند. این باعث می‌شود دقت طبقه‌بندی بالاتر ولی قدرت تولید داده کمتر باشد.

---

اگر بخوای می‌تونم برات:

* ۱۰ یا ۲۰ سوال مصاحبه‌ای با پاسخ مفهومی از همین مبحث بنویسم
* یا ادامه بدم به جلسه ۵ (اگه فایلش رو برام بفرستی)

📩 آماده‌ای برای سوالات مصاحبه‌ای Logistic Regression؟


فوق‌العاده! 🧠 حالا براساس فایل جلسه سوم که موضوعش **رگرسیون لجستیک (Logistic Regression)** هست، برات یک **جزوه‌ی کامل، آموزشی و مفهومی** تهیه می‌کنم، با ساختاری دقیق مثل جلسات قبل، و هر بخشش شامل حدود ۸ تا ۱۰ خط توضیح یادگیری‌محور.

---

## 📘 جزوه آموزشی جلسه ۴

### موضوع: رگرسیون لجستیک (Logistic Regression)

برگرفته از اسلایدهای دکتر علی شریفی زارعی – دانشگاه صنعتی شریف

---

### 1. مقدمه: چرا رگرسیون لجستیک؟

رگرسیون لجستیک مدلی برای حل مسائل **طبقه‌بندی دودویی** است. مثلاً تشخیص اسپم بودن ایمیل، خوش‌خیم یا بدخیم بودن تومور، یا پیش‌بینی ترک کار. برخلاف رگرسیون خطی که خروجی را به‌صورت مقدار پیوسته پیش‌بینی می‌کند، در رگرسیون لجستیک می‌خواهیم خروجی بین ۰ و ۱ باشد (یعنی احتمال تعلق به کلاس مثبت). بنابراین به یک تابع نیاز داریم که خروجی‌اش در بازه \[0,1] باشد.

---

### 2. چرا نمی‌توان از رگرسیون خطی برای طبقه‌بندی استفاده کرد؟

اگر از رگرسیون خطی استفاده کنیم، ممکن است خروجی مدل (hθ(x)) مقادیری خارج از بازه \[0,1] تولید کند (مثلاً 1.5 یا -0.3). این مقادیر را نمی‌توان به‌عنوان احتمال تفسیر کرد. در رگرسیون لجستیک، ما تابعی می‌خواهیم که خروجی‌اش **همیشه بین صفر و یک** باشد تا بتواند احتمال تعلق به کلاس ۱ را بیان کند.

---

### 3. تابع سیگموید (Sigmoid Function)

برای رسیدن به خروجی بین ۰ و ۱، از تابع **سیگموید** استفاده می‌کنیم:

$$
σ(z) = \frac{1}{1 + e^{-z}}
$$

این تابع نرم، مشتق‌پذیر و به‌خوبی در محدوده \[0,1] تعریف شده است. وقتی ورودی بزرگ باشد، خروجی نزدیک به ۱ و وقتی ورودی خیلی منفی باشد، خروجی نزدیک به ۰ می‌شود. بنابراین خروجی را به‌صورت احتمال تفسیر می‌کنیم.

---

### 4. تابع تصمیم در رگرسیون لجستیک

در رگرسیون لجستیک، تابع تصمیم همان تابع سیگموید از ترکیب خطی ویژگی‌هاست:

$$
h(x) = σ(w^T x)
$$

برای پیش‌بینی کلاس، از آستانه ۰.۵ استفاده می‌شود:

* اگر $h(x) ≥ 0.5$ → کلاس ۱
* اگر $h(x) < 0.5$ → کلاس ۰
  این مرز تصمیم در فضای ویژگی‌ها یک **هایپرپلین خطی** خواهد بود.

---

### 5. سطح تصمیم (Decision Boundary)

سطح تصمیم محل‌هایی است که احتمال پیش‌بینی کلاس ۰ و ۱ برابر است. یعنی زمانی که:

$$
σ(w^T x) = 0.5
\Rightarrow w^T x = 0
$$

این یعنی مرز تصمیم در رگرسیون لجستیک **همانند مدل خطی، یک مرز خطی (یا غیرخطی با تبدیل ویژگی‌ها)** است. البته در صورت استفاده از ویژگی‌های غیرخطی، این مرز می‌تواند پیچیده‌تر شود.

---

### 6. برآورد بیشینه درست‌نمایی (Maximum Likelihood Estimation)

برای یادگیری پارامترهای w، از روش **بیشینه‌سازی تابع درست‌نمایی (MLE)** استفاده می‌کنیم. تابع احتمال به شکل زیر است:

$$
P(y|x,w) = σ(w^T x)^y (1 - σ(w^T x))^{1-y}
$$

و تابع لگاریتم درست‌نمایی:

$$
\log L = \sum_i y^{(i)} \log(σ(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - σ(w^T x^{(i)}))
$$

هدف یادگیری، بیشینه کردن این مقدار است.

---

### 7. تابع هزینه و مشتق آن

تابع هزینه رگرسیون لجستیک همان منفی لگاریتم درست‌نمایی است:

$$
J(w) = -\sum_i y^{(i)} \log(σ(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - σ(w^T x^{(i)}))
$$

این تابع **کووِکس (Convex)** است، بنابراین گرادیان دیسنت می‌تواند به‌راحتی مینیمم آن را پیدا کند. مشتق آن برای هر نمونه:

$$
\nabla J(w) = \sum_i (σ(w^T x^{(i)}) - y^{(i)}) x^{(i)}
$$

---

### 8. مقایسه با گرادیان رگرسیون خطی

در رگرسیون خطی گرادیان به صورت:

$$
\nabla J(w) = \sum (w^T x - y) x
$$

در حالی که در رگرسیون لجستیک، تنها تفاوت در وجود تابع غیرخطی سیگموید است. این تفاوت باعث می‌شود یادگیری بر اساس **احتمال تعلق به کلاس‌ها** صورت گیرد، نه فقط فاصله از مقدار عددی.

---

### 9. رگرسیون لجستیک چندکلاسه (Softmax Regression)

در مسائل چندکلاسه، خروجی فقط بین ۰ و ۱ نیست. در اینجا، به جای تابع سیگموید از **تابع softmax** استفاده می‌شود:

$$
P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_j \exp(w_j^T x)}
$$

برای هر کلاس یک بردار وزن مجزا تعریف می‌شود. مجموع خروجی‌های softmax برابر با ۱ است، بنابراین خروجی‌ها قابل تفسیر به‌عنوان احتمال هستند.

---

### 10. دیدگاه احتمالاتی: مدل‌های گشتاورزای (Generative) و تفکیک‌گر (Discriminative)

مدل‌های **Generative** مثل Naive Bayes ابتدا $P(x|C_k)$ و $P(C_k)$ را مدل می‌کنند و با قانون بیز $P(C_k|x)$ را می‌سازند. ولی مدل‌های **Discriminative** مثل Logistic Regression مستقیماً $P(C_k|x)$ را مدل می‌کنند. این باعث می‌شود مدل‌های تفکیک‌گر معمولاً در دقت طبقه‌بندی عملکرد بهتری داشته باشند، ولی توانایی تولید داده ندارند.

---

اگر تأیید کنی، در مرحله‌ی بعد:

✅ این جزوه رو به‌صورت فایل PDF حرفه‌ای و فارسی طراحی می‌کنم
✅ و مجموعه سؤالات مصاحبه‌ای مرتبط با رگرسیون لجستیک رو هم برات آماده می‌کنم (در دو سطح: مفهومی و تکنیکی)

🟢 ادامه بدم؟

--

