-----

## راهنمای جامع مصاحبه رگرسیون لجستیک (Logistic Regression)

**مدرس:** علی شریفی-زارچی
**دپارتمان:** مهندسی کامپیوتر
**دانشگاه:** صنعتی شریف
**درس:** یادگیری ماشین (CE 40717)

-----

### بخش ۱: مفاهیم عمومی و مقدمه

-----

**سوال:** هدف اصلی مسائل **دسته‌بندی (classification)**، برخلاف مسائل **رگرسیون (regression)**، چیست؟

**پاسخ:** در دسته‌بندی، هدف پیش‌بینی یک مجموعه گسسته از کلاس‌ها (مثل بیمار/سالم) است، در حالی که رگرسیون یک مقدار پیوسته (مثل قیمت خانه) را پیش‌بینی می‌کند.

-----

**سوال:** در رگرسیون لجستیک، معمولاً چه برچسب‌هایی برای دسته‌بندی دودویی استفاده می‌شود و چرا؟

**پاسخ:** برچسب‌های **۰ و ۱** به دلیل سادگی و سازگاری با توابع احتمالاتی مورد استفاده در مدل انتخاب می‌شوند.

-----

**سوال:** **پرسپترون (Perceptron)**، که پیش‌تر با آن آشنا شدیم، چه محدودیتی داشت که رگرسیون لجستیک آن را رفع می‌کند؟

**پاسخ:** خروجی پرسپترون یک تابع پله‌ای (مثل -۱ یا ۱) بود که "درجه اطمینان" را نشان نمی‌داد. رگرسیون لجستیک یک خروجی احتمالی فراهم می‌کند.

-----

**سوال:** رگرسیون لجستیک چگونه مشکل "درجه اطمینان" در پیش‌بینی‌ها را حل می‌کند؟

**پاسخ:** این مدل یک **نمره احتمال بین ۰ و ۱** را به عنوان خروجی می‌دهد که نشان‌دهنده احتمال تعلق یک نمونه به یک کلاس خاص است، نه یک برچسب قطعی.

-----

**سوال:** چرا **رگرسیون خطی (Linear Regression)** برای مسائل دسته‌بندی، به خصوص در حضور نقاط پرت (outliers)، نامناسب است؟

**پاسخ:** رگرسیون خطی می‌تواند به شدت تحت تأثیر نقاط پرت قرار گیرد، که باعث می‌شود خط برازش شده به شدت تغییر کرده و منجر به طبقه‌بندی اشتباه بسیاری از نقاط شود.

-----

**سوال:** آیا رگرسیون لجستیک یک الگوریتم رگرسیون است یا طبقه‌بندی؟ چرا "رگرسیون" نامیده می‌شود؟

**پاسخ:** این یک **الگوریتم طبقه‌بندی** است. نام "رگرسیون" به دلیل استفاده از یک رویکرد رگرسیون-مانند برای پیش‌بینی احتمال (که یک مقدار پیوسته است) قبل از تبدیل آن به یک کلاس، می‌باشد.

-----

**سوال:** تفاوت اساسی در خروجی بین یک Perceptron و Logistic Regression برای دسته‌بندی دودویی چیست؟

**پاسخ:** پرسپترون خروجی‌های گسسته (-۱ یا ۱) را ارائه می‌دهد، در حالی که رگرسیون لجستیک یک احتمال پیوسته بین ۰ و ۱ را خروجی می‌دهد.

-----

**سوال:** "سوال محوری" که رگرسیون لجستیک به دنبال پاسخ آن است، چیست؟

**پاسخ:** چگونه الگوریتمی را توسعه دهیم که بتواند به جای یک تابع پله‌ای تیز، نوعی "احتمال" (مانند احتمال بیمار بودن یا سالم بودن) را به عنوان خروجی ارائه دهد؟

-----

**سوال:** در زمینه رگرسیون لجستیک، "مدل‌سازی درجه اطمینان" به چه معناست؟

**پاسخ:** به معنای توسعه تابعی است که خروجی آن به طور هموار بین ۰ و ۱ تغییر می‌کند، و احتمالات مختلف را به جای برچسب‌های کلاسی مجزا نشان می‌دهد.

-----

**سوال:** سه مثال واقعی از مسائل دسته‌بندی دودویی که رگرسیون لجستیک می‌تواند حل کند، نام ببرید.

**پاسخ:** پیش‌بینی اینکه آیا بیمار سرطان دارد/سالم است، مشتری وام را پرداخت می‌کند/نکول می‌کند، یا ایمیل اسپم است/نیست.

-----

**سوال:** در مبحث "Introduction" در فایل PDF، به مثال‌های طبقه‌بندی دودویی اشاره شده است. یک سناریوی دیگر در دنیای واقعی برای طبقه‌بندی دودویی ارائه دهید که با رگرسیون لجستیک قابل حل باشد.

**پاسخ:** **پیش‌بینی ورشکستگی شرکت‌ها (Company Bankruptcy Prediction)**. خروجی ۰ برای "ورشکسته نمی‌شود" و ۱ برای "ورشکسته می‌شود" با ویژگی‌های مالی شرکت.

-----

### بخش ۲: پیش‌پردازش داده (Data Preprocessing)

-----

**سوال:** اولین گام در آماده‌سازی داده‌ها برای رگرسیون لجستیک در مورد ویژگی‌ها چیست؟

**پاسخ:** تمام ویژگی‌ها باید به **مقادیر عددی** تبدیل شوند؛ برای مثال، جنسیت (مرد/زن) می‌تواند به ۰ و ۱ کدگذاری شود.

-----

**سوال:** چرا "مقیاس‌بندی (scaling)" ویژگی‌های عددی در رگرسیون لجستیک مهم است؟

**پاسخ:** ویژگی‌ها می‌توانند مقیاس‌های بسیار متفاوتی داشته باشند (مثلاً ۰-۱ در مقابل ۱۵۰-۲۳۰)، که می‌تواند منجر به تأثیر نامتناسب بر ضرایب مدل و اختلال در فرآیند بهینه‌سازی شود.

-----

**سوال:** دو تکنیک رایج برای مقیاس‌بندی ویژگی‌ها که در متن ذکر شده‌اند، کدامند؟

**پاسخ:** **استانداردسازی (Standardization)** (نرمال‌سازی Z-score) و **مقیاس‌بندی Min-Max (Min-Max Scaling)**.

-----

**سوال:** **استانداردسازی (Z-score normalization)** را توضیح دهید.

**پاسخ:** این تکنیک ویژگی‌ها را به گونه‌ای تغییر می‌دهد که دارای **میانگین صفر و انحراف معیار (یا واریانس) یک** شوند.

-----

**سوال:** **مقیاس‌بندی Min-Max (Min-Max Scaling)** را توضیح دهید.

**پاسخ:** این روش ویژگی‌ها را به یک **محدوده ثابت، معمولاً بین ۰ و ۱**، مقیاس‌بندی می‌کند، با استفاده از فرمول ساده تغییر و مقیاس‌بندی مجدد.

-----

**سوال:** هدف کلی پیش‌پردازش داده در یادگیری ماشین چیست؟

**پاسخ:** اطمینان از **کیفیت داده‌ها** قبل از تغذیه به مدل، شامل مدیریت نقاط پرت، بصری‌سازی، و بررسی توزیع ویژگی‌ها.

-----

**سوال:** آیا می‌توان ردیف‌ها یا ستون‌ها را در طول پیش‌پردازش داده حذف کرد؟ اگر بله، چرا؟

**پاسخ:** بله، برخی ردیف‌ها (نمونه‌ها) یا ستون‌ها (ویژگی‌ها) ممکن است حذف شوند اگر نقاط پرت باشند، حاوی مقادیر گمشده باشند، یا نتایج مدل را به طور منفی تحت تأثیر قرار دهند.

-----

**سوال:** ستون "برچسب (label)" در یک مجموعه داده برای طبقه‌بندی چه چیزی را نشان می‌دهد؟

**پاسخ:** متغیر هدف است که قصد داریم آن را بر اساس ویژگی‌های ورودی پیش‌بینی کنیم (مثلاً "اضافه وزن" بودن یا نبودن).

-----

**سوال:** چگونه مقیاس‌بندی بر قابلیت تفسیر وزن‌ها (coefficients) در یک مدل خطی تأثیر می‌گذارد؟

**پاسخ:** وقتی ویژگی‌ها مقیاس‌بندی می‌شوند، وزن‌ها از نظر اندازه قابل مقایسه‌تر می‌شوند، و اهمیت نسبی آنها را دقیق‌تر منعکس می‌کنند تا اینکه صرفاً تحت تأثیر مقیاس‌های ویژگی باشند.

-----

**سوال:** علاوه بر مقیاس‌بندی، دو مرحله دیگر از پیش‌پردازش داده که صریحاً ذکر نشده‌اند اما در عمل مهم هستند را نام ببرید.

**پاسخ:** **مدیریت مقادیر گمشده (Handling Missing Values)** و **حذف داده‌های تکراری (Duplicate Data Removal)**.

-----

### بخش ۳: تابع سیگموید (Sigmoid Function) یا تابع لجستیک

-----

**سوال:** هدف اصلی **تابع سیگموید (Sigmoid Function)** در رگرسیون لجستیک چیست؟

**پاسخ:** تبدیل ترکیب خطی ورودی‌ها ($w^T x$) به یک **مقدار احتمالی بین ۰ و ۱**.

-----

**سوال:** فرمول ریاضی برای تابع سیگموید، $\\sigma(z)$، چیست؟

**پاسخ:** $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.

-----

**سوال:** چه اتفاقی برای $\\sigma(z)$ می‌افتد زمانی که $z$ به سمت بی‌نهایت مثبت میل می‌کند؟

**پاسخ:** $\\sigma(z)$ به **۱** نزدیک می‌شود.

-----

**سوال:** چه اتفاقی برای $\\sigma(z)$ می‌افتد زمانی که $z$ به سمت بی‌نهایت منفی میل می‌کند؟

**پاسخ:** $\\sigma(z)$ به **۰** نزدیک می‌شود.

-----

**سوال:** مقدار $\\sigma(z)$ زمانی که $z = 0$ است، چقدر است؟

**پاسخ:** $\\sigma(0) = 0.5$.

-----

**سوال:** چرا تابع سیگموید به تابع پله‌ای (step function) برای رگرسیون لجستیک ترجیح داده می‌شود؟

**پاسخ:** زیرا **هموار (smooth) و مشتق‌پذیر (differentiable)** است، که امکان بهینه‌سازی مبتنی بر گرادیان را فراهم می‌کند و "درجه اطمینان" (احتمال) را به جای خروجی‌های ناگهانی ۰/۱ می‌دهد.

-----

**سوال:** تابع سیگموید چگونه امکان نمایش "درجه اطمینان" را فراهم می‌کند؟

**پاسخ:** خروجی پیوسته آن بین ۰ و ۱ مستقیماً با **احتمالات** مطابقت دارد، که نشان می‌دهد یک نمونه با چه احتمالی به یک کلاس تعلق دارد.

-----

**سوال:** احتمال کلاس ۱ را با توجه به $x$ و $w$ با استفاده از نماد تابع سیگموید بنویسید.

**پاسخ:** $P(y=1|x, w) = \\sigma(w^T x)$.

-----

**سوال:** احتمال کلاس ۰ را با توجه به $x$ و $w$ با استفاده از نماد تابع سیگموید بنویسید.

**پاسخ:** $P(y=0|x, w) = 1 - \\sigma(w^T x)$.

-----

**سوال:** مشتق تابع سیگموید، $\\frac{d}{dz}\\sigma(z)$، چیست؟

**پاسخ:** $\\sigma(z)(1 - \\sigma(z))$.

-----

**سوال:** چرا شکل خاص مشتق سیگموید در رگرسیون لجستیک مفید است؟

**پاسخ:** این ویژگی **محاسبه گرادیان‌ها را ساده می‌کند**، و فرآیند بهینه‌سازی (مانند گرادیان دیسنت) را از نظر محاسباتی عملی می‌سازد.

-----

**سوال:** مشتق تابع سیگموئید در چه نقطه‌ای به حداکثر مقدار خود می‌رسد؟

**پاسخ:** مشتق در **$z=0$** به حداکثر می‌رسد، جایی که $\\sigma(z)=0.5$ است.

-----

**سوال:** یک مقدار بالای $\\sigma(w^T x)$ (مثلاً ۰.۹) درباره پیش‌بینی چه چیزی را نشان می‌دهد؟

**پاسخ:** این نشان‌دهنده احتمال بالای (۹۰٪) تعلق نمونه به کلاس ۱ است.

-----

**سوال:** یک مقدار پایین $\\sigma(w^T x)$ (مثلاً ۰.۱) درباره پیش‌بینی چه چیزی را نشان می‌دهد؟

**پاسخ:** این نشان‌دهنده احتمال پایین (۱۰٪) تعلق نمونه به کلاس ۱، یا احتمال بالای (۹۰٪) تعلق آن به کلاس ۰ است.

-----

**سوال:** در عبارت $w^T x$، $w$ و $x$ چه چیزی را نشان می‌دهند؟

**پاسخ:** $w$ بردار **پارامترهای مدل (وزن‌ها)** است، و $x$ بردار **ویژگی‌های ورودی (شامل یک بایاس $x\_0=1$)** است.

-----

**سوال:** ترم "بایاس (bias)" چگونه در عبارت $w^T x$ گنجانده می‌شود؟

**پاسخ:** با افزودن یک ویژگی ثابت **$x\_0=1$** به بردار ویژگی $x$ و یک وزن متناظر **$w\_0$** به بردار وزن $w$.

-----

**سوال:** اگر خروجی $\\sigma(w^T x)$ برابر با ۰.۷ باشد، این به چه معناست؟

**پاسخ:** این نشان‌دهنده **۷۰٪ احتمال تعلق به کلاس ۱** است، یا ۷۰٪ شانس "برد" در مثال داده شده.

-----

**سوال:** حدود خروجی تابع سیگموید چیست؟

**پاسخ:** خروجی همیشه **بین ۰ و ۱** است (شامل ۰ و ۱ نمی‌شود).

-----

**سوال:** آیا تابع سیگموید می‌تواند دقیقاً ۰ یا دقیقاً ۱ را خروجی دهد؟

**پاسخ:** خیر، تابع سیگموید به صورت **مجانبی** به ۰ و ۱ نزدیک می‌شود اما هرگز دقیقاً به آنها نمی‌رسد.

-----

**سوال:** چرا مشتق‌پذیری تابع سیگموئید برای فرآیند یادگیری رگرسیون لجستیک حیاتی است؟

**پاسخ:** مشتق‌پذیری تضمین می‌کند که تابع هزینه نیز مشتق‌پذیر است و می‌توان **گرادیان (شیب) آن را محاسبه کرد**. این گرادیان برای **به‌روزرسانی وزن‌ها** در الگوریتم‌هایی مانند گرادیان دیسنت ضروری است.

-----

**سوال:** در مبحث "Fundamentals" از رگرسیون لجستیک، بیان شده است که برای یک مسئله طبقه‌بندی دودویی، می‌توانیم $P(y=1|x,w) = \\sigma(w^T x)$ و $P(y=0|x,w) = 1 - \\sigma(w^T x)$ داشته باشیم. فرض کنید مدل شما برای یک نمونه خاص $P(y=1|x,w) = 0.85$ را پیش‌بینی می‌کند. این مقدار به صورت یک "احتمال" چگونه تفسیر می‌شود و مدل چه تصمیمی خواهد گرفت؟

**پاسخ:** مدل ۸۵٪ اطمینان دارد که نمونه به **کلاس مثبت ($y=1$)** تعلق دارد. طبق قاعده تصمیم‌گیری استاندارد (آستانه ۰.۵)، چون ۰.۸۵ ≥ ۰.۵ است، مدل پیش‌بینی می‌کند که این نمونه به **کلاس ۱** تعلق دارد.

-----

**سوال:** در مورد تابع سیگموئید، در بخش "Introduction (cont.)" اشاره شده که این تابع "differentiable" (مشتق‌پذیر) است. چرا خاصیت مشتق‌پذیری برای تابع فعال‌سازی در رگرسیون لجستیک اهمیت دارد؟

**پاسخ:** مشتق‌پذیری تضمین می‌کند که تابع هزینه نیز مشتق‌پذیر است و می‌توان **گرادیان (شیب) آن را محاسبه کرد**. این گرادیان برای **به‌روزرسانی وزن‌ها** در الگوریتم‌هایی مانند گرادیان دیسنت ضروری است.

-----

### بخش ۴: سطح تصمیم (Decision Surface)

-----

**سوال:** "سطح تصمیم (Decision Surface)" در رگرسیون لجستیک چیست؟

**پاسخ:** مرزی است که نقاط داده را بر اساس احتمال پیش‌بینی شده، به کلاس‌های مختلف (مثلاً ۰ و ۱) تقسیم می‌کند. این مرز جایی است که $\\sigma(w^T x) = 0.5$ (یا $w^T x = 0$) است.

-----

**سوال:** تفاوت کلیدی سطح تصمیم در رگرسیون لجستیک با پرسپترون چیست؟

**پاسخ:** در رگرسیون لجستیک، سطح تصمیم "بسیار **هموارتر**" است و به صورت پیوسته تغییر می‌کند، در حالی که در پرسپترون "پله‌ای تیز" بود.

-----

**سوال:** چه چیزی باعث می‌شود سطح تصمیم رگرسیون لجستیک هموارتر باشد؟

**پاسخ:** استفاده از **تابع سیگموید**، که یک تابع پیوسته و مشتق‌پذیر است، خروجی‌های احتمالی نرمی را فراهم می‌کند که به یک سطح تصمیم هموار منجر می‌شود.

-----

**سوال:** چگونه می‌توان در رگرسیون لجستیک یک "مرز تصمیم غیرخطی (Non-linear Decision Boundary)" ایجاد کرد؟

**پاسخ:** با استفاده از تکنیک "**مهندسی ویژگی (Feature Engineering)**" و افزودن ویژگی‌های مرتبه بالاتر (مانند $x\_1^2$, $x\_2^2$, $x\_1 x\_2$) به بردار ویژگی‌های ورودی.

-----

**سوال:** "افزایش ابعاد ویژگی (Feature Engineering)" در زمینه ایجاد مرز تصمیم غیرخطی به چه معناست؟

**پاسخ:** به معنای اضافه کردن **ویژگی‌های جدید** (معمولاً ترکیبات یا توان‌های ویژگی‌های موجود) به مجموعه داده، برای افزایش قابلیت مدل در یادگیری الگوهای پیچیده‌تر.

-----

**سوال:** مثالی از معادله یک مرز تصمیم دایره‌ای را برای رگرسیون لجستیک بنویسید.

**پاسخ:** $x\_1^2 + x\_2^2 - 1 = 0$.

-----

**سوال:** در مثال مرز تصمیم دایره‌ای ($x\_1^2 + x\_2^2 - 1 = 0$)، ضرایب ($w$) برای $x\_1^2$, $x\_2^2$, و عرض از مبدأ (جمله ثابت) چه خواهند بود؟

**پاسخ:** ضرایب $x\_1^2$ و $x\_2^2$ مقدار **۱** و برای عرض از مبدأ (جمله ثابت) مقدار **-۱** خواهند بود، و برای $x\_1$ و $x\_2$ صفر.

-----

**سوال:** در مدل با مرز تصمیم دایره‌ای، رگرسیون لجستیک برای نقاط داخل دایره چه مقداری را برمی‌گرداند؟

**پاسخ:** رگرسیون لجستیک برای تمام نقاط داخل دایره، مقداری **نزدیک به ۰** را برمی‌گرداند.

-----

**سوال:** در مدل با مرز تصمیم دایره‌ای، رگرسیون لجستیک برای نقاط خارج دایره چه مقداری را برمی‌گرداند؟

**پاسخ:** رگرسیون لجستیک برای تمام نقاط خارج دایره، مقداری **نزدیک به ۱** را برمی‌گرداند.

-----

**سوال:** تنها "پارامترهای قابل یادگیری" در رگرسیون لجستیک چه هستند؟

**پاسخ:** **وزن‌ها ($w\_0, w\_1, ..., w\_d$)** که شامل ضرایب ویژگی‌های اصلی، ویژگی‌های مرتبه بالاتر و عرض از مبدأ هستند.

-----

**سوال:** در بخش "Decision surface" فایل PDF، بیان شده که "Decision boundary hyperplane always has one less dimension than the feature space". این جمله چه معنایی دارد و چگونه با مثال‌های گرافیکی در صفحه 18 PDF ارتباط پیدا می‌کند؟

**پاسخ:** یعنی اگر فضای ویژگی‌ها $D$ بعد داشته باشد، مرز تصمیم‌گیری یک **هایپرپلین با بعد $D-1$** خواهد بود. در فضای ۲ بعدی (صفحه ۱۸)، مرز تصمیم یک شیء ۱ بعدی (خط) است که دو ناحیه را جدا می‌کند.

-----

### بخش ۵: تخمین پارامتر با استفاده از حداکثر درست‌نمایی (Maximum Likelihood Estimation - MLE)

-----

**سوال:** "تخمین (Estimation)" در زمینه آمار و یادگیری ماشین به چه معناست؟

**پاسخ:** فرآیند تخمین **پارامترهای یک توزیع کلی** از روی داده‌های نمونه‌برداری شده است.

-----

**سوال:** چرا "تخمین حداکثر درست‌نمایی (MLE)" یکی از بهترین روش‌ها برای تخمین پارامتر است؟

**پاسخ:** زیرا MLE به دنبال یافتن پارامترهایی است که باعث شود **احتمال مشاهده داده‌های واقعی (درست‌نمایی) حداکثر** شود.

-----

**سوال:** در مثال پرتاب سکه، هدف از تخمین پارامتر چه بود؟

**پاسخ:** هدف تخمین **احتمال شیر آمدن ($P$)** برای آن سکه، با توجه به توالی مشاهده شده از پرتاب‌ها بود.

-----

**سوال:** مفهوم "IID" (Independent and Identically Distributed) در آمار چه نقشی در MLE دارد؟

**پاسخ:** این فرض به ما اجازه می‌دهد که احتمال کلی مشاهده تمام نمونه‌ها را به صورت **حاصل‌ضرب احتمالات هر نمونه** بنویسیم، که محاسبات را ساده می‌کند.

-----

**سوال:** "تابع درست‌نمایی (Likelihood Function)" در رگرسیون لجستیک چگونه تعریف می‌شود؟

**پاسخ:** $L(w) = \\prod\_{i=1}^{n} P(y^{(i)}|x^{(i)}, w)$. این حاصل‌ضرب احتمال هر نمونه با پارامترهای $w$ است.

-----

**سوال:** چرا برای یافتن پارامترهای بهینه، به جای حداکثر کردن تابع درست‌نمایی، "**لگاریتم درست‌نمایی (Log-Likelihood)**" را حداکثر می‌کنیم؟

**پاسخ:** لگاریتم درست‌نمایی از **مشکلات عددی (مانند Underflow)** جلوگیری می‌کند و **حاصل‌ضرب‌ها را به جمع تبدیل می‌کند** که محاسبات را ساده‌تر می‌کند.

-----

**سوال:** ارتباط بین حداکثر کردن تابع درست‌نمایی و حداکثر کردن تابع لگاریتم درست‌نمایی چیست؟

**پاسخ:** از آنجایی که **تابع لگاریتم یک تابع اکیداً صعودی** است، حداکثر کردن لگاریتم درست‌نمایی معادل با حداکثر کردن خود درست‌نمایی است.

-----

**سوال:** فرمول تابع لگاریتم درست‌نمایی (Log-Likelihood Function) برای رگرسیون لجستیک چیست؟

**پاسخ:** $\\log L(w) = \\sum\_{i=1}^{n} [y^{(i)} \\log(\\sigma(w^T x^{(i)})) + (1-y^{(i)}) \\log(1-\\sigma(w^T x^{(i)}))]$.

-----

**سوال:** برای یافتن پارامترهای $w$ که تابع لگاریتم درست‌نمایی را حداکثر می‌کنند، چه گامی باید برداشته شود؟

**پاسخ:** باید **مشتق $\\log L(w)$ نسبت به $w$ را محاسبه کرده و آن را برابر با صفر** قرار دهیم.

-----

**سوال:** فرمول گرادیان تابع لگاریتم درست‌نمایی ($\\nabla\_w L(w)$) در رگرسیون لجستیک چیست؟

**پاسخ:** $\\nabla\_w L(w) = \\sum\_{i=1}^{n} (y^{(i)} - \\sigma(w^T x^{(i)})) x^{(i)}$.

-----

**سوال:** چرا برای $\\nabla\_w L(w) = 0$ در رگرسیون لجستیک، راه حل "**فرم بسته (Closed-form Solution)**" وجود ندارد؟

**پاسخ:** به دلیل **پیچیدگی تابع سیگموید** در معادله گرادیان، نمی‌توان $w$ را مستقیماً با یک فرمول جبری محاسبه کرد.

-----

**سوال:** فرمول $P(y^{(i)}|x^{(i)}, w) = \\sigma(w^T x^{(i)})^{y^{(i)}}(1-\\sigma(w^T x^{(i)}))^{(1-y^{(i)})}$ چگونه شرط "اگر $y^{(i)}=1$ بود، از $\\sigma$ استفاده کن و اگر $y^{(i)}=0$ بود، از $1-\\sigma$ استفاده کن" را بیان می‌کند؟

**پاسخ:** وقتی $y^{(i)}=1$، ترم دوم ($1-y^{(i)}$) صفر می‌شود و فقط $\\sigma$ باقی می‌ماند. وقتی $y^{(i)}=0$، ترم اول ($y^{(i)}$) صفر می‌شود و فقط $1-\\sigma$ باقی می‌ماند.

-----

**سوال:** اگر در مثال پرتاب سکه، ۱۶ بار سکه پرتاب شود و ۱۲ بار شیر بیاید، مقدار تخمینی $P$ چقدر خواهد بود؟

**پاسخ:** $P = \\frac{12}{16} = 0.75$.

-----

**سوال:** آیا MLE فقط برای تخمین پارامترهای توزیع برنولی (مانند پرتاب سکه) کاربرد دارد؟

**پاسخ:** خیر، MLE می‌تواند برای تخمین پارامترهای توزیع‌های مختلفی مانند نرمال، بتا و پواسون نیز استفاده شود.

-----

**سوال:** مشتق $\\log(\\sigma(z))$ نسبت به $z$ چیست؟

**پاسخ:** $1 - \\sigma(z)$.

-----

**سوال:** مشتق $\\log(1-\\sigma(z))$ نسبت به $z$ چیست؟

**پاسخ:** $-\\sigma(z)$.

-----

**سوال:** هدف نهایی از حداکثر کردن تابع درست‌نمایی در رگرسیون لجستیک چیست؟

**پاسخ:** یافتن مجموعه‌ای از وزن‌های $w$ که به بهترین وجه **احتمال مشاهده داده‌های آموزشی** ما را افزایش می‌دهند.

-----

**سوال:** چرا استفاده از لگاریتم، حاصل‌ضرب‌ها را به جمع تبدیل می‌کند؟

**پاسخ:** به دلیل خاصیت لگاریتم: $\\log(a \\cdot b) = \\log(a) + \\log(b)$.

-----

**سوال:** اگر تابع درستنمایی برای داده‌های آموزشی بسیار کم باشد، چه معنایی دارد؟

**پاسخ:** به این معنی است که مدل فعلی با پارامترهای فعلی، احتمال کمی برای مشاهده داده‌های آموزشی واقعی قائل است، و پارامترها باید بهبود یابند.

-----

**سوال:** مفهوم "IID" (Independent and Identically Distributed) در آمار چه نقشی در MLE دارد؟

**پاسخ:** این فرض به ما اجازه می‌دهد که احتمال کلی مشاهده تمام نمونه‌ها را به صورت **حاصل‌ضرب احتمالات هر نمونه** بنویسیم، که محاسبات را ساده می‌کند.

-----

### بخش ۶: تابع هزینه (Cost Function) و گرادیان در رگرسیون لجستیک

-----

**سوال:** فرم گرادیان در رگرسیون لجستیک ($\\sum\_{i=1}^{n} (\\sigma(w^T x^{(i)}) - y^{(i)}) x^{(i)}$) چه شباهتی به گرادیان در رگرسیون خطی دارد؟

**پاسخ:** هر دو شامل یک عبارت "**خطا**" هستند که تفاوت بین پیش‌بینی مدل و مقدار واقعی را نشان می‌دهد و این خطا در بردار ویژگی ضرب می‌شود.

-----

**سوال:** در رگرسیون لجستیک، عبارت $(\\sigma(w^T x^{(i)}) - y^{(i)})$ چه چیزی را نشان می‌دهد؟

**پاسخ:** این عبارت "**خطا**" یا "ارور" مدل را نشان می‌دهد؛ یعنی تفاوت بین پیش‌بینی احتمالی مدل و برچسب واقعی.

-----

**سوال:** چرا سادگی فرم گرادیان رگرسیون لجستیک از دلایل اصلی انتخاب تابع سیگموید است؟

**پاسخ:** اگر تابع دیگری استفاده می‌شد، گرادیان ممکن بود به قدری پیچیده شود که حل آن از نظر محاسباتی غیرممکن یا بسیار دشوار باشد.

-----

**سوال:** تابع هزینه‌ای که برای رگرسیون لجستیک استفاده می‌شود، چه نام دارد؟

**پاسخ:** "Binary Cross-Entropy Loss" (یا در حالت کلی‌تر "Cross-Entropy Loss").

-----

**سوال:** ویژگی کلیدی تابع هزینه Cross-Entropy چیست؟

**پاسخ:** وجود عبارت **لگاریتمی** است که مجازات مدل را برای پیش‌بینی‌های نادرست (به خصوص با اطمینان بالا) به شدت بزرگ می‌کند.

-----

**سوال:** "محدب بودن (Convexity)" تابع هزینه در رگرسیون لجستیک به چه معناست؟

**پاسخ:** به این معناست که تابع هزینه تنها یک "**بهینه سراسری (Global Optimum)**" دارد و هیچ "بهینه محلی (Local Optima)" در آن وجود ندارد.

-----

**سوال:** چگونه محدب بودن تابع هزینه در رگرسیون لجستیک از نظر ریاضی اثبات می‌شود؟

**پاسخ:** با بررسی **مشتق دوم (هسین)** تابع هزینه. مشتق دوم هر جمله همواره مثبت است و جمع توابع محدب نیز یک تابع محدب است.

-----

**سوال:** محدب بودن تابع هزینه چه تضمینی برای الگوریتم‌های بهینه‌سازی مبتنی بر گرادیان فراهم می‌کند؟

**پاسخ:** تضمین می‌کند که این الگوریتم‌ها (مانند گرادیان دیسنت) همیشه به سمت بهترین جواب (بهینه سراسری) همگرا می‌شوند و در نقاط بهینه محلی گیر نمی‌کنند.

-----

**سوال:** چرا استفاده از **خطای مربعات (Squared Error)** به عنوان تابع هزینه برای رگرسیون لجستیک مناسب نیست؟

**پاسخ:** زیرا منجر به یک تابع هزینه **نامحدب** با "چال‌چوله‌های" متعدد (چندین بهینه محلی) می‌شود، که گرادیان کاهشی ممکن است در آنها گیر کند.

-----

**سوال:** هدف نهایی در فرآیند بهینه‌سازی پارامترهای $w$ در رگرسیون لجستیک چیست؟

**پاسخ:** پیدا کردن پارامترهای $w$ به گونه‌ای که **تابع هزینه مدل حداقل شود**.

-----

**سوال:** از آنجایی که $w$ را نمی‌توان با یک فرمول بسته پیدا کرد، چه الگوریتمی برای به‌روزرسانی پارامترها استفاده می‌شود؟

**پاسخ:** الگوریتم "**گرادیان کاهشی (Gradient Descent)**".

-----

**سوال:** "قانون به‌روزرسانی (Update Rule)" برای پارامترها ($w$) در گرادیان کاهشی چگونه است؟

**پاسخ:** $w\_{جدید} = w\_{قدیم} - \\eta \\times \\text{گرادیان}$.

-----

**سوال:** $\\eta$ (اِتا) در قانون به‌روزرسانی گرادیان کاهشی چه چیزی را نشان می‌دهد؟

**پاسخ:** $\\eta$ "**نرخ یادگیری (Learning Rate)**" است که اندازه گام‌ها را در جهت گرادیان تعیین می‌کند.

-----

**سوال:** گرادیان محاسبه شده چه چیزی را نشان می‌دهد؟

**پاسخ:** گرادیان **جهت شیب** تابع هزینه را نشان می‌دهد.

-----

**سوال:** در گرادیان کاهشی، برای رسیدن به حداقل تابع، در چه جهتی حرکت می‌کنیم؟

**پاسخ:** در جهت "**منفی**" گرادیان حرکت می‌کنیم تا به حداقل تابع برسیم.

-----

**سوال:** چه اتفاقی می‌افتد اگر نرخ یادگیری ($\\eta$) خیلی بزرگ باشد؟

**پاسخ:** الگوریتم ممکن است از حداقل **بگذرد و "جهش" کند، یا حتی واگرا شود** (به همگرا نشود).

-----

**سوال:** چه اتفاقی می‌افافتد اگر نرخ یادگیری ($\\eta$) خیلی کوچک باشد؟

**پاسخ:** همگرایی الگوریتم به حداقل بسیار **کند** خواهد بود و زمان زیادی برای آموزش مدل لازم است.

-----

**سوال:** آیا "گرادیان کاهشی" تنها روش برای بهینه‌سازی در رگرسیون لجستیک است؟

**پاسخ:** خیر، روش‌های بهینه‌سازی دیگری مانند **گرادیان کاهشی تصادفی (Stochastic Gradient Descent - SGD)** و **گرادیان کاهشی دسته‌ای کوچک (Mini-Batch Gradient Descent)** نیز وجود دارند.

-----

**سوال:** تابع هزینه کراس-انتروپی چگونه جریمه‌ای را برای پیش‌بینی‌های مدل اعمال می‌کند؟

**پاسخ:** وقتی مدل به درستی پیش‌بینی می‌کند (مثلاً ۰.۹ برای کلاس ۱)، جریمه کم است. اما وقتی اشتباه می‌کند (مثلاً ۰.۱ برای کلاس ۱ در حالی که برچسب واقعی ۱ است)، **جریمه بسیار بزرگی** اعمال می‌شود.

-----

**سوال:** در نمودار تابع هزینه Binary Cross-Entropy، وقتی مقدار پیش‌بینی شده ($\\hat{y}$) و مقدار واقعی ($y$) متفاوت هستند، چه اتفاقی برای مقدار خطا (Loss) می‌افتد؟

**پاسخ:** وقتی پیش‌بینی از مقدار واقعی دور باشد (مثلاً $y=1$ و $\\hat{y}$ به ۰ نزدیک باشد)، **خطا به شدت زیاد می‌شود** و مدل را به تصحیح واداشته. اگر $y=0$ و $\\hat{y}$ به ۱ نزدیک باشد، باز هم خطا بسیار زیاد می‌شود.

-----

**سوال:** نرخ یادگیری ($\\eta$) در گرادیان دیسنت چه نقشی دارد و انتخاب نادرست آن چه تبعاتی می‌تواند داشته باشد؟

**پاسخ:** $\\eta$ اندازه گام به‌روزرسانی وزن‌ها را تعیین می‌کند. نرخ خیلی بزرگ منجر به **واگرایی** و نرخ خیلی کوچک منجر به **همگرایی بسیار کند** می‌شود.

-----

### بخش ۷: رگولاریزاسیون (Regularization)

-----

**سوال:** نقش **رگولاریزاسیون (Regularization)** در رگرسیون لجستیک چیست و چرا ضروری است؟

**پاسخ:** از **بیش‌برازش (overfitting)** جلوگیری می‌کند، یعنی نمی‌گذارد مدل بیش‌ازحد روی داده‌های آموزش منطبق شود و تعمیم‌پذیری‌اش را کاهش دهد.

-----

**سوال:** در رگرسیون لجستیک، معمولاً از چه نوع رگولاریزاسیونی استفاده می‌شود و هدف هر یک چیست؟

**پاسخ:**

  * **L2 Regularization (Ridge):** برای کوچک نگه‌داشتن وزن‌ها.
  * **L1 Regularization (Lasso):** برای حذف ویژگی‌های کم‌اثر (sparsity) و انتخاب ویژگی.

-----

**سوال:** پارامتر `C` در پیاده‌سازی `LogisticRegression` در Scikit-learn چه نقشی دارد؟

**پاسخ:** `C` معکوس شدت رگولاریزاسیون است؛ `C` بزرگتر یعنی رگولاریزاسیون کمتر، و `C` کوچکتر یعنی رگولاریزاسیون بیشتر.

-----

### بخش ۸: Softmax Regression و دسته‌بندی چندکلاسه

-----

**سوال:** **Softmax Regression** چیست و چه زمانی از آن استفاده می‌شود؟

**پاسخ:** تعمیم رگرسیون لجستیک برای **مسائل طبقه‌بندی چندکلاسه (Multi-class Classification)** است، یعنی زمانی که بیش از دو کلاس وجود دارد و هر نمونه تنها به یکی از کلاس‌ها تعلق دارد.

-----

**سوال:** تابع Softmax چگونه کار می‌کند و فرمول آن چیست؟

**پاسخ:** برای هر کلاس $k$، یک بردار وزن $w\_k$ دارد. امتیاز خطی $w\_k^T x$ را به احتمالاتی تبدیل می‌کند که جمعشان برابر ۱ است: $P(y=k|x) = \\frac{\\exp(w\_k^T x)}{\\sum\_{j=1}^K \\exp(w\_j^T x)}$.

-----

**سوال:** تفاوت بین **Softmax Regression و One-vs-Rest (OvR)** در مسائل چندکلاسه چیست؟

**پاسخ:** در **OvR**، یک مدل دودویی برای هر کلاس (کلاس i در برابر بقیه) ساخته می‌شود. در **Softmax Regression**، همه کلاس‌ها هم‌زمان در یک مدل در نظر گرفته می‌شوند و خروجی یک توزیع احتمالاتی منسجم بین کلاس‌هاست.

-----

**سوال:** در Softmax Regression، منظور از **One-of-K encoding** برای برچسب‌های خروجی چیست؟ مثالی بزنید.

**پاسخ:** هر برچسب کلاس به صورت یک **بردار باینری (صفر و یک)** با طول برابر تعداد کل کلاس‌ها (K) نمایش داده می‌شود، که تنها یک عنصر آن (کلاس صحیح) برابر ۱ و بقیه صفر هستند. مثال: برای کلاس ۳ از ۴ کلاس: $[0,0,1,0]^T$.

-----

**سوال:** چرا Softmax Function در طبقه‌بندی چندکلاسه به جای تابع Max(.) استفاده می‌شود، در حالی که هر دو نهایتاً به انتخاب کلاس با بیشترین امتیاز منجر می‌شوند؟

**پاسخ:** Softmax **هموار و مشتق‌پذیر** است که امکان بهینه‌سازی با گرادیان را فراهم می‌کند. همچنین خروجی آن **احتمالاتی** است که جمعشان ۱ می‌شود، برخلاف Max(.) که فقط کلاس برنده را می‌دهد.

-----

### بخش ۹: تفسیرپذیری و محدودیت‌ها

-----

**سوال:** **آیا Logistic Regression یک مدل خطی است یا غیرخطی؟**

**پاسخ:** از نظر **فضای ویژگی‌ها**، Logistic Regression یک مدل **خطی** است چون تابع تصمیم آن $w^T x + b$ است. اما خروجی نهایی پس از اعمال تابع سیگموید، **غیرخطی** نسبت به $z$ است.

-----

**سوال:** اگر خروجی Logistic Regression برابر 0.7 باشد، چگونه تفسیر می‌شود؟

**پاسخ:** یعنی مدل با احتمال **۷۰٪ پیش‌بینی می‌کند که نمونه به کلاس مثبت (کلاس ۱) تعلق دارد**. این احتمال را می‌توان برای تصمیم‌گیری یا رتبه‌بندی بین نمونه‌ها استفاده کرد.

-----

**سوال:** چه زمانی استفاده از Logistic Regression مناسب نیست؟

**پاسخ:** وقتی داده‌ها **خطی قابل جداسازی نیستند**، مسئله دارای کلاس‌های زیادی با توزیع پیچیده است، یا نسبت کلاس‌ها بسیار نابرابر (class imbalance شدید) باشد.

-----

**سوال:** چه تفاوتی بین مدل **Discriminative (تمایزی)** و **Generative (مولد)** وجود دارد؟ و Logistic Regression در کدام دسته است؟

**پاسخ:** Discriminative مستقیماً $P(y|x)$ را مدل می‌کنند (مثل Logistic Regression). Generative ابتدا $P(x|y)$ و $P(y)$ را مدل می‌کنند و سپس از قانون بیز برای محاسبه $P(y|x)$ استفاده می‌شود (مثل Naive Bayes). Logistic Regression یک مدل **تمایزی (Discriminative)** است.

-----

**سوال:** در Logistic Regression، تابع هزینه چه ویژگی مهمی دارد؟

**پاسخ:** تابع هزینه (Negative Log-Likelihood) یک تابع **محدب (Convex)** است. این یعنی الگوریتم‌های بهینه‌سازی با اطمینان به **کمینه سراسری (Global Minimum)** می‌رسند.

-----

**سوال:** آیا Logistic Regression قابلیت تفسیر دارد؟ چه مزیتی دارد؟

**پاسخ:** بله\! یکی از بزرگ‌ترین مزایای آن است. هر ویژگی $x\_j$ دارای ضریب $w\_j$ است که می‌توان تفسیر کرد: وزن مثبت نشان‌دهنده تأثیر افزایشی بر احتمال کلاس مثبت است.

-----

**سوال:** اگر وزن یکی از ویژگی‌ها در Logistic Regression صفر باشد، چه برداشتی می‌توان داشت؟

**پاسخ:** اگر $w\_j = 0$، یعنی مدل به این ویژگی $x\_j$ برای تصمیم‌گیری اهمیتی نمی‌دهد. در صورت استفاده از L1 Regularization، این می‌تواند نشانه‌ای از حذف ویژگی بی‌اثر باشد.

-----

**سوال:** آیا Logistic Regression برای مسائل زمانی (مثل پیش‌بینی سری زمانی) مناسب است؟

**پاسخ:** به‌صورت مستقیم خیر. Logistic Regression فرض می‌کند داده‌ها مستقل و هم‌توزیع‌اند. اما با اضافه کردن ویژگی‌های زمانی (مانند lagها)، می‌توان از آن به عنوان یک مدل پایه استفاده کرد.

-----

**سوال:** چرا می‌گوییم Logistic Regression در مرز بین مدل‌های آماری و یادگیری ماشین قرار دارد؟

**پاسخ:** چون هم از نظر آماری **تفسیرپذیر** است (با پارامترهای معنی‌دار) و هم در چارچوب یادگیری ماشین قابلیت **یادگیری از داده** را دارد.

-----

**سوال:** چه فرض‌هایی پشت مدل Logistic Regression نهفته است؟

**پاسخ:** نمونه‌ها مستقل هستند، رابطه بین log-odds خروجی و ویژگی‌ها **خطی** است، هیچ هم‌خطی شدیدی بین ویژگی‌ها وجود ندارد و خروجی فقط دو کلاس دارد.

-----

**سوال:** آیا Logistic Regression نسبت به **missing values (مقادیر گمشده)** مقاوم است؟

**پاسخ:** خیر. Logistic Regression نمی‌تواند با داده‌های ناقص کار کند و باید قبل از آموزش، پیش‌پردازش (مانند Imputation) انجام شود.

-----

**سوال:** چه روشی برای ارزیابی اهمیت ویژگی‌ها در Logistic Regression وجود دارد؟

**پاسخ:** بررسی قدر مطلق وزن‌ها ($|w|$): وزن بزرگ‌تر → اهمیت بیشتر. همچنین می‌توان از تحلیل آماری مانند **Wald Test** یا **Likelihood Ratio Test** استفاده کرد.

-----

**سوال:** تفاوت **log-odds (لوگیت)** و احتمال خروجی در Logistic Regression چیست؟

**پاسخ:** **احتمال خروجی ($h(x)$)** مقداری بین ۰ و ۱ است. **Log-odds** برابر است با $\\log\\left( \\frac{p}{1 - p} \\right) = w^T x$. این مقدار می‌تواند منفی یا مثبت باشد و تفسیر آن در تحلیل آماری مفید است.

-----

**سوال:** در رگرسیون لجستیک، عبارت $w^T x$ قبل از اعمال تابع سیگموید، چه مفهومی دارد؟

**پاسخ:** به آن **امتیاز (Score)** یا **لوگیت (Logit)** می‌گویند. این مقدار نشان‌دهنده «شواهد» یا «قدرت» تعلق یک نمونه به کلاس مثبت است.

-----

**سوال:** چرا در تابع هزینه رگرسیون لجستیک از لگاریتم استفاده می‌شود؟

**پاسخ:** برای **پایداری عددی (جلوگیری از Underflow)** و **ساده‌سازی مشتق‌گیری** (تبدیل حاصل‌ضرب‌ها به جمع).

-----

**سوال:** **Classification Problem** در فایل PDF چگونه توضیح داده شده است؟ یک مثال از این نوع مسائل را بیان کنید.

**پاسخ:** مسئله‌ای که در آن خروجی مدل (برچسب $y$) یک مقدار گسسته (معمولاً ۰ یا ۱) است. مثال: تشخیص اسپم/غیر اسپم، بیمار/سالم.

-----

**سوال:** رگرسیون لجستیک به عنوان یک "**Linear Classifier**" طبقه‌بندی شده است. این به چه معناست؟

**پاسخ:** به این معناست که مرز تصمیم آن یک **خط، صفحه یا هایپرپلین خطی** در فضای ویژگی‌ها است که داده‌ها را به دو کلاس جدا می‌کند.

-----

### بخش ۱۰: مسائل پیشرفته و کاربردی

-----

**سوال:** آیا Logistic Regression می‌تواند احتمال دقیق را مدل کند؟

**پاسخ:** خیر، خروجی آن **تخمینی از احتمال** است. برای کاربردهای حساس، ممکن است نیاز به **کالیبراسیون احتمال** (مثلاً با Platt Scaling) باشد.

-----

**سوال:** چرا Logistic Regression نسبت به **خطاهای برچسب‌گذاری (Labeling Errors)** حساس است؟

**پاسخ:** چون مدل به ازای هر نمونه بهینه‌سازی انجام می‌دهد. اگر برچسب اشتباه باشد، مدل یاد می‌گیرد که مقدار احتمال را در جهت نادرست حرکت دهد، که منجر به کاهش دقت کلی می‌شود.

-----

**سوال:** چه ارتباطی بین **Maximum Likelihood Estimation (MLE)** و Logistic Regression وجود دارد؟

**پاسخ:** **پارامترهای Logistic Regression با روش MLE آموزش داده می‌شوند**. در این روش، تابع درست‌نمایی کل داده‌ها را بیشینه می‌کنیم.

-----

**سوال:** آیا Logistic Regression به **outlier (نقاط پرت)** در ویژگی‌ها حساس است؟ چرا؟

**پاسخ:** بله. اگر در ورودی (X) داده‌هایی با مقدارهای بسیار بزرگ یا دورافتاده وجود داشته باشد، این مقادیر می‌توانند اثر بسیار بزرگی در تصمیم‌گیری داشته باشند، چون مدل خطی است.

-----

**سوال:** Logistic Regression را با **Decision Tree** مقایسه کن. چه زمانی کدام بهتر است؟

**پاسخ:** Logistic Regression **پارامتریک، خطی و تفسیرپذیر** است (برای داده‌های ساده). Decision Tree **غیرپارامتریک و انعطاف‌پذیر** است (برای داده‌های پیچیده با تعامل ویژگی‌ها).


-----

**سوال:** چگونه می‌توان عملکرد Logistic Regression را روی **داده‌های نامتوازن (Imbalanced Data)** بهبود داد؟

**پاسخ:** استفاده از `class_weight='balanced'`، بازنمونه‌گیری (Oversampling/Undersampling)، استفاده از معیارهای مناسب مانند AUC، F1-score، و تنظیم آستانه تصمیم‌گیری.

-----

**سوال:** آیا می‌توان از Logistic Regression برای **رگرسیون چندبرچسبی (Multi-label Regression)** استفاده کرد؟ چگونه؟

**پاسخ:** بله، با استفاده از رویکرد **One-vs-Rest (OvR)**، می‌توان برای هر برچسب یک Logistic Regression مستقل آموزش داد.

-----

**سوال:** اگر ویژگی‌ها دارای مقدار **گمشده (missing values)** باشند، Logistic Regression چطور با آن‌ها برخورد می‌کند؟

**پاسخ:** به‌صورت پیش‌فرض نمی‌تواند با آن‌ها کار کند. باید از تکنیک‌های **Imputation** (مانند میانگین، میانه، KNN) استفاده کرد.

-----

**سوال:** Logistic Regression در حالتی که کلاس‌ها به‌صورت **همپوشانی (overlapping)** دارند چگونه عمل می‌کند؟

**پاسخ:** در چنین حالتی مدل نمی‌تواند همه نمونه‌ها را ۱۰۰٪ درست طبقه‌بندی کند و احتمال‌ها را **نرم‌تر و محتاطانه‌تر** تولید می‌کند. هدفش یافتن مرز تصمیم بهینه از نظر احتمال درست‌نمایی است.

-----

**سوال:** آیا Logistic Regression می‌تواند **non-convex (غیرمحدب)** شود؟

**پاسخ:** خیر، در فرم کلاسیک آن با تابع هزینه log-loss و تابع سیگموید، تابع هزینه همیشه **محدب (convex)** است.

-----

**سوال:** چه زمانی باید از Logistic Regression به مدل‌های پیچیده‌تر مثل SVM یا Random Forest مهاجرت کنیم؟

**پاسخ:** وقتی مرز تصمیم به وضوح **غیرخطی** است، Logistic Regression underfit می‌کند، یا داده دارای تعاملات پیچیده بین ویژگی‌هاست.

-----

**سوال:** Logistic Regression در برابر **نویز (Noise)** چقدر مقاوم است؟

**پاسخ:** ذاتاً مقاوم‌تر از مدل‌های با مرز سخت است، اما در صورت نویز زیاد یا برچسب‌های اشتباه، مدل دچار خطا می‌شود و بهتر است از Regularization استفاده شود.

-----

**سوال:** آیا Logistic Regression به **هم‌خطی (Multicollinearity)** حساس است؟

**پاسخ:** بله. هم‌خطی شدید می‌تواند وزن‌های مدل را ناپایدار کند و تفسیر آن را سخت سازد. راه‌حل‌ها شامل حذف ویژگی‌های وابسته یا استفاده از Regularization است.

-----

**سوال:** چرا تابع هزینه Logistic Regression محدب است و چرا مهم است؟

**پاسخ:** چون تابع هزینه مبتنی بر Log Loss است و سیگموید، تابعی با مشتق یکنواخت است، نتیجه نهایی **محدب (Convex)** خواهد بود. این یعنی روش‌هایی مثل گرادیان دیسنت بدون گیر افتادن در مینیمم محلی می‌توانند آن را بیابند.

-----

**سوال:** در چه شرایطی Logistic Regression دچار **Underfitting** می‌شود؟

**پاسخ:** زمانی که رابطه بین ویژگی‌ها و برچسب خروجی **غیرخطی** باشد، یا تعداد ویژگی‌ها کافی نباشد، مدل نمی‌تواند ساختار داده را یاد بگیرد.

-----

**سوال:** در چه شرایطی Logistic Regression دچار **Overfitting** می‌شود؟

**پاسخ:** زمانی که مدل بیش‌ازحد پیچیده باشد (مثلاً با ویژگی‌های زیاد) یا داده‌های آموزشی کافی نباشد و روی نویز داده آموزش ببیند.

-----

**سوال:** اگر مدل Logistic Regression شما فقط روی train خوب باشد ولی test عملکرد بدی داشته باشد، چه باید کرد؟

**پاسخ:** این یعنی مدل دچار **Overfitting** شده است. راه‌حل‌ها: اعمال Regularization قوی‌تر، Cross-validation، کاهش پیچیدگی مدل، یا افزایش حجم داده آموزشی.

-----

**سوال:** در Scikit-learn چطور یک مدل Logistic Regression پیاده‌سازی می‌شود؟ (فقط کد اصلی را ذکر کنید).

**پاسخ:**

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

-----

**سوال:** اگر بخواهید Logistic Regression را روی داده‌های بسیار بزرگ (Big Data) پیاده‌سازی کنید، چه نکاتی را رعایت می‌کنید؟

**پاسخ:** استفاده از **Stochastic Gradient Descent (SGD)** یا **mini-batch** برای کاهش مصرف حافظه، و استفاده از `solver='sag'` یا `solver='saga'` در scikit-learn.

-----

**سوال:** آیا Logistic Regression قابلیت تفسیر دارد؟ چه مزیتی دارد؟

**پاسخ:** بله\! هر ویژگی $x\_j$ دارای ضریب $w\_j$ است که می‌توان تفسیر کرد: وزن بزرگ‌تر تأثیر بیشتری دارد. این تفسیر ساده برای کاربردهای حساس بسیار مهم است.

-----

**سوال:** چه معیاری برای ارزیابی مدل Logistic Regression پیشنهاد می‌شود؟

**پاسخ:** اگر داده متوازن باشد: **Accuracy**؛ اگر داده نامتوازن است: **F1-score، Precision، Recall، ROC Curve و AUC**.

-----

**سوال:** چرا خروجی Logistic Regression نمی‌تواند به‌صورت مستقیم برای طبقه‌بندی استفاده شود؟

**پاسخ:** خروجی یک **احتمال بین ۰ و ۱** است، نه یک پیش‌بینی قطعی کلاس. برای طبقه‌بندی نهایی، باید از یک **آستانه (threshold)** (معمولاً ۰.۵) استفاده کرد.

-----

**سوال:** در Logistic Regression، چرا از تابع سیگموید به‌جای تابع tanh استفاده می‌شود؟

**پاسخ:** تابع tanh خروجی را در بازه \[-1, 1] نگاشت می‌دهد، در حالی که Logistic Regression برای مدل‌سازی احتمال به تابعی نیاز دارد که مقدارش در **\[0,1]** باشد، که سیگموید این ویژگی را دارد.

-----

**سوال:** چه نوع solverهایی برای Logistic Regression وجود دارد و چه زمانی باید از هرکدام استفاده کرد؟

**پاسخ:** `'liblinear'` (برای L1 و مسائل کوچک)، `'saga'` (برای L1 و داده‌های بزرگ)، `'sag'` و `'lbfgs'` (برای مسائل بزرگ و L2).

-----

**سوال:** چرا Logistic Regression در مسائل **high-dimensional (ویژگی‌های زیاد، داده‌ی کم)** خوب عمل نمی‌کند؟

**پاسخ:** در فضاهای با ویژگی‌های زیاد، مدل به‌راحتی **overfit** می‌کند، به‌ویژه اگر تعداد داده‌ها کم باشد. **Regularization** و کاهش ابعاد کلیدی هستند.

-----

**سوال:** آیا می‌توان Logistic Regression را به صورت **آنلاین (online learning)** آموزش داد؟ چگونه؟

**پاسخ:** بله، با استفاده از **Stochastic Gradient Descent (SGD)** می‌توان Logistic Regression را به‌صورت آنلاین یاد گرفت و وزن‌ها را با دیدن هر داده جدید به‌روزرسانی کرد.

-----

**سوال:** **Softmax Function** در طبقه‌بندی چندکلاسه به جای تابع Max(.) استفاده می‌شود، در حالی که هر دو نهایتاً به انتخاب کلاس با بیشترین امتیاز منجر می‌شوند. چرا؟

**پاسخ:** Softmax **هموار و مشتق‌پذیر** است که امکان استفاده از گرادیان دیسنت را می‌دهد. همچنین خروجی آن به صورت **توزیع احتمالاتی** (جمعشان ۱) است که Max(.) نمی‌دهد.

-----

### بخش ۱۱: دیدگاه‌های آماری و ریاضیاتی

-----

**سوال:** در فرمول‌بندی برآورد بیشینه درست‌نمایی (MLE) برای رگرسیون لجستیک، چرا ابتدا از حاصل‌ضرب احتمالات $P(y^{(i)}|x^{(i)}, w)$ استفاده می‌شود و سپس به لگاریتم آن (log-likelihood) تغییر می‌کند؟ مزیت لگاریتم گرفتن چیست؟

**پاسخ:** حاصل‌ضرب احتمالات کوچک منجر به **Underflow** می‌شود. لگاریتم این حاصل‌ضرب را به **جمع** تبدیل می‌کند که از نظر عددی پایدارتر است و مشتق‌گیری را ساده‌تر می‌کند.

-----

**سوال:** چگونه محدب بودن تابع هزینه رگرسیون لجستیک اثبات می‌شود؟ چرا این ویژگی تضمین‌کننده همگرایی به بهینه‌ی سراسری است؟

**پاسخ:** محدب بودن ناشی از این است که تابع هزینه مجموع چندین تابع محدب است. در توابع محدب، هر بهینه‌ی محلی در واقع **بهینه‌ی سراسری** است، بنابراین گرادیان دیسنت همیشه به بهینه سراسری همگرا می‌شود.

-----

**سوال:** در رگرسیون لجستیک، عبارت $w^T x$ قبل از اعمال تابع سیگموید، چه مفهومی دارد؟

**پاسخ:** به آن **امتیاز (Score)** یا **لوگیت (Logit)** می‌گویند. این مقدار نشان‌دهنده «شواهد» یا «قدرت» تعلق یک نمونه به کلاس مثبت است.

-----

**سوال:** نرخ یادگیری ($\\eta$) در گرادیان دیسنت چه نقشی دارد و انتخاب نادرست آن چه تبعاتی می‌تواند داشته باشد؟

**پاسخ:** $\\eta$ اندازه **گام به‌روزرسانی وزن‌ها** را تعیین می‌کند. نرخ خیلی بزرگ منجر به **واگرایی** و نرخ خیلی کوچک منجر به **همگرایی بسیار کند** می‌شود.

-----

**سوال:** در نمودار تابع هزینه Binary Cross-Entropy، وقتی مقدار پیش‌بینی شده ($\\hat{y}$) و مقدار واقعی ($y$) متفاوت هستند، چه اتفاقی برای مقدار خطا (Loss) می‌افتد؟

**پاسخ:** وقتی پیش‌بینی از مقدار واقعی دور باشد (مثلاً $y=1$ و $\\hat{y}$ به ۰ نزدیک باشد)، **خطا به شدت زیاد می‌شود** و مدل را به تصحیح واداشته. اگر $y=0$ و $\\hat{y}$ به ۱ نزدیک باشد، باز هم خطا بسیار زیاد می‌شود.

-----

**سوال:** در Softmax Regression (رگرسیون لجستیک چندکلاسه)، منظور از **One-of-K encoding** برای برچسب‌های خروجی چیست؟ مثالی بزنید.

**پاسخ:** نمایش هر برچسب کلاس به صورت یک **بردار باینری** با طول K (تعداد کلاس‌ها) که فقط یک عنصر آن (کلاس صحیح) برابر ۱ و بقیه صفرند. مثال: برای کلاس ۲ از ۳ کلاس: $[0,1,0]^T$.

-----

**سوال:** در دیدگاه احتمالاتی، مدل‌های **Generative** پس از محاسبه $P(x|C\_k)$ و $P(C\_k)$ چگونه به مرحله "تصمیم‌گیری" می‌رسند؟

**پاسخ:** با استفاده از **قانون بیز (Bayes' Rule)**، احتمال پسین $P(C\_k|x)$ را محاسبه کرده و نمونه را به کلاسی اختصاص می‌دهند که بیشترین احتمال پسین را داشته باشد.

-----

**سوال:** مفهوم "**Random Variable (متغیر تصادفی)**" در زمینه ویژگی‌ها و برچسب کلاس در رگرسیون لجستیک چیست؟

**پاسخ:** ویژگی‌ها و برچسب‌ها به دلیل **عدم قطعیت** مقادیرشان، متغیر تصادفی هستند و با توزیع‌های احتمالی مدل می‌شوند. هدف مدل کاهش این عدم قطعیت با پیش‌بینی محتمل‌ترین کلاس است.

-----

**سوال:** فایل PDF در بخش "Discriminative vs. Generative: example" یک مثال جدولی از $P(x,y)$ و $P(y|x)$ ارائه می‌دهد. با توجه به این مثال، اگر یک نمونه جدید با $x=1$ به مدل داده شود، مدل Discriminative و مدل Generative چه پیش‌بینی خواهند داشت؟

**پاسخ:** هر دو مدل **$y=0$** را پیش‌بینی می‌کنند، زیرا $P(y=0|x=1)=1$ در هر دو حالت است. تفاوت در روش رسیدن به این نتیجه است (یکی مستقیم، دیگری از طریق توزیع توأم).




---

**سوال:** پرسپترون چیست؟

**پاسخ:** پرسپترون یک نورون مصنوعی ساده است که ورودی‌ها را وزن‌دار می‌کند، با بایاس جمع می‌کند و خروجی آن را با یک تابع فعال‌سازی تعیین می‌نماید.

---

**سوال:** فرمول کلی پرسپترون چگونه بیان می‌شود؟

**پاسخ:**
$y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)$
که در آن $f$ تابع فعال‌سازی است و $w_i$، وزن‌های ورودی هستند.

---

**سوال:** اگر تابع فعال‌سازی از پرسپترون حذف شود، چه پیامدی دارد؟

**پاسخ:** مدل فقط می‌تواند روابط خطی را یاد بگیرد و از درک الگوهای غیرخطی عاجز می‌ماند.

---

**سوال:** چرا استفاده از تابع فعال‌سازی در شبکه‌های عصبی حیاتی است؟

**پاسخ:** توابع فعال‌سازی مدل را از خطی بودن خارج می‌کنند و امکان یادگیری الگوهای پیچیده و غیرخطی را فراهم می‌سازند.

---

**سوال:** چند نمونه از توابع فعال‌سازی رایج را نام ببرید.

**پاسخ:** ReLU، Sigmoid، Tanh، Softmax و GELU از جمله توابع فعال‌سازی پرکاربرد هستند.

---

**سوال:** چه تفاوتی بین تابع ReLU و Sigmoid وجود دارد؟

**پاسخ:** ReLU خروجی‌های منفی را صفر و مقادیر مثبت را به همان شکل حفظ می‌کند، در حالی که Sigmoid خروجی را بین ۰ تا ۱ نرمال می‌سازد.

---

**سوال:** اگر شبکه‌ای فقط لایه‌های خطی بدون تابع فعال‌سازی داشته باشد، نتیجه چیست؟

**پاسخ:** حتی با چندین لایه، مدل نهایی صرفاً یک تبدیل خطی خواهد بود و از نظر توانایی یادگیری محدود باقی می‌ماند.

---

**سوال:** تابع Softmax چه کاربردی دارد؟

**پاسخ:** خروجی مدل را به توزیع احتمالاتی روی کلاس‌های مختلف در مسائل چندکلاسه تبدیل می‌کند، طوری که مجموع خروجی‌ها برابر ۱ باشد.

---

**سوال:** تابع فعال‌سازی پیش‌فرض در پرسپترون کلاسیک چیست؟

**پاسخ:** تابع پله‌ای (Step Function) که خروجی را صفر یا یک می‌کند.

---

**سوال:** تفاوت اصلی بین پرسپترون و پرسپترون چندلایه (MLP) چیست؟

**پاسخ:** پرسپترون تنها یک لایه دارد و برای مسائل خطی کافی است؛ ولی MLP چند لایه و تابع غیرخطی دارد و مسائل پیچیده‌تری را حل می‌کند.

---

**سوال:** اگر در شبکه به جای ReLU از تابع خطی استفاده شود، چه رخ می‌دهد؟

**پاسخ:** کل شبکه مثل یک مدل رگرسیون خطی عمل خواهد کرد و نمی‌تواند الگوهای پیچیده را یاد بگیرد.

---

**سوال:** خروجی تابع ReLU برای مقادیر منفی چیست؟

**پاسخ:** صفر

---

**سوال:** نقش بایاس (bias) در پرسپترون چیست؟

**پاسخ:** بایاس به مدل اجازه می‌دهد تا تابع تصمیم را جابه‌جا کرده و انعطاف‌پذیری بیشتری در یادگیری داشته باشد.

---

**سوال:** اشکال تابع Sigmoid در شبکه‌های عمیق چیست؟

**پاسخ:** گرادیان آن در ورودی‌های بزرگ یا کوچک بسیار کوچک می‌شود و باعث مشکل **vanishing gradient** می‌گردد.

---

**سوال:** فرق ReLU و GELU در چیست؟

**پاسخ:** GELU نسخه نرم‌تر و پیوسته‌تری از ReLU است و در بسیاری از مدل‌های مدرن مانند Transformer عملکرد بهتری دارد.

---

**سوال:** چرا شبکه‌های عصبی عمیق به توابع غیرخطی نیاز دارند؟

**پاسخ:** چون برای مدل‌سازی روابط پیچیده و غیرخطی در داده‌های واقعی، توابع غیرخطی ضروری هستند.

---

**سوال:** در چه مواقعی استفاده از تابع فعال‌سازی خطی مناسب است؟

**پاسخ:** معمولاً در لایه خروجی مدل‌های **رگرسیونی** که خروجی بدون محدودیت عددی دارند، از تابع خطی استفاده می‌شود.

---

**سوال:** آیا بدون تابع فعال‌سازی، شبکه قادر به طبقه‌بندی تصاویر خواهد بود؟

**پاسخ:** خیر، حذف تابع فعال‌سازی باعث می‌شود شبکه نتواند ویژگی‌های پیچیده تصویر را یاد بگیرد.

---

**سوال:** چه زمانی باید از Softmax استفاده کرد؟

**پاسخ:** در لایه خروجی مدل‌های دسته‌بندی چندکلاسه برای تبدیل نمرات خام به مقادیر قابل تفسیر احتمالاتی.

---

**سوال:** آیا پرسپترون ساده می‌تواند مسئله XOR را حل کند؟ چرا؟

**پاسخ:** خیر، چون XOR یک مسئله غیرخطی است و پرسپترون فقط قادر به یادگیری روابط خطی است؛ برای حل آن باید از چند لایه و توابع غیرخطی استفاده کرد.

---

