**مقدمه‌ای بر رگرسیون لجستیک (Logistic Regression)**

**مدرس:** علی شریفی-زارچی
**دپارتمان:** مهندسی کامپیوتر
**دانشگاه:** صنعتی شریف
**تاریخ:** 5 اکتبر 2024
**درس:** یادگیری ماشین (CE 40717)

---
**رگرسیون لجستیک: مباحث اولیه و پیش‌پردازش داده**

در جلسه گذشته، مروری بر تکنیک‌های دسته‌بندی (classification) انجام شد و برای اولین بار با مسئله دسته‌بندی آشنا شدیم. [cite_start]برخلاف مسئله رگرسیون که هدف، پیش‌بینی یک مقدار پیوسته بود، در دسته‌بندی هدف پیش‌بینی مجموعه‌ای از کلاس‌ها است[cite: 49]. این کلاس‌ها می‌توانند حالات مختلفی داشته باشند، مانند بیمار/سالم، سگ/گربه، فروش رفتن/نرفتن خانه، خوش‌حساب/بدحساب بودن مشتری، پرداخت وام/عدم پرداخت وام، اسپم بودن/نبودن ایمیل، یا حاوی محتوای توهین‌آمیز بودن/نبودن یک متن، و بسیاری موارد دیگر.

در جلسات قبلی، در ساده‌ترین مدل‌های دسته‌بندی، برچسب کلاس‌ها را منفی یک و یک در نظر می‌گرفتیم. [cite_start]اما در این جلسه، به خاطر سادگی و دلایل ریاضیاتی مدل، برچسب کلاس‌ها از این به بعد 0 و 1 در نظر گرفته می‌شود[cite: 57, 58, 59]. این یک تفاوت اولیه است که باید به آن توجه داشت.

**پرسش محوری:**
در جلسات قبل، با دسته‌بند "پرسپترون" آشنا شدیم که ساختار شبکه عصبی داشت. اما این مدل اشکالی داشت: خروجی آن شبیه یک تابع پله‌ای بود که از یک نقطه خاص (مثلاً صفر) به بعد، کلاس را 1 و قبل از آن، کلاس را منفی 1 در نظر می‌گرفت. [cite_start]عملاً از علامت (sign) استفاده می‌شد[cite: 81, 82]. واقعیت این است که در بسیاری از مواقع، شما یک "درجه اطمینان" دارید. به عنوان مثال، برای یک بیمار، ممکن است 100% مطمئن باشید که سرطان دارد، یا 100% مطمئن باشید که سالم است. اما اغلب اوقات، مقادیری بین این دو حالت وجود دارد. مثلاً، نه می‌توانید 100% مطمئن باشید که فرد سرطان دارد و نه 100% مطمئن باشید که سالم است؛ شاید 70% احتمال می‌دهید سرطان داشته باشد یا 80% احتمال می‌دهید سالم باشد.

**سوال این است که چگونه می‌توانیم این موضوع را مدل‌سازی کنیم؟** چگونه الگوریتم خود را توسعه دهیم تا بتواند یک نوع "احتمال" (مانند احتمال بیمار بودن یا سالم بودن) را به دست آورد؟ این رویکرد، بسیار متفاوت از تابع پله‌ای است که در جلسه قبل دیدیم. ما به دنبال تابعی هستیم که خروجی آن بین 0 و 1 باشد، به گونه‌ای که اگر ورودی در یک ناحیه خاص قرار گیرد، خروجی به 0 نزدیک باشد (مثلاً 100% سالم)، و اگر در ناحیه دیگر باشد، خروجی به 1 نزدیک باشد (مثلاً 100% بیمار). اما در نقاط میانی، خروجی باید مقادیر بین 0 و 1 را بگیرد (مثلاً 0.4 یا 0.6).

**مشکل رگرسیون خطی برای مسائل دسته‌بندی:**
یک راه حل ساده این است که به سراغ رگرسیون خطی برویم، شبیه همان مدلی که قبلاً داشتیم. اگر داده‌هایی با برچسب 0 و 1 داشته باشیم، می‌توانیم یک خط به آنها برازش (fit) دهیم تا حداقل خطای مربعات را داشته باشد. اما مشکل اینجاست که اگر یک نقطه جدید (outlier) به داده‌ها اضافه شود، خط برازش یافته به شدت تغییر می‌کند. این تغییر می‌تواند باعث شود که بسیاری از نقاطی که قبلاً به درستی دسته‌بندی شده بودند، اکنون به اشتباه دسته‌بندی شوند (مثلاً همه نقاط را 1 یا 0 برچسب‌گذاری کند). این یک نمونه از اینکه چرا رگرسیون خطی برای مسائل دسته‌بندی مناسب نیست.

**رگرسیون لجستیک:**
در این جلسه، الگوریتمی معرفی خواهد شد که یکی از پرکاربردترین الگوریتم‌های دسته‌بندی است. این الگوریتم، رگرسیون لجستیک (Logistic Regression) نام دارد.

**قالب‌بندی داده‌ها (Data Format):**
فرض کنید یک جدول داده (table) داریم که شامل تعدادی "ویژگی" (features) است. تلاش می‌کنیم همه این ویژگی‌ها را به مقادیر عددی تبدیل کنیم. به عنوان مثال، اگر ستونی برای جنسیت (male/female) داریم، می‌توانیم آن را به 0 و 1 تبدیل کنیم که کاملاً قراردادی است.

**پیش‌پردازش داده (Data Preprocessing):**
ویژگی‌های عددی ممکن است مقیاس‌های (scales) بسیار متفاوتی داشته باشند. مثلاً، یک ویژگی ممکن است بین 0 تا 1 باشد (مثل جنسیت)، در حالی که دیگری ممکن است بین 150 تا 230 باشد (مثل قد). این دامنه‌های متفاوت باعث می‌شوند که ضرایب (weights) مدل (مثلاً در یک معادله خط ساده) مقیاس‌های بسیار متفاوتی داشته باشند. بنابراین، "مقیاس‌بندی" (scaling) تک‌تک ویژگی‌ها مهم است.
* **استانداردسازی (Standardization):** یک تکنیک این است که میانگین هر ویژگی را صفر و واریانس آن را یک کنیم. این کار به "نرمال‌سازی" یا "استانداردسازی" داده‌ها معروف است.
* **مقیاس‌بندی Min-Max (Min-Max Scaling):** روش دیگر این است که حداقل مقدار هر ویژگی را به صفر و حداکثر آن را به یک برسانیم.

پیش‌پردازش داده‌ها یک مرحله بسیار مهم در یادگیری ماشین است که قبل از قرار دادن داده‌ها در مدل باید انجام شود. در این مرحله، کارهایی مانند حذف داده‌های پرت (outliers)، بصری‌سازی (visualization) داده‌ها، و بررسی توزیع ویژگی‌های مختلف انجام می‌شود تا از کیفیت داده‌ها اطمینان حاصل شود. ممکن است لازم باشد برخی از ردیف‌ها (samples) یا ستون‌های داده (features) را حذف کنید زیرا نتایج مدل را تحت تأثیر قرار می‌دهند. این کار به "پیش‌پردازش داده" معروف است و در عمل یادگیری ماشین، اهمیت زیادی دارد.

در نهایت، یک ستون "برچسب" (label) نیز وجود دارد، مثلاً "اضافه وزن" (overweight) بودن یا نبودن، که هدف ما پیش‌بینی آن از روی ویژگی‌ها است.









































**1. مقدمه (Introduction)**

* **مسائل دسته‌بندی (Classification Problems):** برخلاف مسائل رگرسیون که هدف پیش‌بینی مقادیر پیوسته است، در مسائل دسته‌بندی هدف پیش‌بینی کلاس‌ها یا برچسب‌های گسسته است.
    * **مثال‌هایی از مسائل دسته‌بندی باینری (Binary Classification):**
        * ایمیل: اسپم (Spam) / غیر اسپم (Not Spam)
        * تراکنش‌های آنلاین: کلاهبرداری (Fraudulent) / معتبر (Genuine)
        * تومور: بدخیم (Malignant) / خوش‌خیم (Benign)
    * **برچسب‌گذاری (Labeling):** در این مسائل، خروجی $y$ می‌تواند یکی از دو مقدار $\{0, 1\}$ را بگیرد.
        * 0: "کلاس منفی" (Negative Class)، مانند تومور خوش‌خیم
        * 1: "کلاس مثبت" (Positive Class)، مانند تومور بدخیم
        * **تفاوت با پرسپترون:** در پرسپترون، برچسب کلاس‌ها معمولاً $-1$ و $1$ در نظر گرفته می‌شود، اما در رگرسیون لجستیک برای سادگی محاسبات ریاضی، برچسب‌ها $0$ و $1$ انتخاب می‌شوند.
* **مشکل رگرسیون خطی در مسائل دسته‌بندی:**
    * استفاده از رگرسیون خطی و تعریف آستانه (threshold) در 0.5 برای پیش‌بینی کلاس ($y=1$ اگر $h_{\theta}(x) \ge 0.5$ و $y=0$ اگر $h_{\theta}(x) < 0.5$) می‌تواند منجر به مشکلاتی شود.
    * با اضافه شدن یک نقطه داده جدید (outlier)، خط رگرسیون خطی می‌تواند به شدت تغییر کند و منجر به دسته‌بندی اشتباه بسیاری از نقاط موجود شود.
    * همچنین، خروجی $h_{\theta}(x)$ در رگرسیون خطی می‌تواند مقادیر بزرگتر از 1 یا کوچکتر از 0 را بگیرد، که با مفهوم احتمال (که باید بین 0 و 1 باشد) در تناقض است.
* **نیاز به تابعی در بازه [0, 1]:** برای مدل‌سازی احتمال، به تابعی نیاز داریم که خروجی آن همواره در بازه $[0, 1]$ باشد و بتواند یک درجه اطمینان (probability) را نشان دهد.
    * این تابع به عنوان "تابع فعال‌سازی" (activation function) شناخته می‌شود و با $\sigma(.)$ نمایش داده می‌شود.

---

**2. رگرسیون لجستیک (Logistic Regression)**

**2.1. مبانی (Fundamentals)**

* **تابع سیگموئید (Sigmoid Function) یا تابع لجستیک (Logistic Function):**
    * یک کاندید مناسب برای تابع فعال‌سازی است.
    * فرمول: $\sigma(z) = \frac{1}{1 + e^{-z}}$
    * **ویژگی‌ها:**
        * خروجی آن همواره بین 0 و 1 است.
        * به صورت "هموار" (smoothly) بین 0 و 1 تغییر می‌کند.
        * "مشتق‌پذیر" (differentiable) است.
        * در $z=0$، مقدار $\sigma(0)$ برابر با 0.5 است.
        * وقتی $z \to \infty$، $\sigma(z) \to 1$.
        * وقتی $z \to -\infty$، $\sigma(z) \to 0$.
    * **مشتق تابع سیگموئید:**
        * مشتق $\sigma(z)$ نسبت به $z$ برابر است با $\sigma(z)(1 - \sigma(z))$.
        * این ویژگی، محاسبات گرادیان را در رگرسیون لجستیک بسیار ساده می‌کند.
* **تفسیر خروجی رگرسیون لجستیک:**
    * $\sigma(w^T x)$ نشان‌دهنده احتمال اینکه $y=1$ باشد، با توجه به ورودی $x$ و پارامترهای $w$ است: $P(y=1|x, w) = \sigma(w^T x)$.
    * احتمال اینکه $y=0$ باشد، برابر است با: $P(y=0|x, w) = 1 - \sigma(w^T x)$.
    * $w$ یک بردار شامل ضرایب $w_0, w_1, ..., w_d$ است و $x$ یک بردار ویژگی شامل $x_0=1, x_1, ..., x_d$ است.
    * $w^T x$ همان ضرب داخلی بردار ویژگی‌ها در بردار وزن‌ها است.
    * **مثال:** اگر $\sigma(w^T x) = 0.7$ باشد، به این معنی است که 70 درصد شانس برد در بازی بسکتبال وجود دارد.

**2.2. سطح تصمیم (Decision Surface)**

* **تعریف:** سطح تصمیم یا مرز تصمیم (decision boundary)، ناحیه‌ای در فضای مسئله است که در آن، برچسب خروجی یک دسته‌بند مبهم است.
* **در دسته‌بندی باینری:** این سطح جایی است که احتمال تعلق یک نمونه به هر دو کلاس $y=0$ و $y=1$ برابر است.
* **معادله سطح تصمیم در رگرسیون لجستیک:**
    * $\sigma(w^T x) = 0.5$
    * از آنجایی که $\sigma(z) = 0.5$ فقط زمانی اتفاق می‌افتد که $z=0$، بنابراین مرز تصمیم جایی است که $w^T x = 0$.
    * این به این معنی است که سطوح تصمیم در رگرسیون لجستیک، "توابع خطی" از $x$ هستند.
    * **قانون پیش‌بینی:** اگر $\sigma(w^T x) \ge 0.5$， پیش‌بینی $y=1$ است؛ در غیر این صورت، پیش‌بینی $y=0$ است.
    * این معادل با این است که اگر $w^T x \ge 0$ باشد، $\hat{y}=1$ و در غیر این صورت $\hat{y}=0$.
* **ابعاد مرز تصمیم:** مرز تصمیم همیشه یک بعد کمتر از فضای ویژگی دارد.
* **مرز تصمیم غیرخطی (Non-linear Decision Boundary):**
    * می‌توان با اضافه کردن جملات مرتبه بالاتر (higher order terms) به بردار ویژگی‌ها (همانند مهندسی ویژگی - Feature Engineering)، مرزهای تصمیم پیچیده‌تر و غیرخطی را یاد گرفت.
    * **مثال:** با استفاده از ویژگی‌هایی مانند $x_1, x_2, x_1^2, x_2^2$， می‌توان یک مرز تصمیم دایره‌ای را ایجاد کرد.

**2.3. تخمین حداکثر درستنمایی (Maximum Likelihood Estimation - MLE)**

* **هدف:** در رگرسیون لجستیک، هدف پیدا کردن پارامترهای $w$ است که احتمال (شرطی) مشاهده داده‌های آموزشی را حداکثر کند.
* **احتمال پسین (Posterior Probability) یک نمونه:** $P(y^{(i)}|x^{(i)}, w)$
* **فرمول احتمال پسین برای دسته‌بندی باینری:**
    * از آنجایی که $y^{(i)}$ فقط می‌تواند $0$ یا $1$ باشد، می‌توان عبارت احتمال پسین را به صورت زیر ساده کرد:
    * $P(y^{(i)}|x^{(i)}, w) = \sigma(w^T x^{(i)})^{y^{(i)}} (1 - \sigma(w^T x^{(i)}))^{(1 - y^{(i)})}$
    * اگر $y^{(i)} = 1$ باشد، عبارت به $\sigma(w^T x^{(i)})$ تبدیل می‌شود.
    * اگر $y^{(i)} = 0$ باشد، عبارت به $(1 - \sigma(w^T x^{(i)}))$ تبدیل می‌شود.
* **تابع درستنمایی (Likelihood Function):** برای کل مجموعه داده، فرض می‌کنیم نمونه‌ها مستقل از هم هستند (Independent and Identically Distributed - IID). در این صورت، درستنمایی کل مجموعه داده، حاصل‌ضرب درستنمایی تک‌تک نمونه‌ها است:
    * $L(w) = \prod_{i=1}^{n} P(y^{(i)}|x^{(i)}, w)$
* **تابع لگاریتم درستنمایی (Log-Likelihood Function):** برای سادگی محاسبات، معمولاً لگاریتم تابع درستنمایی را به حداکثر می‌رسانیم (چون تابع لگاریتم یک تابع اکیداً صعودی است و نقطه ماکسیمم را تغییر نمی‌دهد).
    * $\hat{w} = \arg \max_{w} \log \prod_{i=1}^{n} P(y^{(i)}|x^{(i)}, w)$
    * با اعمال لگاریتم، حاصل‌ضرب به جمع تبدیل می‌شود:
    * $\log L(w) = \sum_{i=1}^{n} [y^{(i)} \log(\sigma(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(w^T x^{(i)}))]$

**2.4. تابع هزینه (Cost Function)**

* **ارتباط با لگاریتم درستنمایی:** تابع هزینه در رگرسیون لجستیک، منفی لگاریتم درستنمایی است، زیرا هدف ما "حداقل کردن" تابع هزینه (با استفاده از گرادیان کاهشی) است، در حالی که هدف MLE "حداکثر کردن" درستنمایی است.
    * $J(w) = -\sum_{i=1}^{n} \log P(y^{(i)}|x^{(i)}, w)$
    * $J(w) = \sum_{i=1}^{n} [-y^{(i)} \log(\sigma(w^T x^{(i)})) - (1 - y^{(i)}) \log(1 - \sigma(w^T x^{(i)}))]$
* **عدم وجود فرم بسته (No Closed-Form Solution):** برخلاف رگرسیون خطی، برای پیدا کردن $w$ با برابر قرار دادن گرادیان تابع هزینه به صفر ($\nabla_w J(w) = 0$)، راه حل فرم بسته وجود ندارد.
* **محدب بودن (Convexity):** با این حال، تابع هزینه $J(w)$ یک تابع "محدب" (convex) است.
    * این ویژگی تضمین می‌کند که الگوریتم‌های بهینه‌سازی مبتنی بر گرادیان (مانند گرادیان کاهشی) همواره به یک بهینه سراسری (global optimum) همگرا می‌شوند و در بهینه‌های محلی (local optima) گیر نمی‌کنند.
    * محدب بودن تابع هزینه از آنجا ناشی می‌شود که هر جمله در مجموع ( مربوط به یک نمونه) مشتق‌پذیر (دو بار) است و مشتق دوم آن همواره مثبت است، که نشان‌دهنده محدب بودن هر جمله و در نتیجه کل مجموع است.
* **نمایش بصری تابع هزینه (Binary Cross-Entropy Loss):**
    * این تابع هزینه به عنوان "Binary Cross-Entropy Loss" نیز شناخته می‌شود.
    * نمودار آن نشان می‌دهد که اگر مقدار پیش‌بینی شده (Predicted Value) نزدیک به مقدار واقعی (True Value) باشد، هزینه (Loss) کم است و اگر دور باشد، هزینه زیاد است.

**2.5. گرادیان کاهشی (Gradient Descent)**

* **هدف:** پیدا کردن پارامترهای $w$ که تابع هزینه $J(w)$ را حداقل کنند.
* **قانون به‌روزرسانی (Update Rule):**
    * $w^{t+1} = w^t - \eta \nabla_w J(w^t)$
    * که در آن $\eta$ نرخ یادگیری (learning rate) است.
* **گرادیان تابع هزینه رگرسیون لجستیک:**
    * $\nabla_w J(w) = \sum_{i=1}^{n} (\sigma(w^T x^{(i)}) - y^{(i)}) x^{(i)}$
* **مقایسه با رگرسیون خطی:**
    * گرادیان رگرسیون خطی: $\nabla_w J(w) = \sum_{i=1}^{n} (w^T x^{(i)} - y^{(i)}) x^{(i)}$
    * تنها تفاوت، وجود تابع سیگموئید ($\sigma$) در رگرسیون لجستیک است. این سادگی فرم گرادیان، یکی از دلایل انتخاب تابع سیگموئید است.
    * این فرم گرادیان نشان می‌دهد که "خطا" (تفاوت بین پیش‌بینی و مقدار واقعی) در ویژگی‌ها ضرب می‌شود تا جهت بهینه‌سازی پارامترها مشخص شود.

**2.6. رگرسیون لجستیک چندکلاسه (Multi-class Logistic Regression)**

* **مسئله:** زمانی که بیش از دو کلاس (K کلاس) وجود دارد و هر نمونه فقط به یک کلاس تعلق دارد.
* **تابع Softmax (Normalized Exponential):**
    * برای هر کلاس $k$، $\sigma_k(x; W)$ احتمال $P(y=k|x, W)$ را پیش‌بینی می‌کند.
    * مجموع احتمالات برای همه کلاس‌ها باید برابر با 1 باشد: $\sum_{k=1}^{K} P(y=k|x_0, W) = 1$.
    * **فرمول:** $\sigma_k(x, W) = P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_{j=1}^{K} \exp(w_j^T x)}$.
    * $W$ یک ماتریس وزن است که شامل بردارهای وزن $w_k$ برای هر کلاس است.
    * **ویژگی‌ها:**
        * مقادیر منفی را نیز به خوبی مدیریت می‌کند (به دلیل تابع نمایی).
        * مانند سیگموئید، تابع Softmax نیز هموار (smooth) و مشتق‌پذیر است.
        * مقدار بیشینه را به صورت هموار برجسته می‌کند (برخلاف تابع $\max(.)$ که گسسته و نامشتق‌پذیر است).
* **قانون پیش‌بینی:** برای یک ورودی جدید $x$， کلاسی را انتخاب می‌کنیم که $\sigma_k(x; W)$ آن را حداکثر کند:
    * $\alpha(x) = \arg \max_{k=1,...,K} \sigma_k(x; W)$
* **تابع هزینه (Cross-Entropy Loss برای Multi-class):**
    * تابع هزینه همچنان منفی لگاریتم درستنمایی است.
    * $J(W) = -\sum_{i=1}^{n} \sum_{k=1}^{K} y_k^{(i)} \log(\sigma_k(x^{(i)}; W))$
    * در اینجا $y_k^{(i)}$ نشان‌دهنده یک "کدگذاری یک از K" (1-of-K encoding) برای برچسب $y^{(i)}$ است (اگر نمونه $i$ به کلاس $k$ تعلق داشته باشد، $y_k^{(i)}=1$ و در غیر این صورت $0$).
* **گرادیان کاهشی برای Multi-class Logistic Regression:**
    * همچنان راه حل فرم بسته وجود ندارد.
    * قانون به‌روزرسانی گرادیان کاهشی: $w_j^{t+1} = w_j^t - \eta \nabla_{w_j} J(W^t)$
    * گرادیان نسبت به بردار وزن $w_j$ برای کلاس $j$:
        * $\nabla_{w_j} J(W) = \sum_{i=1}^{n} (\sigma_j(x^{(i)}; W) - y_j^{(i)}) x^{(i)}$
    * که در آن $w_j^t$ بردار وزن برای کلاس $j$ در تکرار $t$-ام است.

---

**3. جمع‌بندی رگرسیون لجستیک (Logistic Regression Summary)**

* **دسته‌بند خطی (Linear Classifier):** رگرسیون لجستیک یک دسته‌بند خطی است (مگر اینکه از ویژگی‌های غیرخطی - Feature Engineering - استفاده شود).
* **بهینه‌سازی با حداکثر درستنمایی (Maximum Likelihood Optimization):** مسئله بهینه‌سازی آن از طریق حداکثر درستنمایی به دست می‌آید.
* **عدم وجود فرم بسته (No Closed-Form Solution):** برای حل مسئله بهینه‌سازی آن راه حل فرم بسته وجود ندارد.
* **تابع هزینه محدب (Convex Cost Function):** تابع هزینه آن محدب است و می‌توان با استفاده از گرادیان صعودی (برای حداکثر کردن درستنمایی) یا گرادیان کاهشی (برای حداقل کردن تابع هزینه) به بهینه سراسری دست یافت.

---

**4. مطالعه بیشتر: دیدگاه احتمالی در دسته‌بندی (Probabilistic View in Classification)**

* **متغیرهای تصادفی:** در مسائل دسته‌بندی، هم ویژگی‌ها (مثل قد یک فرد) و هم برچسب کلاس (مثل اضافه وزن داشتن یا نداشتن) به عنوان متغیرهای تصادفی در نظر گرفته می‌شوند.
* **هدف:** مشاهده ویژگی‌ها ($x$) و پیدا کردن برچسب کلاس ($y$).

**4.1. تعاریف (Definitions)**

* **احتمال پسین (Posterior Probability):** احتمال یک برچسب کلاس $C_k$ با توجه به یک نمونه $x$.
    * $P(C_k|x)$
* **درستنمایی (Likelihood) یا احتمال شرطی کلاس (Class Conditional Probability):** تابع چگالی احتمال (PDF) بردار ویژگی $x$ برای نمونه‌های کلاس $C_k$.
    * $P(x|C_k)$
* **احتمال پیشین (Prior Probability):** احتمال وقوع کلاس $C_k$.
    * $P(C_k)$
* **تابع چگالی احتمال (PDF) بردار ویژگی $x$:**
    * $P(x) = \sum_{k=1}^{K} P(x|C_k) P(C_k)$

**4.2. دسته‌بندهای احتمالی (Probabilistic Classifiers)**

* **رویکردهای اصلی:**
    * **مدل‌های مولد (Generative Models):**
        * یادگیری توزیع احتمال توأم $P(x, y)$.
        * سپس از آن برای پیدا کردن $P(C_k|x)$ (با استفاده از قضیه بیز) استفاده می‌شود.
        * می‌توانند برای تولید جفت‌های $(x, y)$ نیز استفاده شوند.
    * **مدل‌های تمایزدهنده (Discriminative Models):**
        * مستقیماً توزیع احتمال شرطی $P(y|x)$ را تخمین می‌زنند.
        * **رگرسیون لجستیک یک رویکرد تمایزدهنده است.**
        * در رگرسیون لجستیک، ما مستقیماً می‌خواهیم برچسب کلاس را با $\sigma(w^T x)$ مشخص کنیم.

---

**5. مراجع (References)**

این اسلایدها توسط دانیال غریب تهیه شده‌اند.

---
