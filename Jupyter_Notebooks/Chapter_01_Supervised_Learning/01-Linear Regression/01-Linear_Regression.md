
## مفهوم کلی: رگرسیون چندجمله‌ای چیه؟

اول از همه، بیایم ببینیم **رگرسیون (Regression)** چیه. تو دنیای واقعی، ما اغلب می‌خوایم رابطه‌ی بین دو یا چند چیز رو پیدا کنیم. مثلاً، رابطه‌ی بین میزان تبلیغات و فروش محصول، یا رابطه‌ی بین ساعت مطالعه و نمره‌ی امتحان. رگرسیون یک روش آماری و یادگیری ماشینه که سعی می‌کنه این رابطه رو با استفاده از یک **مدل ریاضی** پیدا کنه تا بتونیم برای داده‌های جدید، پیش‌بینی انجام بدیم.

**رگرسیون خطی (Linear Regression)** ساده‌ترین نوع رگرسیونه که فرض می‌کنه رابطه بین داده‌ها یک خط مستقیمه. اما همیشه اینطور نیست\! گاهی اوقات رابطه‌ی بین داده‌ها پیچیده‌تر و **غیرخطی (Nonlinear)** هست، یعنی با یک خط صاف نمیشه اون رو نشون داد.

اینجاست که **رگرسیون چندجمله‌ای (Polynomial Regression)** وارد میشه. این روش به ما اجازه میده تا به جای خط صاف، از یک **منحنی (Curve)** برای مدل کردن رابطه استفاده کنیم. این منحنی‌ها با استفاده از **چندجمله‌ای‌ها (Polynomials)** ساخته میشن. مثلاً به جای $y = ax + b$ (خطی)، می‌تونیم از $y = ax^2 + bx + c$ (درجه ۲) یا $y = ax^3 + bx^2 + cx + d$ (درجه ۳) و غیره استفاده کنیم. **درجه (Degree)** چندجمله‌ای نشون میده که منحنی چقدر می‌تونه پیچیده باشه.

-----

## بررسی گام به گام کد

حالا که یک دید کلی پیدا کردی، بریم سراغ جزئیات کد:

### ۱. توابع و کتابخانه‌های مورد نیاز

قبل از هر چیز، باید بدونیم که این کد از چند کتابخانه مهم پایتون استفاده می‌کنه. معمولاً این خطوط در ابتدای هر فایل پایتون قرار می‌گیرند (البته در کدی که دادی نیست، ولی وجودشون لازمه):

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
```

  * **`numpy` (با نام مستعار `np`)**: این کتابخانه برای انجام عملیات عددی و کار با آرایه‌ها (Arrays) استفاده میشه. تقریباً تمام محاسبات ریاضی و ماتریسی در این کد با `numpy` انجام میشه.
  * **`matplotlib.pyplot` (با نام مستعار `plt`)**: این کتابخانه برای رسم نمودار و **ویژوالایز کردن (Visualize)** یا بصری‌سازی داده‌ها و نتایج مدل استفاده میشه.
  * **`sklearn.model_selection.train_test_split`**: این تابع از کتابخانه `scikit-learn` (که معمولاً بهش `sklearn` میگن) برای تقسیم داده‌ها به دو بخش **آموزش (Train)** و **آزمایش (Test)** استفاده میشه. بعداً در موردش توضیح میدم که چرا این کار مهمه.

### ۲. تابع `polynomial_features`: ساخت ویژگی‌های چندجمله‌ای

```python
# Function to generate polynomial features (input matrix X')
def polynomial_features(X, degree):
    X_poly = np.c_[np.ones(len(X))]
    for i in range(1, degree + 1):
        X_poly = np.c_[X_poly, X**i]
    return X_poly
```

این تابع قلب رگرسیون چندجمله‌ایه. وظیفه‌اش اینه که داده‌های ورودی `X` رو "آماده" کنه تا بتونیم یک منحنی رو روشون فیت کنیم.

  * **`def polynomial_features(X, degree):`**: این خط تعریف یک **تابع (Function)** در پایتونه. `polynomial_features` اسم تابعیه که ما ساختیم. این تابع دو تا ورودی می‌گیره:
      * `X`: داده‌های ورودی ما (مثلاً ساعت مطالعه).
      * `degree`: درجه چندجمله‌ای که می‌خوایم بسازیم (مثلاً اگر `degree` برابر با 2 باشه، می‌خوایم تا $x^2$ رو در نظر بگیریم).
  * **`X_poly = np.c_[np.ones(len(X))]`**:
      * `len(X)`: تعداد داده‌های موجود در `X` رو برمی‌گردونه.
      * `np.ones(len(X))`: یک آرایه (لیستی از اعداد) می‌سازه که تمام عناصرش 1 هستن و تعدادشون به اندازه‌ی `len(X)` هست. چرا 1؟ چون در رگرسیون، معمولاً یک **عرض از مبدأ (Intercept)** یا **بایاس (Bias)** داریم که ضریبش همیشه در 1 ضرب میشه. این 1 ها ستون اول ماتریس ورودی ما رو تشکیل میدن.
      * `np.c_[]`: این یک امکان در `numpy` هست که به ما اجازه میده آرایه‌ها رو **کنار هم (Column-wise)** بچسبونیم. اینجا داریم اون ستون 1 ها رو به عنوان شروع ماتریس `X_poly` قرار میدیم.
  * **`for i in range(1, degree + 1):`**: این یک **حلقه `for` (For Loop)** هست. حلقه‌ها برای تکرار یک سری عملیات استفاده میشن. اینجا، این حلقه از `i = 1` شروع میشه و تا `degree` ادامه پیدا می‌کنه (مثلاً اگر `degree` 2 باشه، `i` یک بار 1 میشه و یک بار 2).
  * **`X_poly = np.c_[X_poly, X**i]`**: داخل حلقه، هر بار یک ستون جدید به `X_poly` اضافه میشه:
      * `X**i`: یعنی `X` به توان `i`. مثلاً اگر `i` برابر 2 باشه، `X` به توان 2 می‌رسه.
      * `np.c_[X_poly, ...]` : این ستون جدید (`X**i`) رو به `X_poly` موجود اضافه می‌کنه.
      * **نتیجه این تابع**: در نهایت، `X_poly` یک ماتریس (مثل یک جدول) میشه که ستون اولش همه‌اش 1 هست، ستون دومش `X` اصلیه، ستون سومش `X^2` هست، و همینطور تا `X^degree` ادامه پیدا می‌کنه. این ماتریس جدید رو **ماتریس ویژگی‌های چندجمله‌ای (Polynomial Features Matrix)** میگن.

### ۳. تابع `polynomial_regression`: پیاده‌سازی رگرسیون چندجمله‌ای

```python
def polynomial_regression(X, y, degree):
    X_poly = polynomial_features(X, degree)
    # Closed-form solution: w = (X'^T * X')^-1 * X'^T * y
    w = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)
    return w
```

این تابع مدل رگرسیون چندجمله‌ای رو می‌سازه و **پارامترهای (Parameters)** مدل رو محاسبه می‌کنه. پارامترها (که اینجا با `w` نشون داده میشن) همون ضریب‌های $a, b, c$ و ... در معادله‌ی چندجمله‌ای ما هستند.

  * **`def polynomial_regression(X, y, degree):`**: تعریف تابع `polynomial_regression` با سه ورودی `X` (داده‌های ورودی), `y` (خروجی‌های واقعی که می‌خوایم پیش‌بینی کنیم، مثلاً نمره‌ی امتحان), و `degree`.

  * **`X_poly = polynomial_features(X, degree)`**: اینجا تابع `polynomial_features` رو که بالا تعریف کردیم، صدا می‌زنیم تا ماتریس ویژگی‌های چندجمله‌ای رو بسازه.

  * **`w = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)`**: این خط مهم‌ترین قسمت این تابع هست. این فرمول **راه حل تحلیلی (Analytical Solution)** یا **فرم بسته (Closed-form Solution)** برای پیدا کردن پارامترهای `w` در رگرسیون خطی/چندجمله‌ایه. اجازه بده جزئیاتش رو بگم:

      * `X_poly.T`: یعنی **ترانهاده (Transpose)** ماتریس `X_poly`. ترانهاده یعنی سطرها و ستون‌های ماتریس جابجا میشن.
      * `.dot(...)`: این متد برای **ضرب ماتریسی (Matrix Multiplication)** در `numpy` استفاده میشه.
      * `X_poly.T.dot(X_poly)`: ضرب ترانهاده `X_poly` در خود `X_poly`.
      * `np.linalg.inv(...)`: این تابع **معکوس (Inverse)** یک ماتریس رو محاسبه می‌کنه.
      * **کل عبارت**: در نهایت، کل این فرمول `w = (X'^T * X')^-1 * X'^T * y` پارامترهای `w` رو به ما میده که بهترین منحنی چندجمله‌ای رو برای داده‌های ما نشون میده. (از نظر ریاضی، این فرمول از روش **کمترین مربعات (Least Squares)** مشتق شده).

  * **`return w`**: تابع، بردار `w` (همون پارامترهای مدل) رو برمی‌گردونه.

  * **`m = 5`**: یک **متغیر (Variable)** به نام `m` تعریف شده و مقدار 5 بهش داده شده. این `m` همون `degree` یا درجه‌ی چندجمله‌ای هست که قراره برای مدل استفاده بشه.

  * **`w_poly = polynomial_regression(X, y, m)`**: تابع `polynomial_regression` رو با داده‌های `X`, `y` و درجه `m` (که 5 هست) صدا می‌زنیم. نتیجه (پارامترهای `w`) در متغیر `w_poly` ذخیره میشه.

  * **`print(f"Parameters (w) for Degree {m}: {w_poly}")`**: این خط پارامترهای محاسبه شده رو چاپ می‌کنه. `f-string` یک روش راحت برای فرمت کردن رشته‌ها در پایتونه که به شما اجازه میده متغیرها رو مستقیماً داخل رشته قرار بدید.

### ۴. `X` و `y` از کجا میان؟ (نکته مهم)

در کدی که شما دادی، `X` و `y` تعریف نشدن. این `X` و `y` همون داده‌های ورودی و خروجی ما هستند. معمولاً قبل از این قسمت کد، این داده‌ها تولید یا از یک فایل خونده میشن. مثلاً ممکنه اینطور تعریف شده باشن:

```python
# مثال: تولید داده‌های X و y (شبیه سازی)
np.random.seed(0) # برای اینکه هر بار اجرای کد، نتایج یکسان باشه
X = 2 * np.random.rand(100, 1) # 100 نقطه داده تصادفی بین 0 و 2
y = 4 + 3 * X + 2 * X**2 + np.random.randn(100, 1) # y با یک رابطه چندجمله‌ای و کمی نویز (اختلاف تصادفی)
```

**قبل از اجرای این کد، حتماً باید X و y رو تعریف کنی یا از جایی لود کنی.**

### ۵. `Visualize the Polynomial Fit`: بصری‌سازی مدل چندجمله‌ای

```python
X_fit = np.linspace(X.min(), X.max(), 200)
X_fit_poly = polynomial_features(X_fit, m)
y_poly_pred = X_fit_poly.dot(w_poly) # h_w(x) = X' * w

# Plot the actual data and the polynomial fit
plt.scatter(X, y, color='blue', label='Actual Data')
plt.plot(X_fit, y_poly_pred, color='green', label=f'Polynomial Fit (Degree {m})')
plt.title(f"Polynomial Regression (Degree {m})")
plt.xlabel("$x$")
plt.ylabel("$y$")
plt.legend()
plt.show()
```

این بخش برای رسم نمودار و دیدن اینکه مدل چقدر خوب روی داده‌ها فیت شده، استفاده میشه.

  * **`X_fit = np.linspace(X.min(), X.max(), 200)`**:
      * `X.min()` و `X.max()`: کوچکترین و بزرگترین مقدار در داده‌های `X` رو پیدا می‌کنه.
      * `np.linspace(start, stop, num)`: این تابع `numpy` یک آرایه از `num` عدد رو به صورت مساوی توزیع شده (با فواصل یکسان) بین `start` و `stop` ایجاد می‌کنه. اینجا 200 نقطه بین حداقل و حداکثر `X` ایجاد میشه. این `X_fit` برای رسم یک منحنی صاف استفاده میشه.
  * **`X_fit_poly = polynomial_features(X_fit, m)`**: با استفاده از `X_fit` (نقاط برای رسم منحنی صاف) و درجه `m`، ویژگی‌های چندجمله‌ای رو می‌سازیم.
  * **`y_poly_pred = X_fit_poly.dot(w_poly)`**: این خط مهم‌ترین بخش برای **پیش‌بینی (Prediction)** هست. اینجا، ماتریس ویژگی‌های `X_fit_poly` رو در پارامترهای `w_poly` که قبلاً محاسبه کردیم، ضرب می‌کنیم. نتیجه، مقادیر `y` پیش‌بینی شده (`y_poly_pred`) توسط مدل ما روی نقاط `X_fit` هست. این همون $h\_w(x) = X' \* w$ هست که در توضیحات اشاره شده.
  * **`plt.scatter(X, y, color='blue', label='Actual Data')`**: این خط داده‌های اصلی ما (`X` و `y`) رو به صورت **نقاط (Scatter Plot)** آبی رنگ روی نمودار رسم می‌کنه. `label` برای نمایش در `legend` (راهنما) استفاده میشه.
  * **`plt.plot(X_fit, y_poly_pred, color='green', label=f'Polynomial Fit (Degree {m})')`**: این خط منحنی مدل چندجمله‌ای پیش‌بینی شده رو به صورت یک خط سبز رنگ روی نمودار رسم می‌کنه.
  * **`plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.legend()`**: این‌ها برای اضافه کردن عنوان به نمودار، برچسب به محورها، و نمایش راهنما (که `label`ها رو نشون میده) استفاده میشن.
  * **`plt.show()`**: این خط نمودار رو نمایش میده.

-----

## `Visualizing $E_{rms}$`: ارزیابی عملکرد مدل با RMSE

**RMSE (Root Mean Square Error)** یا **خطای میانگین مربع ریشه**، یک معیار خیلی مهم برای ارزیابی عملکرد مدل‌های رگرسیونه. این معیار به ما میگه که به طور متوسط، پیش‌بینی‌های مدل ما چقدر از مقادیر واقعی فاصله دارن. هرچی RMSE کمتر باشه، مدل ما بهتره.

  * **`def compute_rms_error(y_true, y_pred):`**: تعریف تابعی برای محاسبه RMSE که دو ورودی می‌گیره:
      * `y_true`: مقادیر واقعی `y`.
      * `y_pred`: مقادیر `y` که توسط مدل پیش‌بینی شده.
  * **`return np.sqrt(np.mean((y_true - y_pred) ** 2))`**: این خط فرمول RMSE رو پیاده‌سازی می‌کنه:
      * `(y_true - y_pred)`: اختلاف بین مقدار واقعی و پیش‌بینی شده (همون خطا).
      * `(... ** 2)`: مربع کردن خطاها (تا خطاهای منفی و مثبت با هم جمع نشن و خطاهای بزرگتر جریمه‌ی بیشتری بشن).
      * `np.mean(...)`: میانگین مربع خطاها رو محاسبه می‌کنه.
      * `np.sqrt(...)`: ریشه‌ی دوم میانگین مربع خطاها رو می‌گیره تا به واحد اصلی `y` برگرده.

-----

## `Visualizing RMSE for different Polynomial degrees`: بررسی RMSE برای درجات مختلف

این بخش کد، یک کار خیلی مهم رو انجام میده: **انتخاب بهترین درجه (Degree)** برای مدل. اگر درجه خیلی کم باشه، مدل ممکنه نتونه رابطه‌ی واقعی رو یاد بگیره (به این میگن **Underfitting**). اگر درجه خیلی زیاد باشه، مدل ممکنه جزئیات بی‌اهمیت و نویز داده رو هم یاد بگیره و روی داده‌های جدید خوب عمل نکنه (به این میگن **Overfitting**).

### تقسیم داده‌ها به Train و Test

```python
from sklearn.model_selection import train_test_split

# Split the data into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

برای جلوگیری از Overfitting و ارزیابی صحیح مدل، داده‌ها رو به دو بخش تقسیم می‌کنیم:

  * **`train_test_split(X, y, test_size=0.2, random_state=42)`**:
      * `X, y`: داده‌های ورودی و خروجی ما.
      * `test_size=0.2`: یعنی 20 درصد از داده‌ها برای بخش **آزمایش (Test Set)** و 80 درصد برای بخش **آموزش (Training Set)** استفاده میشه.
      * `random_state=42`: این عدد باعث میشه هر بار که کد رو اجرا می‌کنیم، تقسیم‌بندی داده‌ها به Train و Test دقیقاً یکسان باشه. این برای **تکرارپذیری (Reproducibility)** نتایج خیلی مهمه.
  * **`X_train, X_test, y_train, y_test`**: متغیرهایی که داده‌های تقسیم شده رو نگه می‌دارند. `_train` برای آموزش و `_test` برای آزمایش.

### حلقه برای درجات مختلف و محاسبه RMSE

```python
degrees = range(0, 9)
train_rms_errors = []
test_rms_errors = []

for d in degrees:
    # Train the model on the training set
    w_poly = polynomial_regression(X_train, y_train, d)

    # Compute predictions for the training set
    X_train_poly = polynomial_features(X_train, d)
    y_train_pred = X_train_poly.dot(w_poly)

    # Compute predictions for the test set
    X_test_poly = polynomial_features(X_test, d)
    y_test_pred = X_test_poly.dot(w_poly)

    # Calculate RMSE for both training and test sets
    train_rms_error = compute_rms_error(y_train, y_train_pred)
    test_rms_error = compute_rms_error(y_test, y_test_pred)

    # Store the errors
    train_rms_errors.append(train_rms_error)
    test_rms_errors.append(test_rms_error)

    # Print the RMSE for the current degree
    print(f"Degree {d}: Train RMSE = {train_rms_error:.2f}, Test RMSE = {test_rms_error:.2f}")

    # Plot the polynomial fit on the training data
    plt.scatter(X_train, y_train, color='blue', label="Training Data")
    plt.scatter(X_test, y_test, color='red', label="Test Data", alpha=0.6)
    X_fit = np.linspace(X.min(), X.max(), 200)
    X_fit_poly = polynomial_features(X_fit, d)
    y_fit_pred = X_fit_poly.dot(w_poly)
    plt.plot(X_fit, y_fit_pred, label=f"Degree {d} Fit", color='green')
    plt.title(f"Polynomial Regression (Degree {d})")
    plt.xlabel("$x$")
    plt.ylabel("$y$")
    plt.legend()
    plt.show()
```

  * **`degrees = range(0, 9)`**: یک لیست از اعداد 0 تا 8 (درجه‌ها) رو ایجاد می‌کنه.
  * **`train_rms_errors = []` و `test_rms_errors = []`**: دو لیست خالی برای ذخیره RMSE آموزش و آزمایش در هر درجه.
  * **`for d in degrees:`**: یک حلقه `for` دیگه که برای هر درجه `d` (از 0 تا 8) کارهای زیر رو تکرار می‌کنه:
      * **`w_poly = polynomial_regression(X_train, y_train, d)`**: مدل رو روی **داده‌های آموزش (`X_train`, `y_train`)** با درجه `d` آموزش میده و پارامترهای `w` رو محاسبه می‌کنه. **خیلی مهمه که مدل فقط روی داده‌های آموزش ببینه.**
      * **محاسبه پیش‌بینی برای داده‌های آموزش و آزمایش**:
          * `y_train_pred`: پیش‌بینی‌های مدل روی `X_train`.
          * `y_test_pred`: پیش‌بینی‌های مدل روی `X_test`.
      * **محاسبه RMSE برای آموزش و آزمایش**:
          * `train_rms_error = compute_rms_error(y_train, y_train_pred)`: RMSE برای داده‌های آموزشی محاسبه میشه.
          * `test_rms_error = compute_rms_error(y_test, y_test_pred)`: RMSE برای **داده‌های آزمایشی** محاسبه میشه. این **مهم‌ترین معیار** برای ارزیابی عملکرد واقعی مدل روی داده‌های "دیده نشده" است.
      * **ذخیره خطاها**: RMSEهای محاسبه شده به لیست‌های `train_rms_errors` و `test_rms_errors` اضافه میشن.
      * **چاپ RMSE**: RMSE هر درجه رو چاپ می‌کنه.
      * **رسم نمودار برای هر درجه**: برای هر درجه `d`، یک نمودار جداگانه رسم میشه که نقاط آموزش (آبی)، نقاط آزمایش (قرمز) و منحنی فیت شده (سبز) رو نشون میده. این به شما کمک می‌کنه **بصری (visually)** ببینید که مدل در هر درجه چطور روی داده‌ها فیت میشه.

### رسم نمودار نهایی RMSE

```python
# Plot RMSE for training and test sets
plt.plot(degrees, train_rms_errors, marker='o', linestyle='-', color='blue', label='Train RMSE')
plt.plot(degrees, test_rms_errors, marker='o', linestyle='-', color='red', label='Test RMSE')
plt.title("Train vs Test RMSE vs Polynomial Degree")
plt.xlabel("Polynomial Degree")
plt.ylabel("RMSE")
plt.xticks(degrees)
plt.grid(True)
plt.legend()
plt.show()
```

در پایان، این کد یک نمودار کلی رسم می‌کنه که تغییرات RMSE آموزش و آزمایش رو بر حسب درجه چندجمله‌ای نشون میده.

  * **`plt.plot(...)`**: دو خط رو روی نمودار رسم می‌کنه: یکی برای `train_rms_errors` (آبی) و دیگری برای `test_rms_errors` (قرمز).
      * **`marker='o'`**: نقاط رو با دایره نشون میده.
      * **`linestyle='-'`**: خطوط رو به صورت پیوسته نشون میده.
  * **`plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.xticks(...)`, `plt.grid(True)`, `plt.legend()`, `plt.show()`**: این‌ها هم برای بهتر شدن ظاهر نمودار و نمایش اطلاعات بیشتر هستن.

**تفسیر این نمودار نهایی خیلی مهمه:**

  * معمولاً با افزایش درجه، `Train RMSE` کم میشه (مدل روی داده‌های آموزش بهتر فیت میشه).
  * اما `Test RMSE` در ابتدا کم میشه و بعد از یک نقطه‌ای شروع به افزایش می‌کنه. **نقطه‌ای که `Test RMSE` حداقل مقدار رو داره، بهترین درجه برای مدل شماست.** بعد از اون نقطه، مدل دچار **Overfitting** میشه.

-----

## جمع‌بندی و مراحل بعدی

خب، تا اینجا با هم همه‌ی قسمت‌های کد رو بررسی کردیم و توضیح دادم که هر بخش چی کار می‌کنه و سینتکس پایتونیش چیه و چه مفاهیم یادگیری ماشینی پشتش هست.

**خلاصه:**
این کد یک پیاده‌سازی از رگرسیون چندجمله‌ای هست که:

1.  **ویژگی‌های چندجمله‌ای** رو از داده‌های اصلی می‌سازه.
2.  با استفاده از **فرمول تحلیلی**، بهترین پارامترهای مدل رو پیدا می‌کنه.
3.  عملکرد مدل رو با **RMSE** ارزیابی می‌کنه.
4.  با استفاده از **تقسیم داده‌ها به Train/Test**، به شما کمک می‌کنه تا بهترین درجه برای مدل رو انتخاب کنید و از **Overfitting** جلوگیری کنید.
5.  نتایج رو با **نمودارها** بصری‌سازی می‌کنه.

-----

**حالا نوبت توئه\!**

1.  **آیا قسمت خاصی از توضیحات هست که برات گنگ باشه؟**
2.  **می‌خوای `X` و `y` رو تولید کنیم و بعد این کد رو گام به گام اجرا کنیم تا نتایج رو ببینی و بیشتر متوجه بشی؟**
3.  **ممکنه سوالاتی در مورد مفاهیم مثل "ماتریس"، "ترانهاده"، "معکوس ماتریس"، "Overfitting" یا "Underfitting" داشته باشی؟**

هر سوالی که داری بپرس تا عمیق‌تر بهش بپردازیم\!مرسی که کد رو به اشتراک گذاشتی\! این کد برای **رگرسیون چندجمله‌ای (Polynomial Regression)** نوشته شده، که یکی از روش‌های **یادگیری ماشین (Machine Learning)** برای پیدا کردن رابطه‌ی بین داده‌هاست.

اجازه بده تا قدم به قدم این کد رو با هم بررسی کنیم تا هم سینتکس پایتونش رو یاد بگیری و هم بفهمیم چی کار می‌کنه.

-----

## مفهوم کلی: رگرسیون چندجمله‌ای چیه؟

اول از همه، بیایم ببینیم **رگرسیون (Regression)** چیه. تو دنیای واقعی، ما اغلب می‌خوایم رابطه‌ی بین دو یا چند چیز رو پیدا کنیم. مثلاً، رابطه‌ی بین میزان تبلیغات و فروش محصول، یا رابطه‌ی بین ساعت مطالعه و نمره‌ی امتحان. رگرسیون یک روش آماری و یادگیری ماشینه که سعی می‌کنه این رابطه رو با استفاده از یک **مدل ریاضی** پیدا کنه تا بتونیم برای داده‌های جدید، پیش‌بینی انجام بدیم.

**رگرسیون خطی (Linear Regression)** ساده‌ترین نوع رگرسیونه که فرض می‌کنه رابطه بین داده‌ها یک خط مستقیمه. اما همیشه اینطور نیست\! گاهی اوقات رابطه‌ی بین داده‌ها پیچیده‌تر و **غیرخطی (Nonlinear)** هست، یعنی با یک خط صاف نمیشه اون رو نشون داد.

اینجاست که **رگرسیون چندجمله‌ای (Polynomial Regression)** وارد میشه. این روش به ما اجازه میده تا به جای خط صاف، از یک **منحنی (Curve)** برای مدل کردن رابطه استفاده کنیم. این منحنی‌ها با استفاده از **چندجمله‌ای‌ها (Polynomials)** ساخته میشن. مثلاً به جای $y = ax + b$ (خطی)، می‌تونیم از $y = ax^2 + bx + c$ (درجه ۲) یا $y = ax^3 + bx^2 + cx + d$ (درجه ۳) و غیره استفاده کنیم. **درجه (Degree)** چندجمله‌ای نشون میده که منحنی چقدر می‌تونه پیچیده باشه.

-----

## بررسی گام به گام کد

حالا که یک دید کلی پیدا کردی، بریم سراغ جزئیات کد:

### ۱. توابع و کتابخانه‌های مورد نیاز

قبل از هر چیز، باید بدونیم که این کد از چند کتابخانه مهم پایتون استفاده می‌کنه. معمولاً این خطوط در ابتدای هر فایل پایتون قرار می‌گیرند (البته در کدی که دادی نیست، ولی وجودشون لازمه):

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
```

  * **`numpy` (با نام مستعار `np`)**: این کتابخانه برای انجام عملیات عددی و کار با آرایه‌ها (Arrays) استفاده میشه. تقریباً تمام محاسبات ریاضی و ماتریسی در این کد با `numpy` انجام میشه.
  * **`matplotlib.pyplot` (با نام مستعار `plt`)**: این کتابخانه برای رسم نمودار و **ویژوالایز کردن (Visualize)** یا بصری‌سازی داده‌ها و نتایج مدل استفاده میشه.
  * **`sklearn.model_selection.train_test_split`**: این تابع از کتابخانه `scikit-learn` (که معمولاً بهش `sklearn` میگن) برای تقسیم داده‌ها به دو بخش **آموزش (Train)** و **آزمایش (Test)** استفاده میشه. بعداً در موردش توضیح میدم که چرا این کار مهمه.

### ۲. تابع `polynomial_features`: ساخت ویژگی‌های چندجمله‌ای

```python
# Function to generate polynomial features (input matrix X')
def polynomial_features(X, degree):
    X_poly = np.c_[np.ones(len(X))]
    for i in range(1, degree + 1):
        X_poly = np.c_[X_poly, X**i]
    return X_poly
```

این تابع قلب رگرسیون چندجمله‌ایه. وظیفه‌اش اینه که داده‌های ورودی `X` رو "آماده" کنه تا بتونیم یک منحنی رو روشون فیت کنیم.

  * **`def polynomial_features(X, degree):`**: این خط تعریف یک **تابع (Function)** در پایتونه. `polynomial_features` اسم تابعیه که ما ساختیم. این تابع دو تا ورودی می‌گیره:
      * `X`: داده‌های ورودی ما (مثلاً ساعت مطالعه).
      * `degree`: درجه چندجمله‌ای که می‌خوایم بسازیم (مثلاً اگر `degree` برابر با 2 باشه، می‌خوایم تا $x^2$ رو در نظر بگیریم).
  * **`X_poly = np.c_[np.ones(len(X))]`**:
      * `len(X)`: تعداد داده‌های موجود در `X` رو برمی‌گردونه.
      * `np.ones(len(X))`: یک آرایه (لیستی از اعداد) می‌سازه که تمام عناصرش 1 هستن و تعدادشون به اندازه‌ی `len(X)` هست. چرا 1؟ چون در رگرسیون، معمولاً یک **عرض از مبدأ (Intercept)** یا **بایاس (Bias)** داریم که ضریبش همیشه در 1 ضرب میشه. این 1 ها ستون اول ماتریس ورودی ما رو تشکیل میدن.
      * `np.c_[]`: این یک امکان در `numpy` هست که به ما اجازه میده آرایه‌ها رو **کنار هم (Column-wise)** بچسبونیم. اینجا داریم اون ستون 1 ها رو به عنوان شروع ماتریس `X_poly` قرار میدیم.
  * **`for i in range(1, degree + 1):`**: این یک **حلقه `for` (For Loop)** هست. حلقه‌ها برای تکرار یک سری عملیات استفاده میشن. اینجا، این حلقه از `i = 1` شروع میشه و تا `degree` ادامه پیدا می‌کنه (مثلاً اگر `degree` 2 باشه، `i` یک بار 1 میشه و یک بار 2).
  * **`X_poly = np.c_[X_poly, X**i]`**: داخل حلقه، هر بار یک ستون جدید به `X_poly` اضافه میشه:
      * `X**i`: یعنی `X` به توان `i`. مثلاً اگر `i` برابر 2 باشه، `X` به توان 2 می‌رسه.
      * `np.c_[X_poly, ...]` : این ستون جدید (`X**i`) رو به `X_poly` موجود اضافه می‌کنه.
      * **نتیجه این تابع**: در نهایت، `X_poly` یک ماتریس (مثل یک جدول) میشه که ستون اولش همه‌اش 1 هست، ستون دومش `X` اصلیه، ستون سومش `X^2` هست، و همینطور تا `X^degree` ادامه پیدا می‌کنه. این ماتریس جدید رو **ماتریس ویژگی‌های چندجمله‌ای (Polynomial Features Matrix)** میگن.

### ۳. تابع `polynomial_regression`: پیاده‌سازی رگرسیون چندجمله‌ای

```python
def polynomial_regression(X, y, degree):
    X_poly = polynomial_features(X, degree)
    # Closed-form solution: w = (X'^T * X')^-1 * X'^T * y
    w = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)
    return w
```

این تابع مدل رگرسیون چندجمله‌ای رو می‌سازه و **پارامترهای (Parameters)** مدل رو محاسبه می‌کنه. پارامترها (که اینجا با `w` نشون داده میشن) همون ضریب‌های $a, b, c$ و ... در معادله‌ی چندجمله‌ای ما هستند.

  * **`def polynomial_regression(X, y, degree):`**: تعریف تابع `polynomial_regression` با سه ورودی `X` (داده‌های ورودی), `y` (خروجی‌های واقعی که می‌خوایم پیش‌بینی کنیم، مثلاً نمره‌ی امتحان), و `degree`.

  * **`X_poly = polynomial_features(X, degree)`**: اینجا تابع `polynomial_features` رو که بالا تعریف کردیم، صدا می‌زنیم تا ماتریس ویژگی‌های چندجمله‌ای رو بسازه.

  * **`w = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)`**: این خط مهم‌ترین قسمت این تابع هست. این فرمول **راه حل تحلیلی (Analytical Solution)** یا **فرم بسته (Closed-form Solution)** برای پیدا کردن پارامترهای `w` در رگرسیون خطی/چندجمله‌ایه. اجازه بده جزئیاتش رو بگم:

      * `X_poly.T`: یعنی **ترانهاده (Transpose)** ماتریس `X_poly`. ترانهاده یعنی سطرها و ستون‌های ماتریس جابجا میشن.
      * `.dot(...)`: این متد برای **ضرب ماتریسی (Matrix Multiplication)** در `numpy` استفاده میشه.
      * `X_poly.T.dot(X_poly)`: ضرب ترانهاده `X_poly` در خود `X_poly`.
      * `np.linalg.inv(...)`: این تابع **معکوس (Inverse)** یک ماتریس رو محاسبه می‌کنه.
      * **کل عبارت**: در نهایت، کل این فرمول `w = (X'^T * X')^-1 * X'^T * y` پارامترهای `w` رو به ما میده که بهترین منحنی چندجمله‌ای رو برای داده‌های ما نشون میده. (از نظر ریاضی، این فرمول از روش **کمترین مربعات (Least Squares)** مشتق شده).

  * **`return w`**: تابع، بردار `w` (همون پارامترهای مدل) رو برمی‌گردونه.

  * **`m = 5`**: یک **متغیر (Variable)** به نام `m` تعریف شده و مقدار 5 بهش داده شده. این `m` همون `degree` یا درجه‌ی چندجمله‌ای هست که قراره برای مدل استفاده بشه.

  * **`w_poly = polynomial_regression(X, y, m)`**: تابع `polynomial_regression` رو با داده‌های `X`, `y` و درجه `m` (که 5 هست) صدا می‌زنیم. نتیجه (پارامترهای `w`) در متغیر `w_poly` ذخیره میشه.

  * **`print(f"Parameters (w) for Degree {m}: {w_poly}")`**: این خط پارامترهای محاسبه شده رو چاپ می‌کنه. `f-string` یک روش راحت برای فرمت کردن رشته‌ها در پایتونه که به شما اجازه میده متغیرها رو مستقیماً داخل رشته قرار بدید.

### ۴. `X` و `y` از کجا میان؟ (نکته مهم)

در کدی که شما دادی، `X` و `y` تعریف نشدن. این `X` و `y` همون داده‌های ورودی و خروجی ما هستند. معمولاً قبل از این قسمت کد، این داده‌ها تولید یا از یک فایل خونده میشن. مثلاً ممکنه اینطور تعریف شده باشن:

```python
# مثال: تولید داده‌های X و y (شبیه سازی)
np.random.seed(0) # برای اینکه هر بار اجرای کد، نتایج یکسان باشه
X = 2 * np.random.rand(100, 1) # 100 نقطه داده تصادفی بین 0 و 2
y = 4 + 3 * X + 2 * X**2 + np.random.randn(100, 1) # y با یک رابطه چندجمله‌ای و کمی نویز (اختلاف تصادفی)
```

**قبل از اجرای این کد، حتماً باید X و y رو تعریف کنی یا از جایی لود کنی.**

### ۵. `Visualize the Polynomial Fit`: بصری‌سازی مدل چندجمله‌ای

```python
X_fit = np.linspace(X.min(), X.max(), 200)
X_fit_poly = polynomial_features(X_fit, m)
y_poly_pred = X_fit_poly.dot(w_poly) # h_w(x) = X' * w

# Plot the actual data and the polynomial fit
plt.scatter(X, y, color='blue', label='Actual Data')
plt.plot(X_fit, y_poly_pred, color='green', label=f'Polynomial Fit (Degree {m})')
plt.title(f"Polynomial Regression (Degree {m})")
plt.xlabel("$x$")
plt.ylabel("$y$")
plt.legend()
plt.show()
```

این بخش برای رسم نمودار و دیدن اینکه مدل چقدر خوب روی داده‌ها فیت شده، استفاده میشه.

  * **`X_fit = np.linspace(X.min(), X.max(), 200)`**:
      * `X.min()` و `X.max()`: کوچکترین و بزرگترین مقدار در داده‌های `X` رو پیدا می‌کنه.
      * `np.linspace(start, stop, num)`: این تابع `numpy` یک آرایه از `num` عدد رو به صورت مساوی توزیع شده (با فواصل یکسان) بین `start` و `stop` ایجاد می‌کنه. اینجا 200 نقطه بین حداقل و حداکثر `X` ایجاد میشه. این `X_fit` برای رسم یک منحنی صاف استفاده میشه.
  * **`X_fit_poly = polynomial_features(X_fit, m)`**: با استفاده از `X_fit` (نقاط برای رسم منحنی صاف) و درجه `m`، ویژگی‌های چندجمله‌ای رو می‌سازیم.
  * **`y_poly_pred = X_fit_poly.dot(w_poly)`**: این خط مهم‌ترین بخش برای **پیش‌بینی (Prediction)** هست. اینجا، ماتریس ویژگی‌های `X_fit_poly` رو در پارامترهای `w_poly` که قبلاً محاسبه کردیم، ضرب می‌کنیم. نتیجه، مقادیر `y` پیش‌بینی شده (`y_poly_pred`) توسط مدل ما روی نقاط `X_fit` هست. این همون $h\_w(x) = X' \* w$ هست که در توضیحات اشاره شده.
  * **`plt.scatter(X, y, color='blue', label='Actual Data')`**: این خط داده‌های اصلی ما (`X` و `y`) رو به صورت **نقاط (Scatter Plot)** آبی رنگ روی نمودار رسم می‌کنه. `label` برای نمایش در `legend` (راهنما) استفاده میشه.
  * **`plt.plot(X_fit, y_poly_pred, color='green', label=f'Polynomial Fit (Degree {m})')`**: این خط منحنی مدل چندجمله‌ای پیش‌بینی شده رو به صورت یک خط سبز رنگ روی نمودار رسم می‌کنه.
  * **`plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.legend()`**: این‌ها برای اضافه کردن عنوان به نمودار، برچسب به محورها، و نمایش راهنما (که `label`ها رو نشون میده) استفاده میشن.
  * **`plt.show()`**: این خط نمودار رو نمایش میده.

-----

## `Visualizing $E_{rms}$`: ارزیابی عملکرد مدل با RMSE

**RMSE (Root Mean Square Error)** یا **خطای میانگین مربع ریشه**، یک معیار خیلی مهم برای ارزیابی عملکرد مدل‌های رگرسیونه. این معیار به ما میگه که به طور متوسط، پیش‌بینی‌های مدل ما چقدر از مقادیر واقعی فاصله دارن. هرچی RMSE کمتر باشه، مدل ما بهتره.

  * **`def compute_rms_error(y_true, y_pred):`**: تعریف تابعی برای محاسبه RMSE که دو ورودی می‌گیره:
      * `y_true`: مقادیر واقعی `y`.
      * `y_pred`: مقادیر `y` که توسط مدل پیش‌بینی شده.
  * **`return np.sqrt(np.mean((y_true - y_pred) ** 2))`**: این خط فرمول RMSE رو پیاده‌سازی می‌کنه:
      * `(y_true - y_pred)`: اختلاف بین مقدار واقعی و پیش‌بینی شده (همون خطا).
      * `(... ** 2)`: مربع کردن خطاها (تا خطاهای منفی و مثبت با هم جمع نشن و خطاهای بزرگتر جریمه‌ی بیشتری بشن).
      * `np.mean(...)`: میانگین مربع خطاها رو محاسبه می‌کنه.
      * `np.sqrt(...)`: ریشه‌ی دوم میانگین مربع خطاها رو می‌گیره تا به واحد اصلی `y` برگرده.

-----

## `Visualizing RMSE for different Polynomial degrees`: بررسی RMSE برای درجات مختلف

این بخش کد، یک کار خیلی مهم رو انجام میده: **انتخاب بهترین درجه (Degree)** برای مدل. اگر درجه خیلی کم باشه، مدل ممکنه نتونه رابطه‌ی واقعی رو یاد بگیره (به این میگن **Underfitting**). اگر درجه خیلی زیاد باشه، مدل ممکنه جزئیات بی‌اهمیت و نویز داده رو هم یاد بگیره و روی داده‌های جدید خوب عمل نکنه (به این میگن **Overfitting**).

### تقسیم داده‌ها به Train و Test

```python
from sklearn.model_selection import train_test_split

# Split the data into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

برای جلوگیری از Overfitting و ارزیابی صحیح مدل، داده‌ها رو به دو بخش تقسیم می‌کنیم:

  * **`train_test_split(X, y, test_size=0.2, random_state=42)`**:
      * `X, y`: داده‌های ورودی و خروجی ما.
      * `test_size=0.2`: یعنی 20 درصد از داده‌ها برای بخش **آزمایش (Test Set)** و 80 درصد برای بخش **آموزش (Training Set)** استفاده میشه.
      * `random_state=42`: این عدد باعث میشه هر بار که کد رو اجرا می‌کنیم، تقسیم‌بندی داده‌ها به Train و Test دقیقاً یکسان باشه. این برای **تکرارپذیری (Reproducibility)** نتایج خیلی مهمه.
  * **`X_train, X_test, y_train, y_test`**: متغیرهایی که داده‌های تقسیم شده رو نگه می‌دارند. `_train` برای آموزش و `_test` برای آزمایش.

### حلقه برای درجات مختلف و محاسبه RMSE

```python
degrees = range(0, 9)
train_rms_errors = []
test_rms_errors = []

for d in degrees:
    # Train the model on the training set
    w_poly = polynomial_regression(X_train, y_train, d)

    # Compute predictions for the training set
    X_train_poly = polynomial_features(X_train, d)
    y_train_pred = X_train_poly.dot(w_poly)

    # Compute predictions for the test set
    X_test_poly = polynomial_features(X_test, d)
    y_test_pred = X_test_poly.dot(w_poly)

    # Calculate RMSE for both training and test sets
    train_rms_error = compute_rms_error(y_train, y_train_pred)
    test_rms_error = compute_rms_error(y_test, y_test_pred)

    # Store the errors
    train_rms_errors.append(train_rms_error)
    test_rms_errors.append(test_rms_error)

    # Print the RMSE for the current degree
    print(f"Degree {d}: Train RMSE = {train_rms_error:.2f}, Test RMSE = {test_rms_error:.2f}")

    # Plot the polynomial fit on the training data
    plt.scatter(X_train, y_train, color='blue', label="Training Data")
    plt.scatter(X_test, y_test, color='red', label="Test Data", alpha=0.6)
    X_fit = np.linspace(X.min(), X.max(), 200)
    X_fit_poly = polynomial_features(X_fit, d)
    y_fit_pred = X_fit_poly.dot(w_poly)
    plt.plot(X_fit, y_fit_pred, label=f"Degree {d} Fit", color='green')
    plt.title(f"Polynomial Regression (Degree {d})")
    plt.xlabel("$x$")
    plt.ylabel("$y$")
    plt.legend()
    plt.show()
```

  * **`degrees = range(0, 9)`**: یک لیست از اعداد 0 تا 8 (درجه‌ها) رو ایجاد می‌کنه.
  * **`train_rms_errors = []` و `test_rms_errors = []`**: دو لیست خالی برای ذخیره RMSE آموزش و آزمایش در هر درجه.
  * **`for d in degrees:`**: یک حلقه `for` دیگه که برای هر درجه `d` (از 0 تا 8) کارهای زیر رو تکرار می‌کنه:
      * **`w_poly = polynomial_regression(X_train, y_train, d)`**: مدل رو روی **داده‌های آموزش (`X_train`, `y_train`)** با درجه `d` آموزش میده و پارامترهای `w` رو محاسبه می‌کنه. **خیلی مهمه که مدل فقط روی داده‌های آموزش ببینه.**
      * **محاسبه پیش‌بینی برای داده‌های آموزش و آزمایش**:
          * `y_train_pred`: پیش‌بینی‌های مدل روی `X_train`.
          * `y_test_pred`: پیش‌بینی‌های مدل روی `X_test`.
      * **محاسبه RMSE برای آموزش و آزمایش**:
          * `train_rms_error = compute_rms_error(y_train, y_train_pred)`: RMSE برای داده‌های آموزشی محاسبه میشه.
          * `test_rms_error = compute_rms_error(y_test, y_test_pred)`: RMSE برای **داده‌های آزمایشی** محاسبه میشه. این **مهم‌ترین معیار** برای ارزیابی عملکرد واقعی مدل روی داده‌های "دیده نشده" است.
      * **ذخیره خطاها**: RMSEهای محاسبه شده به لیست‌های `train_rms_errors` و `test_rms_errors` اضافه میشن.
      * **چاپ RMSE**: RMSE هر درجه رو چاپ می‌کنه.
      * **رسم نمودار برای هر درجه**: برای هر درجه `d`، یک نمودار جداگانه رسم میشه که نقاط آموزش (آبی)، نقاط آزمایش (قرمز) و منحنی فیت شده (سبز) رو نشون میده. این به شما کمک می‌کنه **بصری (visually)** ببینید که مدل در هر درجه چطور روی داده‌ها فیت میشه.

### رسم نمودار نهایی RMSE

```python
# Plot RMSE for training and test sets
plt.plot(degrees, train_rms_errors, marker='o', linestyle='-', color='blue', label='Train RMSE')
plt.plot(degrees, test_rms_errors, marker='o', linestyle='-', color='red', label='Test RMSE')
plt.title("Train vs Test RMSE vs Polynomial Degree")
plt.xlabel("Polynomial Degree")
plt.ylabel("RMSE")
plt.xticks(degrees)
plt.grid(True)
plt.legend()
plt.show()
```

در پایان، این کد یک نمودار کلی رسم می‌کنه که تغییرات RMSE آموزش و آزمایش رو بر حسب درجه چندجمله‌ای نشون میده.

  * **`plt.plot(...)`**: دو خط رو روی نمودار رسم می‌کنه: یکی برای `train_rms_errors` (آبی) و دیگری برای `test_rms_errors` (قرمز).
      * **`marker='o'`**: نقاط رو با دایره نشون میده.
      * **`linestyle='-'`**: خطوط رو به صورت پیوسته نشون میده.
  * **`plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`, `plt.xticks(...)`, `plt.grid(True)`, `plt.legend()`, `plt.show()`**: این‌ها هم برای بهتر شدن ظاهر نمودار و نمایش اطلاعات بیشتر هستن.

**تفسیر این نمودار نهایی خیلی مهمه:**

  * معمولاً با افزایش درجه، `Train RMSE` کم میشه (مدل روی داده‌های آموزش بهتر فیت میشه).
  * اما `Test RMSE` در ابتدا کم میشه و بعد از یک نقطه‌ای شروع به افزایش می‌کنه. **نقطه‌ای که `Test RMSE` حداقل مقدار رو داره، بهترین درجه برای مدل شماست.** بعد از اون نقطه، مدل دچار **Overfitting** میشه.

-----

## جمع‌بندی و مراحل بعدی

خب، تا اینجا با هم همه‌ی قسمت‌های کد رو بررسی کردیم و توضیح دادم که هر بخش چی کار می‌کنه و سینتکس پایتونیش چیه و چه مفاهیم یادگیری ماشینی پشتش هست.

**خلاصه:**
این کد یک پیاده‌سازی از رگرسیون چندجمله‌ای هست که:

1.  **ویژگی‌های چندجمله‌ای** رو از داده‌های اصلی می‌سازه.
2.  با استفاده از **فرمول تحلیلی**، بهترین پارامترهای مدل رو پیدا می‌کنه.
3.  عملکرد مدل رو با **RMSE** ارزیابی می‌کنه.
4.  با استفاده از **تقسیم داده‌ها به Train/Test**، به شما کمک می‌کنه تا بهترین درجه برای مدل رو انتخاب کنید و از **Overfitting** جلوگیری کنید.
5.  نتایج رو با **نمودارها** بصری‌سازی می‌کنه.

-----




$w = (X^T \* X)^{-1} \* X^T \* y$

این فرمول **راه حل تحلیلی (Analytical Solution)** یا **فرم بسته (Closed-form Solution)** برای روش **کمترین مربعات معمولی (Ordinary Least Squares - OLS)** هست. این روش به دنبال پیدا کردن پارامترهایی ($w$) هست که **مجموع مربعات خطاها (Sum of Squared Errors)** رو کمینه کنه.

### پس فرق کجاست؟

تفاوت کلیدی بین رگرسیون خطی و رگرسیون چندجمله‌ای، در **ماتریس ورودی $X$** است که به این فرمول داده میشه. اجازه بده تا با جزئیات بیشتر توضیح بدم:

-----

### ۱. رگرسیون خطی (Linear Regression)

در رگرسیون خطی ساده (که در کد `generate_data` و `linear_regression_closed_form` دیدی)، فرض می‌کنیم که رابطه بین $X$ و $y$ یک خط مستقیمه:

$h\_w(x) = w\_0 + w\_1 \* x$

برای اینکه این معادله رو به فرم ماتریسی مناسب فرمول OLS بنویسیم، ماتریس ورودی $X$ رو به شکل `X_b` در میاریم. `X_b` شامل یک ستون از یک‌ها برای ضریب $w\_0$ (عرض از مبدأ یا Bias) و یک ستون برای مقادیر اصلی $X$ (ضریب $w\_1$) میشه:

```python
# در کد خطی شما:
# X_b = np.c_[np.ones((len(X), 1)), X]
# مثلاً اگر X فقط 3 تا داده داشته باشه:
# X = [[x1], [x2], [x3]]
# X_b میشه:
# [[1, x1],
#  [1, x2],
#  [1, x3]]
```

این ماتریس `X_b` به فرمول $w = (X^T \* X)^{-1} \* X^T \* y$ داده میشه و `w` (که شامل $w\_0$ و $w\_1$ هست) رو محاسبه می‌کنه.

-----

### ۲. رگرسیون چندجمله‌ای (Polynomial Regression)

در رگرسیون چندجمله‌ای، ما می‌خوایم یک رابطه‌ی منحنی رو مدل کنیم. مثلاً برای یک چندجمله‌ای درجه 2، معادله ما این شکلی میشه:

$h\_w(x) = w\_0 + w\_1 \* x + w\_2 \* x^2$

اینجا هم ما همچنان یک **رابطه "خطی"** بین **پارامترهای $w$** داریم، اما نه بین $x$ و $y$. برای اینکه بتونیم از همون فرمول OLS استفاده کنیم، باید کاری کنیم که $x^2$ (و هر توان دیگه از $x$) به عنوان یک **"ویژگی (Feature)" جدید** برای مدل در نظر گرفته بشه.

اینجاست که تابع `polynomial_features` وارد عمل میشه. این تابع، ماتریس $X$ اولیه رو "تبدیل" می‌کنه و ستون‌های جدیدی شامل $x^2, x^3, \\dots, x^{degree}$ به اون اضافه می‌کنه.

```python
# در کد چندجمله‌ای:
# X_poly = polynomial_features(X, degree)
# مثلاً اگر X فقط 3 تا داده داشته باشه و degree=2:
# X = [[x1], [x2], [x3]]
# X_poly میشه:
# [[1, x1, x1^2],
#  [1, x2, x2^2],
#  [1, x3, x3^2]]
```

حالا این ماتریس `X_poly` که شامل ویژگی‌های چندجمله‌ای هست، به جای `X_b` (در حالت خطی) به فرمول $w = (X^T \* X)^{-1} \* X^T \* y$ داده میشه. نتیجه‌ی این محاسبه، بردار `w` خواهد بود که شامل $w\_0, w\_1, w\_2, \\dots, w\_{degree}$ هست.

### مقایسه‌ی X ورودی به فرمول:

| ویژگی | رگرسیون خطی | رگرسیون چندجمله‌ای |
|---|---|---|
| **رابطه‌ی مدل** | خط مستقیم: $y = w\_0 + w\_1x$ | منحنی: $y = w\_0 + w\_1x + w\_2x^2 + \\dots + w\_nx^n$ |
| **ماتریس $X$ ورودی به فرمول** | `X_b` (شامل `[1, x]`) | `X_poly` (شامل `[1, x, x^2, ..., x^n]`) |
| **نقش `X` اصلی** | مستقیماً به عنوان یک ستون در ماتریس ورودی استفاده میشه. | از `X` اصلی، **ویژگی‌های جدیدی** (توان‌های مختلف `X`) تولید میشه و این ویژگی‌های جدید به عنوان ستون در ماتریس ورودی استفاده میشن. |
| **تعداد پارامترها (`w`)** | 2 (یکی برای عرض از مبدأ، یکی برای ضریب $x$) | `degree + 1` (یکی برای عرض از مبدأ، یکی برای $x^1$, یکی برای $x^2$ و ...) |
| **نوع `polynomial_features`** | لازم نیست (فقط `np.c_[np.ones(...), X]` کافیه) | ضروری برای تبدیل $X$ به فرم چندجمله‌ای. |

-----

### مثال برای درک بهتر:

فرض کن می‌خوایم رابطه‌ی بین **اندازه‌ی خانه** و **قیمت خانه** رو مدل کنیم.

  * **رگرسیون خطی:**
      * مدل می‌گه: `قیمت = w0 + w1 * اندازه_خانه`
      * ماتریس $X$ (برای فرمول): `[[1, اندازه1], [1, اندازه2], ...]`
  * **رگرسیون چندجمله‌ای (درجه 2):**
      * مدل می‌گه: `قیمت = w0 + w1 * اندازه_خانه + w2 * (اندازه_خانه)^2`
      * ماتریس $X$ (برای فرمول): `[[1, اندازه1, (اندازه1)^2], [1, اندازه2, (اندازه2)^2], ...]`

می‌بینی؟ **همون فرمول** OLS استفاده میشه، ولی ورودی‌ای که بهش میدیم (ماتریس $X$) در حالت چندجمله‌ای "غنی‌تر" شده با اضافه کردن ستون‌هایی برای توان‌های مختلف متغیر ورودی. این تبدیل ورودی به توان‌های بالاتر، به مدل اجازه میده تا منحنی‌ها رو فیت کنه، در حالی که از نظر ریاضی، مسئله همچنان به عنوان یک "رگرسیون خطی" از جنس پارامترها حل میشه.

-----

**پس خلاصه‌ی اصلی اینه:**

فرمول ریاضی برای یافتن پارامترها (`w`) در هر دو حالت خطی و چندجمله‌ای (با استفاده از روش OLS) یکسانه. چیزی که تغییر می‌کنه، **ساختار و محتوای ماتریس ورودی `X`** است که به این فرمول داده میشه. در رگرسیون چندجمله‌ای، ما با افزودن توان‌های بالاتر متغیرهای ورودی به `X`، اجازه می‌دهیم مدل روابط غیرخطی را یاد بگیرد، در حالی که مسئله را همچنان به صورت خطی در پارامترها (ضرایب $w$) حل می‌کنیم.

-----


