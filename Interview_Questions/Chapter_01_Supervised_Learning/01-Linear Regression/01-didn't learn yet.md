Regularization 

---

**چگونه می‌توان از بیش‌برازش جلوگیری کرد؟**

**پاسخ**:
استفاده از تنظیم‌سازی (Regularization)، داده‌های اعتبارسنجی، و Cross-Validation.

---

**بیش‌برازش چیست و چگونه می‌توان از آن جلوگیری کرد؟**

**پاسخ**:
بیش‌برازش زمانی رخ می‌دهد که مدل بیش از حد به داده‌های آموزشی وابسته شود و روی داده‌های جدید ضعیف عمل کند. برای جلوگیری: منظم‌سازی (ریج، لاسو)، افزایش داده، و اعتبارسنجی متقاطع.

---

**راه‌های جلوگیری از Overfitting؟**

**پاسخ**:
برای جلوگیری از overfitting می‌توان از روش‌هایی مانند:

* Regularization
* Dropout
* استفاده از داده‌های بیشتر
* مدل ساده‌تر استفاده کرد.

---

**منظور از Regularization چیست؟**

**پاسخ**:
برای جلوگیری از پیچیدگی بیش‌ازحد مدل.

---

**Regularization چگونه از overfitting جلوگیری می‌کند؟**

**پاسخ**:
Regularization با اضافه کردن جریمه به وزن‌ها، مدل را مجبور می‌کند که ویژگی‌های مهم‌تر را انتخاب کرده و از پیچیدگی زیاد خودداری کند. این کار باعث جلوگیری از بیش‌برازش (overfitting) و بهبود تعمیم‌پذیری مدل می‌شود.

---

**چه زمانی از Regularization استفاده می‌کنیم؟**

**پاسخ**:
Regularization زمانی استفاده می‌شود که مدل دچار overfitting شود و نیاز به کاهش پیچیدگی مدل باشد، تا از یادگیری نویز و جزئیات غیرضروری جلوگیری شود.

---

**تفاوت L1 و L2 Regularization چیست؟**

**پاسخ**:
L1 وزن‌ها را به صفر می‌رساند (انتخاب ویژگی)، L2 وزن‌ها را کوچک می‌کند.

---

**تفاوت L1 و L2 چیست؟**

**پاسخ**:

* **L1 Regularization (Lasso)** ویژگی‌ها را به صفر می‌رساند و باعث sparsity می‌شود (ویژگی‌های غیرمفید حذف می‌شوند).
* **L2 Regularization (Ridge)** وزن‌ها را کوچک می‌کند، اما آن‌ها را به صفر نمی‌رساند.

---

**آیا L1 همیشه بهتر از L2 است؟**

**پاسخ**:
نه، بسته به داده‌ها و هدف مدل، ممکن است L2 عملکرد بهتری داشته باشد. L1 بیشتر برای انتخاب ویژگی‌ها مفید است، در حالی که L2 به مدل کمک می‌کند که ویژگی‌ها را با دقت بیشتر تنظیم کند.

---

**تفاوت L1 و L2 Regularization در نتایج نهایی چیست؟**

**پاسخ**:

* **L1 Regularization (Lasso)** تمایل دارد که برخی وزن‌ها را به صفر برساند، که این ویژگی به ویژگی‌گزینی (Feature Selection) منجر می‌شود.
* **L2 Regularization (Ridge)** وزن‌ها را کاهش می‌دهد اما به‌ندرت آن‌ها را به صفر می‌رساند. این کار مدل را نرم‌تر می‌کند و پیچیدگی آن را کاهش می‌دهد بدون حذف کامل ویژگی‌ها.

در مسائل با تعداد زیاد ویژگی‌ها (high-dimensional) اگر فرض کنیم که برخی ویژگی‌ها بی‌ارزش هستند، استفاده از L1 مناسب‌تر است.

---

**روش‌های منظم‌سازی مانند ریج و لاسو چگونه کار می‌کنند؟**

**پاسخ**:
ریج با افزودن جریمه $L_2$:

$$
\lambda \sum \beta_i^2
$$

و لاسو با جریمه $L_1$:

$$
\lambda \sum |\beta_i|
$$

به تابع هزینه، پیچیدگی مدل را کاهش می‌دهند و از بیش‌برازش جلوگیری می‌کنند.

---

### **مدل‌های پیشرفته‌تر و مشکلات داده**

---

**چرا رگرسیون چندجمله‌ای ممکن است بیش‌برازش کند؟**

**پاسخ**:
چون با افزایش درجه، مدل ممکن است نویز داده‌ها را هم یاد بگیرد.

---

**چرا مدل‌های پیچیده با درجات بالای چندجمله‌ای (Polynomial Regression) معمولاً دچار Overfitting می‌شوند؟**

**پاسخ**:
در رگرسیون چندجمله‌ای با درجات بالا، مدل قادر است دقیقاً با داده‌های آموزشی تطابق پیدا کند، از جمله نویزهای تصادفی که در داده‌ها وجود دارد. این انعطاف‌پذیری زیاد باعث می‌شود که مدل الگوی کلی داده‌ها را درک نکند و به جای آن، به جزئیات و نوسانات تصادفی پاسخ دهد. در نتیجه، مدل عملکرد ضعیفی روی داده‌های جدید خواهد داشت. برای جلوگیری از این مشکل می‌توان از روش‌هایی مانند Regularization یا کاهش درجه چندجمله‌ای استفاده کرد.

---

**در رگرسیون چندجمله‌ای، چگونه می‌توان از بیش‌براش جلوگیری کرد؟**

**پاسخ**:
برای جلوگیری از بیش‌براش در رگرسیون چندجمله‌ای، می‌توان از تکنیک‌های مختلفی استفاده کرد:

* **رگولاریزاسیون (Regularization)**: با اضافه کردن یک ترم جریمه به تابع هزینه (مانند L2 regularization)، می‌توان وزن‌های بزرگ را کاهش داده و پیچیدگی مدل را کنترل کرد. این کار باعث می‌شود که مدل از پیچیدگی اضافی که ممکن است منجر به بیش‌براش شود، جلوگیری کند.
* **انتخاب درجه بهینه**: به جای استفاده از درجات بالای زیاد (که ممکن است منجر به بیش‌براش شود)، باید از انتخاب دقتی درجه مدل استفاده کرد. این کار می‌تواند شامل استفاده از اعتبارسنجی متقاطع (cross-validation) برای تعیین درجه مناسب باشد.
* **استفاده از مدل‌های ساده‌تر**: گاهی اوقات استفاده از مدل‌های ساده‌تر مانند رگرسیون خطی یا رگرسیون چندجمله‌ای با درجات پایین می‌تواند بهتر از مدل‌های پیچیده‌تر باشد که ممکن است بیش از حد به داده‌های آموزشی فیت شوند.

---

**مفهوم "نفرین ابعاد" چیست و چگونه بر مدل‌های یادگیری ماشین تأثیر می‌گذارد؟**

**پاسخ**:
نفرین ابعاد به مشکلاتی اطلاق می‌شود که زمانی پیش می‌آید که تعداد ویژگی‌ها (ابعاد داده‌ها) بسیار زیاد می‌شود. افزایش ابعاد باعث کاهش تعمیم‌پذیری مدل، افزایش پیچیدگی، و نیاز به داده‌های بیشتر می‌شود.

---

**داده‌های پرت چگونه بر عملکرد رگرسیون خطی تأثیر می‌گذارند؟**

**پاسخ**:
داده‌های پرت می‌توانند ضرایب مدل را به‌شدت تحت تأثیر قرار دهند، زیرا این داده‌ها ممکن است الگویی غلط برای مدل ایجاد کنند. در نتیجه، خطای پیش‌بینی افزایش می‌یابد و مدل ممکن است قادر به تعمیم درست به داده‌های جدید نباشد.

---

**چگونه می‌توان اثر داده‌های پرت را در رگرسیون خطی کاهش داد؟**

**پاسخ**:
برای کاهش اثر داده‌های پرت می‌توان از روش‌های قوی رگرسیون (مثل رگرسیون قوی) استفاده کرد که در برابر داده‌های پرت مقاوم‌تر هستند، یا اینکه از حذف داده‌های پرت و وزن‌دهی به داده‌ها بهره برد تا تأثیر آن‌ها بر مدل کم شود.

---

**چرا داده‌های آموزشی باید نماینده داده‌های واقعی باشند؟**

**پاسخ**:
اگر داده‌های آموزشی نماینده‌ی داده‌های واقعی نباشند، مدل ممکن است الگوهای غلط یاد بگیرد و در داده‌های جدید عملکرد ضعیفی داشته باشد. بنابراین، برای تعادل و تعمیم‌پذیری بهتر، داده‌های آموزشی باید نماینده‌ای از دنیای واقعی باشند.

---

**مفهوم "افزایش داده" چیست و چگونه به تعمیم‌پذیری کمک می‌کند؟**

**پاسخ**:
افزایش داده فرآیند تولید داده‌های مصنوعی است که با تغییرات در داده‌های اصلی (مانند چرخش یا تغییر مقیاس تصاویر) انجام می‌شود. این کار باعث افزایش تنوع داده‌ها و جلوگیری از بیش‌برازش می‌شود و در نتیجه تعمیم‌پذیری مدل را بهبود می‌بخشد.

---

### **مدل‌های احتمالاتی و بایزی**

---

**مفهوم "تابع احتمال" در رگرسیون احتمالی چیست؟**

**پاسخ**:
تابع احتمال در رگرسیون احتمالی، توزیع احتمال پیش‌بینی‌ها را توصیف می‌کند. به‌عنوان مثال، در رگرسیون خطی، فرض می‌شود که خطای پیش‌بینی‌ها از توزیع نرمال با میانگین صفر و واریانس معین پیروی می‌کند.

---

**رگرسیون احتمالی چیست و چه تفاوتی با رگرسیون خطی دارد؟**

**پاسخ**:
رگرسیون احتمالی توزیع احتمالی خروجی‌ها را مدل می‌کند، نه فقط یک مقدار دقیق. بر خلاف رگرسیون خطی، عدم قطعیت را نیز در نظر می‌گیرد.

---

**توزیع نرمال در رگرسیون احتمالی چه نقشی دارد؟**

**پاسخ**:
توزیع نرمال اغلب برای مدل‌سازی خروجی‌ها یا خطاها استفاده می‌شود، زیرا بسیاری از پدیده‌ها در طبیعت از این توزیع پیروی می‌کنند.

---

**مفهوم "انحراف معیار" در رگرسیون احتمالی چیست؟**

**پاسخ**:
انحراف معیار در رگرسیون احتمالی میزان پراکندگی توزیع پیش‌بینی‌ها را نشان می‌دهد و به‌عنوان معیاری برای سنجش عدم قطعیت پیش‌بینی‌ها مورد استفاده قرار می‌گیرد.

---

**چگونه می‌توان توزیع پیش‌بینی‌ها را در رگرسیون احتمالی تحلیل کرد؟**
**پاسخ**:
برای تحلیل توزیع پیش‌بینی‌ها، می‌توان به میانگین، واریانس، و بازه‌های اطمینان توجه کرد. این بازه‌ها می‌توانند عدم قطعیت پیش‌بینی‌ها را به‌طور دقیق‌تری مشخص کنند.

---

**چرا مدل‌های احتمالی در پیش‌بینی‌های مالی مفید هستند؟**

**پاسخ**:
مدل‌های احتمالی با ارائه توزیع احتمالات و عدم قطعیت، به تحلیل ریسک و تصمیم‌گیری در شرایط نامطمئن کمک می‌کنند.

---

**رگرسیون بیزی چیست و چگونه از آن استفاده می‌شود؟**

**پاسخ**:
رگرسیون بیزی از اصول بیزی برای تخمین توزیع ضرایب استفاده می‌کند و با ترکیب دانش پیشین (prior) و داده‌ها، عدم قطعیت را مدل می‌کند.

---

**در رگرسیون چه فرضی در مورد نویز داریم؟**

**پاسخ**:
در رگرسیون معمولاً فرض می‌کنیم که نویز از توزیع نرمال با میانگین صفر و واریانس ثابت پیروی می‌کند.

---


**چرا در MLE فرض می‌کنیم نویز نرمال است؟**

**پاسخ**:
فرض نرمال بودن نویز به دلیل قضیه حد مرکزی است که می‌گوید ترکیب نویز ناشی از فرآیندهای مختلف معمولاً توزیع نرمال دارد. این فرض در بسیاری از پدیده‌های طبیعی درست است.

---

**در مدل رگرسیون با دیدگاه احتمالاتی، چرا از توزیع نرمال برای نویز استفاده می‌شود؟**

**پاسخ**:
توزیع نرمال به‌دلیل خواص ریاضی مناسب (تقارن، مشتق‌پذیری) و قضیه حد مرکزی انتخاب می‌شود. فرض می‌کنیم:

$$
y = f(x; w) + \epsilon, \quad \epsilon \sim N(0, \sigma^2)
$$

این فرض باعث می‌شود تخمین پارامترها با روش **Maximum Likelihood Estimation (MLE)** به ساده‌سازی تابع هزینه منجر شود (معادل MSE).

---

**در رگرسیون احتمالاتی چرا log-likelihood را بیشینه می‌کنیم؟**

**پاسخ**:
در رگرسیون احتمالاتی، بیشینه کردن log-likelihood معادل کمینه‌سازی SSE (مجموع مربعات خطا) است و پیاده‌سازی آن از لحاظ ریاضی راحت‌تر است.

---

**تعریف log-likelihood؟**

**پاسخ**:
Log-likelihood لگاریتم تابع درست‌نمایی است که به دلیل خواص ریاضیاتی، مشتق‌گیری و محاسبه آن را ساده‌تر می‌کند.

---

**چرا log-likelihood را بیشتر از likelihood استفاده می‌کنیم؟**

**پاسخ**:
چون log-likelihood باعث می‌شود که ضرب‌ها به جمع تبدیل شوند و مشتق‌گیری را ساده‌تر می‌کند.

---

**چه ارتباطی بین Maximum Likelihood و Least Squares وجود دارد؟**

**پاسخ کامل**:
اگر فرض کنیم که نویز در مدل ما نرمال و با واریانس ثابت است، آنگاه بیشینه‌سازی تابع درست‌نمایی (Maximum Likelihood) معادل کمینه‌سازی SSE (Sum of Squared Errors) خواهد بود. به عبارت دیگر:

$$
\arg \max_w \log L(w) \equiv \arg \min_w \sum (y_i - f(x_i))^2
$$

این پیوند بین آمار و یادگیری ماشین یکی از دلایل محبوبیت مدل‌های رگرسیون خطی است.

---

**چگونه نشان می‌دهیم که Maximizing Log-Likelihood معادل Minimizing MSE است؟**

**پاسخ**:
اگر نویز نرمال با میانگین صفر و واریانس $\sigma^2$ فرض شود، تابع لگاریتم درست‌نمایی به شکل زیر درمی‌آید:

$$
\log L = -\frac{1}{2\sigma^2} \sum (y_i - f(x_i; w))^2 + \text{const}
$$

با حذف ثابت‌ها، بیشینه‌سازی log-likelihood معادل کمینه‌سازی Sum of Squared Errors (MSE) می‌شود.

---

### **مدل‌های پارامتری و غیرپارامتری**

---

**تفاوت بین مدل‌های پارامتری و غیرپارامتری چیست؟**

**پاسخ**:
مدل‌های پارامتری (مثل رگرسیون خطی) تعداد محدودی پارامتر دارند، در حالی که مدل‌های غیرپارامتری (مثل KNN) ساختار منعطف‌تری دارند و می‌توانند پیچیدگی بیشتری را مدل کنند.

---
---

### **مدل‌های پارامتری و غیرپارامتری**

---

**چه زمانی از مدل‌های non-parametric استفاده می‌کنیم؟**

**پاسخ**:
مدل‌های non-parametric زمانی استفاده می‌شوند که نمی‌خواهیم هیچ فرض خاصی در مورد توزیع داده‌ها اعمال کنیم. این مدل‌ها می‌توانند از داده‌های خود برای ساخت مدل بدون نیاز به فرض‌های اولیه استفاده کنند.

---

**تفاوت مدل خطی و مدل غیرخطی چیست؟**

**پاسخ**:
مدل‌های خطی خروجی را به‌صورت ترکیب خطی از ورودی‌ها تولید می‌کنند (مثال:

$$
y = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n
$$

)، در حالی که مدل‌های غیرخطی قادرند روابط پیچیده‌تری بین ورودی‌ها و خروجی‌ها را مدل کنند (مثال: شبکه‌های عصبی، رگرسیون چندجمله‌ای، یا درخت‌های تصمیم).

---

### **تکنیک‌های پیشرفته و مقابله با مشکلات**

---

**چگونه می‌توان از بیش‌برازش جلوگیری کرد؟**

**پاسخ**:
برای جلوگیری از overfitting می‌توان از روش‌هایی مانند:

* **Regularization**
* **Dropout**
* **استفاده از داده‌های بیشتر**
* **مدل ساده‌تر استفاده کرد.**

---

**بیش‌برازش چیست و چگونه می‌توان از آن جلوگیری کرد؟**

**پاسخ**:
بیش‌برازش زمانی رخ می‌دهد که مدل بیش از حد به داده‌های آموزشی وابسته شود و روی داده‌های جدید ضعیف عمل کند. برای جلوگیری:

* **منظم‌سازی (ریج، لاسو)**
* **افزایش داده**
* **اعتبارسنجی متقاطع**.

---

**راه‌های جلوگیری از Overfitting؟**

**پاسخ**:
برای جلوگیری از overfitting می‌توان از روش‌هایی مانند:

* **Regularization**
* **Dropout**
* **استفاده از داده‌های بیشتر**
* **مدل ساده‌تر استفاده کرد.**

---

**منظور از Regularization چیست؟**

**پاسخ**:
Regularization برای جلوگیری از پیچیدگی بیش‌ازحد مدل استفاده می‌شود.

---

**Regularization چگونه از overfitting جلوگیری می‌کند؟**
**پاسخ**:
Regularization با اضافه کردن جریمه به وزن‌ها، مدل را مجبور می‌کند که ویژگی‌های مهم‌تر را انتخاب کرده و از پیچیدگی زیاد خودداری کند. این کار باعث جلوگیری از بیش‌برازش (overfitting) و بهبود تعمیم‌پذیری مدل می‌شود.

---

**چه زمانی از Regularization استفاده می‌کنیم؟**

**پاسخ**:
Regularization زمانی استفاده می‌شود که مدل دچار overfitting شود و نیاز به کاهش پیچیدگی مدل باشد، تا از یادگیری نویز و جزئیات غیرضروری جلوگیری شود.

---

### **رگولاریزاسیون (L1 و L2)**

---

**تفاوت L1 و L2 Regularization چیست؟**

**پاسخ**:
L1 وزن‌ها را به صفر می‌رساند (انتخاب ویژگی)، L2 وزن‌ها را کوچک می‌کند.

---

**تفاوت L1 و L2 چیست؟**

**پاسخ**:

* **L1 Regularization (Lasso)**: ویژگی‌ها را به صفر می‌رساند و باعث **sparsity** می‌شود (ویژگی‌های غیرمفید حذف می‌شوند).
* **L2 Regularization (Ridge)**: وزن‌ها را کوچک می‌کند، اما آن‌ها را به صفر نمی‌رساند.

---

**آیا L1 همیشه بهتر از L2 است؟**

**پاسخ**:
نه، بسته به داده‌ها و هدف مدل، ممکن است L2 عملکرد بهتری داشته باشد. L1 بیشتر برای انتخاب ویژگی‌ها مفید است، در حالی که L2 به مدل کمک می‌کند که ویژگی‌ها را با دقت بیشتر تنظیم کند.

---

**روش‌های منظم‌سازی مانند ریج و لاسو چگونه کار می‌کنند؟**

**پاسخ**:

* **ریج** با افزودن جریمه $L_2$:

$$
\lambda \sum \beta_i^2
$$

* **لاسو** با جریمه $L_1$:

$$
\lambda \sum |\beta_i|
$$

به تابع هزینه، پیچیدگی مدل را کاهش می‌دهند و از بیش‌برازش جلوگیری می‌کنند.

---

**تفاوت L1 و L2 Regularization در نتایج نهایی چیست؟**

**پاسخ**:

* **L1 Regularization (Lasso)**: تمایل دارد که برخی وزن‌ها را به صفر برساند، که این ویژگی به **ویژگی‌گزینی (Feature Selection)** منجر می‌شود.
* **L2 Regularization (Ridge)**: وزن‌ها را کاهش می‌دهد اما به‌ندرت آن‌ها را به صفر می‌رساند. این کار مدل را نرم‌تر می‌کند و پیچیدگی آن را کاهش می‌دهد بدون حذف کامل ویژگی‌ها.

در مسائل با تعداد زیاد ویژگی‌ها (high-dimensional) اگر فرض کنیم که برخی ویژگی‌ها بی‌ارزش هستند، استفاده از L1 مناسب‌تر است.

---

**در scikit-learn چطور یک مدل Ridge می‌سازید؟**

**پاسخ**:
برای ساخت یک مدل Ridge در scikit-learn، باید از کلاس **Ridge** استفاده کنید و پارامتر **alpha** را تنظیم کنید که میزان **regularization** را تعیین می‌کند. سپس می‌توانید مدل را با داده‌های آموزشی آموزش دهید:

```python
from sklearn.linear_model import Ridge
model = Ridge(alpha=1.0)  # مقدار alpha را می‌توان برای تنظیم regularization تغییر داد
model.fit(X_train, y_train)
```

---

**چه زمانی باید از رگرسیون لاسو (Lasso Regression) و رگرسیون ریج (Ridge Regression) استفاده کرد؟**

**پاسخ**:

* **رگرسیون لاسو (Lasso)**: این نوع رگولاریزاسیون از **L1 Regularization** استفاده می‌کند و می‌تواند به **انتخاب ویژگی‌ها** کمک کند. در شرایطی که تعداد زیادی ویژگی در داده‌ها وجود دارد و برخی از ویژگی‌ها ممکن است غیرمفید باشند، رگرسیون لاسو مفید است زیرا به ویژگی‌های غیرمفید وزن صفر می‌دهد.
* **رگرسیون ریج (Ridge)**: رگرسیون ریج از **L2 Regularization** استفاده می‌کند و می‌تواند به کاهش پیچیدگی مدل کمک کند بدون اینکه ویژگی‌ها را به طور کامل حذف کند. این روش برای مدل‌هایی مناسب است که داده‌های با ویژگی‌های همبسته دارند و مدل می‌تواند از این ویژگی‌ها به طور همزمان استفاده کند.

انتخاب بین لاسو و ریج بستگی به نوع داده‌ها و نیاز به انتخاب ویژگی‌ها دارد. اگر می‌خواهید برخی ویژگی‌ها را حذف کنید، لاسو مناسب است؛ در غیر این صورت، ریج می‌تواند انتخاب بهتری باشد.

---

### **اعتبارسنجی متقاطع (Cross-Validation)**

---

**Cross-Validation چیست؟**

**پاسخ**:
Cross-validation یک تکنیک برای ارزیابی مدل با تقسیم داده‌ها به چندین زیرمجموعه است.

---

**اعتبارسنجی متقاطع چیست و چرا استفاده می‌شود؟**

**پاسخ**:
اعتبارسنجی متقاطع (مانند **k-fold**) داده‌ها را به **k** زیرمجموعه تقسیم می‌کند و مدل را **k** بار آموزش و ارزیابی می‌کند تا عملکرد پایدار و تعمیم‌پذیری را بررسی کند.

---

**چرا از Cross-validation استفاده می‌کنیم؟**

**پاسخ**:
Cross-validation برای ارزیابی عملکرد مدل روی نمونه‌های مختلف داده‌ها استفاده می‌شود و کمک می‌کند تا واریانس ارزیابی کاهش یابد و مدل به‌طور موثرتری تعمیم‌پذیر شود.

---

**چند نوع Cross-validation داریم؟**

**پاسخ**:
چندین نوع Cross-validation وجود دارد، از جمله:

* **K-Fold**
* **Leave-One-Out**
* **Stratified K-Fold**

که هر کدام بسته به نوع داده‌ها و هدف مدل استفاده می‌شود.

---

**مزایای استفاده از K-Fold Cross-validation چیست؟**

**پاسخ**:
K-Fold Cross-validation مزایایی از جمله استفاده مؤثر از داده‌ها و ارزیابی پایدارتر عملکرد مدل بر روی مجموعه‌های مختلف داده‌ها را فراهم می‌آورد، که به تعمیم‌پذیری بهتر مدل کمک می‌کند.

---

### **هدف از Cross-Validation چیست و چرا مهم است؟**


**پاسخ**:
Cross-validation یک روش برای ارزیابی عملکرد مدل به شکلی مستقل از داده‌های آموزش و تست است.
در **K-Fold Cross-Validation**، داده‌ها به **K** قسمت تقسیم می‌شوند. مدل بر روی **K-1** قسمت آموزش داده می‌شود و سپس روی قسمت باقی‌مانده اعتبارسنجی می‌شود.
این روش کمک می‌کند که از **Overfitting** جلوگیری کنیم و مقدار بهینه‌ی **λ** یا درجه‌ی مدل را انتخاب کنیم.

---

### **چگونه می‌توان از K-fold Cross Validation برای ارزیابی عملکرد یک مدل یادگیری ماشین استفاده کرد؟**

**پاسخ**:
در **K-fold Cross Validation**، داده‌ها به **K** قسمت تقسیم می‌شوند. در هر دور، یکی از این بخش‌ها به عنوان مجموعه تست و باقی‌مانده به عنوان مجموعه آموزشی استفاده می‌شود. این فرآیند به تعداد **K** بار تکرار می‌شود، بنابراین هر نمونه داده حداقل یک بار به عنوان داده تست مورد استفاده قرار می‌گیرد. نتیجه عملکرد مدل، میانگین خطای به دست آمده از هر **K** تکرار است.
این روش باعث می‌شود که مدل به طور جامع‌تری ارزیابی شود و از تأثیرات تصادفی تقسیم‌بندی داده‌ها جلوگیری شود. این روش به کاهش واریانس ارزیابی مدل کمک کرده و مطمئن می‌شود که مدل در داده‌های جدید به خوبی عمل می‌کند.

---

### **چگونه می‌توان از اعتبارسنجی متقابل (Cross-Validation) برای ارزیابی عملکرد مدل استفاده کرد؟**

**پاسخ**:
اعتبارسنجی متقابل یکی از تکنیک‌های اصلی برای ارزیابی عملکرد مدل‌های یادگیری است که برای جلوگیری از **بیش‌براش** و **کم‌براش** استفاده می‌شود. در این روش:

* داده‌ها به **K** بخش تقسیم می‌شوند: مجموعه داده به **K** بخش مساوی تقسیم می‌شود (معمولاً 5 یا 10).
* **آموزش و ارزیابی مدل**: مدل **K** بار آموزش داده می‌شود و هر بار یک بخش به عنوان داده‌های اعتبارسنجی استفاده می‌شود، در حالی که بقیه بخش‌ها برای آموزش مدل به کار می‌روند.
* **میانگین نتایج**: پس از هر دور از ارزیابی، نتایج (مانند دقت، خطا، $R^2$) محاسبه شده و میانگین نتایج به عنوان ارزیابی نهایی مدل انتخاب می‌شود.

این روش به طور موثری عملکرد مدل را بر اساس داده‌های مختلف ارزیابی کرده و از انتخاب مدل‌هایی که ممکن است بر اساس تقسیم‌بندی‌های خاص داده بیش از حد بهینه شوند، جلوگیری می‌کند.

---

### **چگونه N-fold Cross-Validation می‌تواند به ارزیابی بهتر مدل کمک کند؟**

**پاسخ**:
در **N-fold Cross-Validation**، داده‌ها به **N** بخش تقسیم می‌شوند و هر بخش به نوبت به عنوان مجموعه تست و بقیه بخش‌ها به عنوان مجموعه آموزش استفاده می‌شوند. این فرآیند **N** بار تکرار می‌شود.
مزیت این روش این است که تمام داده‌ها به طور یکسان به عنوان داده‌های آموزش و تست استفاده می‌شوند، که موجب افزایش دقت و کاهش واریانس ارزیابی می‌شود و احتمال تصادفی بودن نتایج را کاهش می‌دهد.

---

### **سایر مفاهیم و الگوریتم‌ها**

---

### **مفهوم "نفرین ابعاد" چیست و چگونه بر مدل‌های یادگیری ماشین تأثیر می‌گذارد؟**

**پاسخ**:
نفرین ابعاد به مشکلاتی اطلاق می‌شود که زمانی پیش می‌آید که تعداد ویژگی‌ها (ابعاد داده‌ها) بسیار زیاد می‌شود. افزایش ابعاد باعث کاهش تعمیم‌پذیری مدل، افزایش پیچیدگی، و نیاز به داده‌های بیشتر می‌شود.

---

### **تفاوت بین رگرسیون خطی ساده و چندگانه چیست؟**

**پاسخ**:
رگرسیون خطی ساده تنها یک متغیر مستقل دارد، در حالی که رگرسیون چندگانه چندین متغیر مستقل را همزمان مدل می‌کند و روابط پیچیده‌تری را تحلیل می‌کند.

---

### **رگرسیون خطی چگونه می‌تواند با داده‌های غیرخطی کار کند؟**

**پاسخ**:
مدل‌های خطی می‌توانند با داده‌های غیرخطی کار کنند از طریق پیش‌پردازش ویژگی‌ها (Feature Engineering) یا بسط ویژگی‌ها (Polynomial Features) که فضای ویژگی‌های داده را به یک فضای خطی‌تر تبدیل می‌کند. به این ترتیب، مدل همچنان خطی باقی می‌ماند اما در فضای جدیدی که ویژگی‌های پیچیده‌تری دارد، عمل می‌کند.

**مثال**:

```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X_train)
```

---

### **چه تفاوتی بین مدل Discriminative و Generative چیست؟**

**پاسخ**:

* مدل‌های **Discriminative** به طور مستقیم تابع شرطی $p(y|x)$ را یاد می‌گیرند، که هدف آن تشخیص مرزهای بین کلاس‌ها است. مثالی از این مدل‌ها، رگرسیون لجستیک است.
* مدل‌های **Generative** توزیع مشترک $p(x, y)$ یا $p(x|y)$ و $p(y)$ را مدل می‌کنند، به این معنی که مدل علاوه بر یادگیری مرز بین کلاس‌ها، ویژگی‌های کلاس‌ها را نیز یاد می‌گیرد. مثالی از این مدل‌ها، **Naive Bayes** است.

در اکثر مسائل یادگیری ماشین، مدل‌های **Discriminative** ترجیح داده می‌شوند، زیرا معمولاً دقت بالاتری دارند.

---

### **تفاوت بین رگرسیون خطی و رگرسیون لجستیک چیست؟**

**پاسخ**:

* **رگرسیون خطی** برای پیش‌بینی مقادیر عددی (پیوسته) استفاده می‌شود.
* **رگرسیون لجستیک** برای پیش‌بینی مقادیر دسته‌ای (مانند 0 و 1) استفاده می‌شود. رگرسیون لجستیک از تابع **سیگموید** برای مدل‌سازی احتمالات استفاده می‌کند.

---

### **مفهوم Early Stopping چیست؟**

**پاسخ**:
**Early Stopping** یک تکنیک است که آموزش مدل را زمانی متوقف می‌کند که عملکرد آن بر روی داده‌های اعتبارسنجی شروع به بدتر شدن کند. این کار برای جلوگیری از **بیش‌براش** و کاهش زمان آموزش مفید است.

---

### **Batch size در آموزش چه تأثیری دارد؟**

**پاسخ**:
**Batch size** بر نوسان گرادیان، حافظه و سرعت یادگیری تأثیر دارد. اندازه‌های بزرگ‌تر ممکن است منجر به یادگیری پایدارتر شوند، اما نیاز به حافظه بیشتری دارند.

---

### **اگر تابع هزینه به‌صورت غیرمحدب باشد چه مشکلی ایجاد می‌شود؟**

**پاسخ کامل**:
اگر تابع هزینه به‌صورت غیرمحدب باشد، ممکن است الگوریتم به جای رسیدن به مینیمم سراسری، در مینیمم محلی گیر کند و نتایج بهینه‌ای به دست نیاید.

---

### **مزایای مدل‌های احتمالاتی نسبت به مدل‌های قطعی چیست؟**

**پاسخ کامل**:
مدل‌های احتمالاتی قابلیت مدل‌سازی عدم قطعیت را دارند و می‌توانند توزیع‌های احتمال برای خروجی‌ها پیش‌بینی کنند. این ویژگی در مقایسه با مدل‌های قطعی که تنها یک پیش‌بینی واحد ارائه می‌دهند، مفیدتر است.

---

### **در چه شرایطی Regularization به مدل آسیب می‌زند؟**

**پاسخ**:
اگر مقدار **λ** در **Regularization** بسیار بزرگ باشد، می‌تواند مدل را بیش از حد محدود کند و باعث **underfitting** شود، یعنی مدل قادر به یادگیری الگوهای داده‌ها نباشد.

---

### **چرا استفاده زیاد از ویژگی‌ها ممکن است عملکرد مدل را کاهش دهد؟**

**پاسخ**:
افزایش تعداد ویژگی‌ها می‌تواند باعث **curse of dimensionality** شود، به این معنا که با افزایش ابعاد داده، واریانس مدل نیز افزایش می‌یابد و کارایی آن کاهش می‌یابد.

---

### **در کدام شرایط dropout مؤثرتر از L2 است؟**

**پاسخ**:
در شبکه‌های عصبی عمیق که دارای لایه‌های زیاد هستند، استفاده از **Dropout** می‌تواند موثرتر از **L2 Regularization** باشد. **Dropout** به مدل کمک می‌کند تا از یادگیری بیش از حد وابستگی‌ها بین ویژگی‌ها جلوگیری کند.

---

### **تفاوت بین "Bagging" و "Boosting" در الگوریتم‌های یادگیری نظارت‌شده وجود دارد؟**

**پاسخ**:

* **Bagging (Bootstrap Aggregating)**: در این تکنیک، چندین مدل به طور مستقل و به صورت موازی بر روی زیرمجموعه‌های مختلف داده‌ها آموزش داده می‌شوند. این مدل‌ها معمولاً برای کاهش **واریانس** و جلوگیری از **بیش‌براش** استفاده می‌شوند.
* **Boosting**: در این روش، مدل‌ها


به صورت ترتیبی آموزش داده می‌شوند و هر مدل جدید سعی می‌کند خطاهای مدل قبلی را اصلاح کند. این مدل‌ها معمولاً برای کاهش **بایاس** استفاده می‌شوند و در نتیجه ممکن است به **بیش‌براش** منجر شوند.

---

### **مفهوم هم‌خطی (Multicollinearity) چیست و چگونه می‌توان آن را تشخیص داد؟**

**پاسخ**:
هم‌خطی زمانی رخ می‌دهد که ویژگی‌ها با یکدیگر همبستگی بالا دارند، که این می‌تواند باعث مشکلات در تخمین ضرایب مدل شود. برای تشخیص هم‌خطی می‌توان از **ماتریس همبستگی** یا محاسبه **VIF** (عامل تورم واریانس) استفاده کرد.

---

### **رگرسیون ریج چگونه هم‌خطی را مدیریت می‌کند؟**

**پاسخ**:
رگرسیون ریج با افزودن جریمه $L_2$ به تابع هزینه، تأثیر ویژگی‌های هم‌خطی را کاهش می‌دهد و از رگرسیون پایدارتر و با تعمیم‌پذیری بهتر برخوردار می‌شود.

---

### **روش‌های انتخاب ویژگی چیست و چرا مهم هستند؟**

**پاسخ**:
روش‌های انتخاب ویژگی شامل **فیلترها**، **بسته‌بندی (wrapper)**، و **تعبیه‌شده (embedded)** هستند. این روش‌ها به انتخاب ویژگی‌های مرتبط برای مدل کمک می‌کنند و در نتیجه باعث کاهش پیچیدگی و بهبود تعمیم‌پذیری می‌شوند.

---
سوالات کلی

---


### **پارامتر λ در Regularization چه نقشی دارد؟**

**پاسخ**:
پارامتر λ میزان تاثیر ترم **Regularization** را کنترل می‌کند:

* اگر λ خیلی بزرگ باشد → مدل بیش‌ازحد ساده می‌شود (**Underfitting**)
* اگر λ خیلی کوچک باشد → مدل آزادانه وزن‌ها را انتخاب می‌کند (**Overfitting**)

انتخاب مناسب λ باعث رسیدن به مدلی متعادل بین دقت و سادگی می‌شود.

---


### **رگولاریزاسیون چه نقشی در بهبود تعمیم‌پذیری مدل‌ها دارد و چرا در بسیاری از مدل‌ها از آن استفاده می‌شود؟**

**پاسخ**:
رگولاریزاسیون یک تکنیک است که برای کاهش بیش‌براش و جلوگیری از پیچیدگی‌های زیاد در مدل‌ها به کار می‌رود. با افزودن یک جریمه به تابع هزینه، رگولاریزیشن می‌تواند مدل را به سمت استفاده از وزن‌های کوچکتر سوق دهد که این امر موجب کاهش پیچیدگی مدل و بهبود توانایی آن در تعمیم به داده‌های جدید می‌شود.
در رگرسیون **L2 (Ridge)** و **L1 (Lasso)**، این تکنیک می‌تواند باعث کاهش حساسیت مدل به نویز داده‌ها و جلوگیری از بیش‌براش شود.

---
