عالیه! 🧠 فایل دوم که فرستادی مربوط به جلسه‌ی سوم درس «یادگیری ماشین» با موضوع **طبقه‌بندی خطی (Linear Classification)** هست و توسط دکتر علی شریفی زارعی تدریس شده.

حالا برات یک **جزوه‌ی کامل، آموزش‌محور و مفهومی** آماده کردم، دقیقاً مثل جزوه جلسه‌ی قبل، اما این بار بر اساس مباحث:

> 📚 Introduction to Classification
> 🧮 Discriminant Functions
> ➗ Linear Classifiers
> 🔁 Perceptron & Cost Functions
> ✅ Cross-validation
> 🔢 Multi-class Classification

---

## 📘 جزوه‌ی جلسه سوم: طبقه‌بندی خطی (Linear Classification)

---

### 1. مقدمه‌ای بر طبقه‌بندی (Classification)

**توضیح تکمیلی:**
طبقه‌بندی (Classification) یکی از بنیادی‌ترین مسائل یادگیری ماشین است که هدف آن **تعیین برچسب کلاس برای نمونه‌های ورودی** است. برخلاف رگرسیون که مقدار پیوسته پیش‌بینی می‌کند، خروجی در طبقه‌بندی **دسته‌بندی‌شده و گسسته** است.

**مثال‌های واقعی:**

* تشخیص اسپم بودن ایمیل
* پیش‌بینی بیماری (مثبت/منفی)
* تشخیص ترک شغل (Churn)

---

### 2. تفاوت Classification و Regression

| ویژگی     | رگرسیون (Regression)         | طبقه‌بندی (Classification)       |
| --------- | ---------------------------- | -------------------------------- |
| نوع خروجی | عدد پیوسته (مثلاً قیمت خانه) | برچسب گسسته (مثلاً اسپم / نرمال) |
| هدف       | پیش‌بینی مقدار دقیق          | تعیین کلاس صحیح                  |
| کاربرد    | پیش‌بینی روند قیمت، آب‌وهوا  | تشخیص بیماری، فیلتر اسپم         |

---

### 3. تابع تشخیص (Discriminant Function)

**توضیح تکمیلی:**
تابع تشخیص (Discriminant Function) تابعی است که به یک ورودی x، **یک نمره اختصاص می‌دهد** که بیانگر میزان شباهت آن به هر کلاس است.

**نحوه عملکرد:**

* در حالت دودسته‌ای:

  $$
  \hat{y} = 
  \begin{cases}
  C_1 & \text{اگر } g_1(x) > g_2(x) \\
  C_2 & \text{در غیر این صورت}
  \end{cases}
  $$

* در حالت چند‌کلاسه:

  $$
  \hat{y} = \arg\max_i g_i(x)
  $$

---

### 4. طبقه‌بند خطی (Linear Classifier)

**توضیح تکمیلی:**
در مدل‌های خطی، تصمیم‌گیری از طریق یک **مرز تصمیم خطی** انجام می‌شود.
فرم تابع تشخیص:

$$
g(x) = w^T x + w_0
$$

که w بردار وزن و $w_0$ بایاس است.

**قانون تصمیم‌گیری:**

$$
\hat{y} = 
\begin{cases}
C_1 & \text{اگر } g(x) \geq 0 \\
C_2 & \text{در غیر این صورت}
\end{cases}
$$

---

### 5. مفهوم Decision Boundary

**مرز تصمیم (Decision Boundary)** صفحه‌ای در فضای ویژگی‌هاست که داده‌های متعلق به کلاس‌های مختلف را از هم جدا می‌کند.

در بعد ۲، این یک خط است.
در بعد ۳، یک صفحه.
در بعد d، یک هایپرپلین (Hyperplane) از مرتبه d−1.

---

### 6. تبدیل ویژگی‌ها (Feature Transformation)

**مسئله:** داده‌ها گاهی **خطی قابل تفکیک نیستند.**

**راه‌حل:** نگاشت داده‌ها به فضای با بعد بالاتر با استفاده از تابع φ(x)، به‌طوری‌که در فضای جدید، داده‌ها قابل جداسازی باشند. این کار در مدل‌هایی مثل SVM و MLP مرسوم است.

---

### 7. پرسپترون (Perceptron)

**توضیح تکمیلی:**
مدل پرسپترون یک **نورون مصنوعی ساده** است. یک تابع خطی از ورودی‌ها می‌گیرد و روی آن یک تابع آستانه (مثل sign یا step) اعمال می‌کند.

$$
y = f(w^T x + w_0)
$$

**محدودیت:** تنها قادر به حل مسائل **خطی قابل تفکیک** است، مثل AND و OR، اما نه XOR.

---

### 8. الگوریتم پرسپترون

**نسخه Batch:**

* همزمان از همه نقاط اشتباه استفاده می‌شود.
* به کمک گرادیان کاهشی به‌روزرسانی می‌شود:

  $$
  w \leftarrow w - \eta \sum_{i \in M} y^{(i)} x^{(i)}
  $$

**نسخه Single-sample (SGD):**

* بعد از هر نمونه اشتباه، وزن آپدیت می‌شود:

  $$
  w \leftarrow w + \eta y^{(i)} x^{(i)}
  $$

---

### 9. الگوریتم Pocket

**توضیح تکمیلی:**
اگر داده‌ها **قابل جداسازی خطی نباشند** (مثلاً به دلیل نویز)، الگوریتم پرسپترون هرگز همگرا نمی‌شود.

**راه‌حل:**
الگوریتم Pocket بهترین وزن دیده‌شده را تا آن لحظه نگه می‌دارد و همیشه خروجی نهایی را بر اساس بهترین وزن ارائه می‌دهد.

---

### 10. توابع هزینه در طبقه‌بندی

**1. Sum of Squared Error (SSE):**

$$
J(w) = \sum_{i=1}^n (y^{(i)} - \hat{y}^{(i)})^2
$$

در طبقه‌بندی زیاد مناسب نیست چون به مقادیر نزدیک ولی اشتباه هم امتیاز مثبت می‌دهد.

**2. تعداد نمونه‌های اشتباه:**

$$
J(w) = \sum_{i=1}^n \left(\frac{y^{(i)} - \text{sign}(w^T x^{(i)})}{2}\right)^2
$$

غیر مشتق‌پذیر است → نمی‌توان از گرادیان دیسنت استفاده کرد.

---

### 11. Cross-Validation و ارزیابی مدل

**توضیح تکمیلی:**
برای اینکه بفهمیم مدل روی داده جدید چقدر خوب عمل می‌کند، از Cross-validation استفاده می‌کنیم.

* **K-Fold CV:** داده به K قسمت تقسیم شده، به‌تناوب آموزش و ارزیابی می‌شود.
* **LOOCV (Leave-One-Out):** هر بار فقط یک داده به عنوان تست در نظر گرفته می‌شود (دقت بالا ولی زمان‌بر).

---

### 12. طبقه‌بندی چندکلاسه (Multi-Class Classification)

**روش‌های متداول:**

* One-vs-Rest (OvR): برای هر کلاس یک مدل دوکلاسه ساخته می‌شود.
* One-vs-One (OvO): برای هر زوج کلاس، یک طبقه‌بند ساخته می‌شود.
* Linear Machines: تعریف یک تابع g برای هر کلاس و انتخاب بیشینه:

  $$
  \hat{y} = \arg\max_i g_i(x)
  $$

---

### 13. پرسپترون چندکلاسه

مدلی مشابه پرسپترون ساده، اما با وزن‌های متفاوت برای هر کلاس:

$$
W \in \mathbb{R}^{m \times K}
$$

اگر x به اشتباه به کلاس $\hat{y}$ تخصیص داده شود:

* وزن کلاس درست زیاد می‌شود
* وزن کلاس اشتباه کم می‌شود

---
خیلی عالی، الان برات یک **جزوه‌ی کامل آموزش‌محور** می‌نویسم که هر بخشش شامل حدود **۱۰ خط توضیح مفهومی، آموزشی و یادگیری‌محور** هست. این ساختار طوری طراحی شده که انگار دارم بهت تدریس می‌کنم.

---

## 📘 جزوه‌ی آموزشی جلسه ۳

### موضوع: طبقه‌بندی خطی (Linear Classification)

بر اساس اسلایدهای دکتر علی شریفی زارعی – دانشگاه صنعتی شریف

---

### 1. مقدمه‌ای بر طبقه‌بندی

در یادگیری ماشین، طبقه‌بندی (Classification) یکی از مهم‌ترین مسائل است که در آن هدف ما پیش‌بینی **دسته یا کلاس یک نمونه** است. برای مثال، ما می‌خواهیم یک ایمیل را به عنوان "اسپم" یا "نرمال" تشخیص دهیم. برخلاف رگرسیون که مقدار پیوسته مانند دما یا قیمت پیش‌بینی می‌کند، در طبقه‌بندی ما خروجی گسسته داریم مثل ۰/۱ یا کلاس A/B/C. معمولاً داده‌های ورودی ما x هستند (ویژگی‌ها) و خروجی y یک برچسب از بین چند دسته ممکن است. این مسئله در کاربردهایی مانند پزشکی، فیلتر ایمیل، و تحلیل بازار کاربرد دارد.

---

### 2. تابع تشخیص (Discriminant Function)

برای اینکه بتوانیم بین کلاس‌ها تصمیم‌گیری کنیم، باید به هر نمونه‌ی ورودی یک «نمره‌ی تعلق» بدهیم. این کار با تابع تشخیص انجام می‌شود. در ساده‌ترین حالت، برای دو کلاس، دو تابع g₁(x) و g₂(x) تعریف می‌کنیم و کلاس با نمره بالاتر را انتخاب می‌کنیم. در حالت چندکلاسه، از تابع $\hat{y} = \arg\max_i g_i(x)$ استفاده می‌کنیم. این توابع معمولاً به صورت خطی یا غیرخطی از x هستند. هدف اصلی این است که این توابع بتوانند نمونه‌های هر کلاس را به خوبی جدا کنند.

---

### 3. مرز تصمیم (Decision Boundary)

مرز تصمیم همان سطحی است که فضای ورودی را به ناحیه‌هایی تقسیم می‌کند که هر کدام مربوط به یک کلاس هستند. اگر از تابع g(x) = wᵗx + w₀ استفاده کنیم، مرز تصمیم زمانی است که g(x) = 0 می‌شود. در فضای دو‌بعدی، این مرز یک خط است. در فضای سه‌بعدی یک صفحه و در فضای d-بعدی یک هایپرپلین (صفحه با d−1 بعد). نمونه‌های دو طرف این مرز به دو کلاس متفاوت تعلق دارند. در مدل‌های خطی، مرز تصمیم نیز خطی است.

---

### 4. طبقه‌بند خطی (Linear Classifier)

در طبقه‌بند خطی، تصمیم‌گیری از طریق یک تابع خطی مانند $g(x) = w^T x + w_0$ انجام می‌شود. بردار w وزن‌هایی است که یاد گرفته می‌شود، و w₀ بایاس است که مکان مرز تصمیم را تعیین می‌کند. این مدل ساده، سریع و بسیار پرکاربرد است، به‌خصوص در مسائل با داده‌های زیاد یا ویژگی‌های زیاد. داده‌ها وقتی "linearly separable" باشند، یعنی بشود با یک خط آن‌ها را جدا کرد، این مدل به خوبی کار می‌کند.

---

### 5. تبدیل ویژگی‌ها برای تصمیم‌های غیرخطی

گاهی اوقات داده‌ها به صورت خطی قابل جدا شدن نیستند. در چنین شرایطی ما از **تبدیل ویژگی‌ها** استفاده می‌کنیم: ویژگی‌ها را به فضای جدیدی نگاشت می‌کنیم که در آن، داده‌ها قابل جدا شدن باشند. برای مثال، اگر در فضای اولیه داده‌ها شبیه دایره باشند، با افزودن ویژگی‌هایی مثل $x_1^2 + x_2^2$، آن‌ها را در یک فضای جدید به‌صورت خطی جدا می‌کنیم. این ایده، اساس کار مدل‌هایی مثل Kernel SVM و شبکه‌های عصبی است.

---

### 6. پرسپترون (Perceptron)

پرسپترون یکی از قدیمی‌ترین و ساده‌ترین مدل‌های طبقه‌بندی دودویی است. ایده‌ی اصلی آن ساده است: اگر $w^T x + w_0$ بزرگ‌تر از صفر باشد، خروجی را ۱ می‌گیرد وگرنه ۰. این مدل می‌تواند فقط مسائل خطی قابل تفکیک را حل کند، مثلاً AND یا OR، ولی نمی‌تواند مسئله XOR را حل کند. به همین دلیل بعدها پرسپترون چند‌لایه (MLP) معرفی شد که بتواند تصمیم‌های غیرخطی بگیرد.

---

### 7. الگوریتم پرسپترون

در پرسپترون، وزن‌ها به‌صورت تکراری به‌روزرسانی می‌شوند تا نمونه‌های اشتباه به درستی طبقه‌بندی شوند. اگر نمونه x به اشتباه طبقه‌بندی شده باشد، وزن‌ها به این صورت به‌روزرسانی می‌شوند:

$$
w \leftarrow w + \eta y x
$$

این الگوریتم تا زمانی که داده‌ها قابل تفکیک خطی باشند، همگرا می‌شود. دو نسخه رایج دارد: **Batch** که وزن‌ها را بر اساس تمام نمونه‌های اشتباه آپدیت می‌کند، و **Single Sample (SGD)** که فقط از یک نمونه در هر مرحله استفاده می‌کند.

---

### 8. الگوریتم Pocket

وقتی داده‌ها قابل جداسازی نیستند (مثلاً نویز دارند)، الگوریتم پرسپترون همگرا نمی‌شود. در این حالت از الگوریتم Pocket استفاده می‌کنیم. ایده‌اش این است که همیشه بهترین wای که تاکنون دیده شده را نگه می‌دارد. اگر در طی به‌روزرسانی‌ها، یک وزن جدید عملکرد بهتری روی داده آموزش داشته باشد، جایگزین w قبلی می‌شود. این الگوریتم به ویژه در داده‌های واقعی کاربردی است.

---

### 9. تابع هزینه در طبقه‌بندی

در یادگیری، هدف ما کمینه کردن یک تابع هزینه است. در طبقه‌بندی ساده‌ترین تابع هزینه، شمارش نمونه‌های اشتباه است، ولی این تابع **غیرمشتق‌پذیر** است. بنابراین گاهی از **Sum of Squared Errors (SSE)** استفاده می‌شود، ولی SSE برای طبقه‌بندی ایده‌آل نیست چون خطاها را به‌صورت مربعی می‌سنجد. در پرسپترون از تابع هزینه‌ای استفاده می‌شود که فقط روی نمونه‌های اشتباه تأکید دارد: $J(w) = - \sum y^{(i)} w^T x^{(i)}$

---

### 10. Cross-validation و انتخاب مدل

در یادگیری ماشین، ارزیابی درست مدل بسیار مهم است. Cross-validation روشی است که به ما کمک می‌کند مدل را روی داده‌ای که قبلاً ندیده، آزمایش کنیم. رایج‌ترین روش، K-fold است که داده‌ها را به K بخش تقسیم می‌کند. در هر دور یکی از بخش‌ها تست می‌شود و بقیه آموزش. سپس میانگین عملکرد گزارش می‌شود. این کار باعث می‌شود احتمال overfitting کم‌تر و ارزیابی مدل واقع‌بینانه‌تر باشد.

---

### 11. طبقه‌بندی چندکلاسه (Multi-Class)

بسیاری از مسائل دنیای واقعی بیشتر از دو کلاس دارند. برای حل این نوع مسائل از دو روش استفاده می‌شود:

* One-vs-Rest (OvR): یک مدل برای هر کلاس ساخته می‌شود که آن کلاس را از بقیه جدا کند.
* One-vs-One (OvO): برای هر جفت کلاس، یک طبقه‌بند ساخته می‌شود.

راه دیگر، ساختن تابع تشخیص برای هر کلاس است و انتخاب کلاس با بیشترین خروجی:

$$
\hat{y} = \arg\max_i g_i(x)
$$

---

### 12. پرسپترون چندکلاسه

برای طبقه‌بندی چندکلاسه، از بردار وزن جداگانه برای هر کلاس استفاده می‌کنیم. ماتریس وزن‌ها $W \in \mathbb{R}^{m \times K}$ تعریف می‌شود. برای هر نمونه، خروجی کلاس با بزرگ‌ترین $w_k^T x$ به عنوان پیش‌بینی انتخاب می‌شود. در صورت اشتباه، وزن کلاس اشتباه کاهش و وزن کلاس درست افزایش می‌یابد. این الگوریتم سریع و ساده است و در عمل کارایی خوبی دارد، به‌خصوص وقتی داده‌ها بزرگ اما نسبتاً ساده باشند.

---

اگر تأیید کنی، می‌تونم همین متن رو به صورت یک فایل PDF ساخت‌یافته هم برات بسازم. همچنین می‌تونم سوالات مصاحبه‌ای مختص همین جلسه رو هم بهش اضافه کنم.

✅ آیا ادامه بدم با PDF و سوالات مصاحبه‌ای؟

