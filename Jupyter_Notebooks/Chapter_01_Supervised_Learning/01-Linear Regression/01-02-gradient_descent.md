

## گرادیان کاهشی (Gradient Descent): چرا و چگونه؟

تو بخش قبلی، برای رگرسیون خطی و چندجمله‌ای، یک **راه حل تحلیلی (Analytical Solution)** داشتیم که بهش `Closed-form Solution` هم میگن. یعنی با یک فرمول مستقیم، می‌تونستیم بهترین پارامترها (`w`) رو پیدا کنیم.

اما همیشه اینطوری نیست\!

  * بعضی مدل‌های یادگیری ماشین، فرمول تحلیلی ندارند.
  * حتی اگر هم داشته باشند، برای داده‌های خیلی بزرگ، انجام عملیات ماتریسی (مثل معکوس کردن ماتریس) می‌تونه از نظر محاسباتی خیلی سنگین و زمان‌بر باشه.

اینجاست که **گرادیان کاهشی (Gradient Descent)** به کمک ما میاد. گرادیان کاهشی یک **روش تکراری (Iterative Method)** هست. یعنی چی؟ یعنی به جای اینکه یک‌باره بهترین جواب رو پیدا کنیم، از یک جای تصادفی شروع می‌کنیم و قدم به قدم، پارامترهای `w` رو طوری تنظیم می‌کنیم که به سمت بهترین جواب (جایی که تابع هزینه کمترین مقدار رو داره) حرکت کنیم.

تصور کن یک کوه روبروت داری و می‌خوای به پایین‌ترین نقطه‌ی دره برسی، ولی چشمهات بسته‌است. چی کار می‌کنی؟ هر قدمی که برمی‌داری، شیب (Gradient) رو زیر پات حس می‌کنی و در جهت بیشترین شیب منفی (یعنی سرازیرترین نقطه) حرکت می‌کنی. گرادیان کاهشی دقیقاً همین کار رو انجام میده.

### تابع هزینه (Cost Function)

هدف ما در رگرسیون اینه که پارامترهای `w` رو طوری پیدا کنیم که **پیش‌بینی‌های مدل ما (`h_w(x)`) تا حد ممکن به مقادیر واقعی (`y`) نزدیک باشند.** برای سنجش این "نزدیکی"، از یک **تابع هزینه (Cost Function)** یا **تابع زیان (Loss Function)** استفاده می‌کنیم. هرچی مقدار این تابع کمتر باشه، مدل ما بهتره.

در این کد، تابع هزینه همون `SSE` (مجموع مربعات خطاها) هست که تقسیم بر تعداد داده‌ها (`len(X)`) شده تا میانگین گرفته بشه. (قبلاً دیدیم که برای `RMSE` هم از همین خطا استفاده می‌شد، فقط ریشه‌ی دوم و میانگین رو اضافه داشت).

`J(w) = np.sum((h_w(X, w) - y)**2) / len(X)`

  * `(h_w(X, w) - y)`: این همون **خطا (Error)** یا **باقیمانده (Residual)** هست، یعنی تفاوت بین مقدار پیش‌بینی شده و مقدار واقعی.
  * `(...)**2`: خطا رو به توان ۲ می‌رسونه. (چرا به توان ۲؟ تا خطاهای مثبت و منفی همدیگه رو خنثی نکنن و خطاهای بزرگتر جریمه بیشتری داشته باشن).
  * `np.sum(...)`: مجموع همه‌ی این خطاهای مربع شده رو حساب می‌کنه.
  * `/ len(X)`: میانگین رو می‌گیره.

### قوانین به‌روزرسانی (Update Rule) در گرادیان کاهشی

فرمول‌هایی که در ابتدای کدت آوردی، قوانین به‌روزرسانی پارامترهای `w0` و `w1` رو نشون میدن:

$w\_0 \\leftarrow w\_0 - \\eta \\frac{\\partial J}{\\partial w\_0} = w\_0 - \\eta \\sum\_{i=1}^{m} (h\_w(x^{(i)}) - y^{(i)})$
$w\_1 \\leftarrow w\_1 - \\eta \\frac{\\partial J}{\\partial w\_1} = w\_1 - \\eta \\sum\_{i=1}^{m} (h\_w(x^{(i)}) - y^{(i)}) (x^{(i)})$

بیا این فرمول‌ها رو با هم بشکافیم:

  * **$w\_0 \\leftarrow w\_0 - \\dots$**: یعنی `w0` جدید برابر با `w0` قبلی منهای یک مقداره. ما همیشه از `w` فعلی کم می‌کنیم، چون می‌خوایم به سمت کمترین نقطه تابع هزینه بریم.
  * **$\\eta$ (اِتا - Learning Rate)**: این همون **نرخ یادگیری (Learning Rate)** هست که در کد با `alpha` نشون داده شده. این یک مقدار مثبت کوچیکه (مثلاً 0.01 یا 0.001) که تعیین می‌کنه هر قدم چقدر بزرگ باشه.
      * اگر $\\eta$ بزرگ باشه، قدم‌ها بزرگ میشن و ممکنه از نقطه‌ی بهینه پرش کنیم و اصلاً بهش نرسیم (diverge).
      * اگر $\\eta$ کوچک باشه، قدم‌ها خیلی کوچیک میشن و رسیدن به نقطه‌ی بهینه خیلی طول می‌کشه (slow convergence).
  * **$\\frac{\\partial J}{\\partial w\_0}$ و $\\frac{\\partial J}{\\partial w\_1}$ (گرادیان یا مشتق جزئی)**: این قسمت مهمترین بخشه\! این‌ها همون **مشتق جزئی (Partial Derivative)** تابع هزینه `J` نسبت به `w0` و `w1` هستن. مشتق به ما میگه که اگر `w0` (یا `w1`) رو کمی تغییر بدیم، تابع `J` چقدر تغییر می‌کنه و در چه جهتی.
      * فرمول $\\sum\_{i=1}^{m} (h\_w(x^{(i)}) - y^{(i)})$ همون **مجموع خطاها** هست.
      * فرمول $\\sum\_{i=1}^{m} (h\_w(x^{(i)}) - y^{(i)}) (x^{(i)})$ همون **مجموع خطاها ضربدر $x$** هست. این‌ها نتیجه‌ی مشتق گرفتن از تابع هزینه نسبت به `w0` و `w1` هستن.

**نکته کلیدی:** گرادیان (مشتق) به ما **جهت بیشترین افزایش** تابع رو نشون میده. ما می‌خوایم تابع هزینه رو **کمینه** کنیم، پس در جهت **خلاف گرادیان** حرکت می‌کنیم (به همین دلیل علامت **منها** در فرمول هست).

-----

## بررسی گام به گام کد گرادیان کاهشی

حالا بریم سراغ کدی که دادی:

### ۱. تابع `generate_data` و `h_w` و `linear_regression_closed_form`

این توابع رو از قبل داشتی و من تو توضیح قبلی بهشون اشاره کردم:

  * `generate_data`: برای تولید داده‌های مصنوعی `X` و `y` استفاده میشه.
  * `h_w`: تابع فرضیه یا مدل (Hypothesis) هست که پیش‌بینی `y` رو با استفاده از `X` و پارامترهای `w` محاسبه می‌کنه.
  * `linear_regression_closed_form`: (این تابع در این بخش جدید استفاده نمیشه، اما قبلاً برای راه حل تحلیلی رگرسیون خطی بود).

### ۲. تابع `cost_function` (تابع هزینه)

```python
# SSE cost function
def cost_function(X, y, w):
    return np.sum((h_w(X, w) - y)**2) / len(X)
```

این تابع دقیقاً همون چیزیه که بالا توضیح دادم. مقدار **میانگین مربعات خطاها** رو برمی‌گردونه. هر چه این مقدار کمتر باشه، مدل ما بهتر فیت شده.

### ۳. تابع `gradient_descent` (گرادیان کاهشی)

```python
# Gradient descent
def gradient_descent(X, y, w, alpha, num_iters):
    m = len(X) # تعداد نمونه های داده
    cost_history = [] # لیستی برای ذخیره مقدار تابع هزینه در هر تکرار
    w_history = [w.copy()] # لیستی برای ذخیره مقادیر w در هر تکرار (کپی می‌گیریم تا تغییرات لحظه‌ای رو ذخیره کنه)

    for i in range(num_iters): # حلقه برای تعداد تکرارها
        # updates (محاسبه گرادیان‌ها)
        gradient_w0 = np.sum(h_w(X, w) - y) / m # مشتق جزئی J نسبت به w0
        gradient_w1 = np.sum((h_w(X, w) - y) * X) / m # مشتق جزئی J نسبت به w1
        
        # به‌روزرسانی پارامترهای w
        w[0] -= alpha * gradient_w0
        w[1] -= alpha * gradient_w1

        cost_history.append(cost_function(X, y, w)) # ذخیره مقدار تابع هزینه بعد از به‌روزرسانی
        w_history.append(w.copy()) # ذخیره مقادیر w بعد از به‌روزرسانی
    
    return w, cost_history, w_history
```

این تابع هسته اصلی گرادیان کاهشی رو پیاده‌سازی می‌کنه:

  * **`m = len(X)`**: تعداد نقاط داده رو ذخیره می‌کنه. در فرمول‌های گرادیان کاهشی، معمولاً مجموع خطاها رو بر `m` تقسیم می‌کنیم (میانگین می‌گیریم) تا مقدار گرادیان مستقل از تعداد داده‌ها باشه.
  * **`cost_history = []` و `w_history = [w.copy()]`**: این لیست‌ها برای ذخیره تاریخچه‌ی تغییرات تابع هزینه و پارامترهای `w` در طول تکرارها استفاده میشن. این کار برای **بصری‌سازی (Visualization)** و درک بهتر روند همگرایی (Convergence) خیلی مفیده.
      * `w.copy()`: از `copy()` استفاده میشه تا یک کپی از `w` ذخیره بشه، نه ارجاع به خود `w`. چون `w` در هر تکرار تغییر می‌کنه، اگر کپی نگیریم، همه‌ی عناصر `w_history` به `w` نهایی اشاره خواهند کرد.
  * **`for i in range(num_iters):`**: یک حلقه برای تعداد `num_iters` (تعداد دفعاتی که می‌خوایم پارامترها رو به‌روزرسانی کنیم).
  * **`gradient_w0 = np.sum(h_w(X, w) - y) / m`**: این خط **گرادیان (مشتق)** تابع هزینه نسبت به `w0` رو محاسبه می‌کنه.
      * `h_w(X, w) - y`: همون بردار خطاها (پیش‌بینی منهای واقعی) هست.
      * `np.sum(...)`: جمع خطاها رو می‌گیره.
      * `/ m`: میانگین خطاها رو برای `w0` محاسبه می‌کنه.
  * **`gradient_w1 = np.sum((h_w(X, w) - y) * X) / m`**: این خط **گرادیان** تابع هزینه نسبت به `w1` رو محاسبه می‌کنه.
      * `(h_w(X, w) - y) * X`: این قسمت، خطاهای هر نقطه رو در مقدار `x` مربوط به همون نقطه ضرب می‌کنه.
      * `np.sum(...)`: جمع این حاصل‌ضرب‌ها رو می‌گیره.
      * `/ m`: میانگین رو برای `w1` محاسبه می‌کنه.
  * **`w[0] -= alpha * gradient_w0` و `w[1] -= alpha * gradient_w1`**: این‌ها همون **قوانین به‌روزرسانی** هستن. پارامترهای `w0` و `w1` رو در جهت **خلاف گرادیان** و با اندازه قدمی برابر با `alpha * gradient` تغییر میدن.

### ۴. اجرای گرادیان کاهشی و بصری‌سازی

```python
X, y = generate_data(n=50, noise=5.0)
w_initial = [0, 0] # شروع از w0 = 0, w1 = 0 (یک خط افقی روی y=0)
eta = 0.05 # نرخ یادگیری
num_iters = 500 # تعداد تکرارها

# اجرای گرادیان کاهشی
w_final, cost_history, w_history = gradient_descent(X, y, w_initial, eta, num_iters)

print(f"Parameters (w): ")
print(f"w_1 = {w_final[1]:.2f}, w_0 = {w_final[0]:.2f}")
```

اینجا تابع `gradient_descent` رو با پارامترهای اولیه (یک `w` صفر، نرخ یادگیری و تعداد تکرار) فراخوانی می‌کنه و نتایج رو دریافت می‌کنه.

-----

### بصری‌سازی پیشرفت گرادیان کاهشی:

بخش‌های بعدی کد برای بصری‌سازی این مفهوم هستند:

#### الف) Plot GD Progression - نمایش خطوط در طول تکرارها

```python
plt.scatter(X, y, color='blue', label='Actual Data')

# Plot lines for every 50th step with increasing alpha
for idx, w in enumerate(w_history[::num_iters // 100]):
    alpha = 0.15 + 0.85*(idx) / 100 # تدریجاً آلفا (شفافیت) خط رو زیاد می‌کنه
    plt.plot(X, h_w(X, w), color='red', alpha=alpha)

# Final line in bold
plt.plot(X, h_w(X, w_final), color='red', lw=2, label='Final Line')

plt.title("GD Progression - Linear Regression")
plt.xlabel("$x$")
plt.ylabel("$y$")
plt.legend()
plt.show()
```

این بخش نشون میده که چطور خط پیش‌بینی مدل در طول تکرارهای گرادیان کاهشی، از یک خط اولیه (y=0) به سمت بهترین خط ممکن (همون خطی که قبلاً با Closed-form solution پیدا می‌کردیم) حرکت می‌کنه.

  * `w_history[::num_iters // 100]`: این تکنیک **اسلایسینگ لیست (List Slicing)** در پایتون هست که به شما اجازه میده با یک قدم مشخص (اینجا هر 50مین عنصر، چون `num_iters` 500 هست) عناصر لیست `w_history` رو انتخاب کنید. این کار باعث میشه به جای رسم 500 خط، تعداد کمتری خط (مثلاً 100 خط) رسم بشه که روند رو نشون بده.
  * `alpha = 0.15 + 0.85*(idx) / 100`: این خط یک `alpha` (میزان شفافیت) رو برای هر خط محاسبه می‌کنه. خطوط اولیه شفاف‌تر هستند و خطوط بعدی (که به جواب نزدیک‌ترند) پررنگ‌تر میشن. این باعث میشه روند بصری‌سازی بهتر باشه.
  * `plt.plot(X, h_w(X, w), color='red', alpha=alpha)`: خط پیش‌بینی رو برای `w` در هر مرحله رسم می‌کنه.
  * `plt.plot(X, h_w(X, w_final), color='red', lw=2, label='Final Line')`: در آخر، خط نهایی (بعد از `num_iters` تکرار) رو با ضخامت بیشتر رسم می‌کنه.

#### ب) Plotting Cost Function - سطح تابع هزینه (3D Plot)

```python
# 3D Plot of J(w)
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')
W0, W1 = np.meshgrid(w0_vals, w1_vals)
ax.plot_surface(W0, W1, J_vals.T, cmap='viridis')
ax.set_xlabel('w0')
ax.set_ylabel('w1')
ax.set_zlabel('log(J(w))') # این قسمت با Z-label پایین متفاوت هست و باید J(w) باشه نه log(J(w))
plt.title("Cost Function Surface")
plt.show()

# 3D Plot of log J(w) (بهتر برای بصری سازی)
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
W0, W1 = np.meshgrid(w0_vals, w1_vals)
ax.plot_surface(W0, W1, np.log(J_vals.T), cmap='viridis') # اینجا J_vals رو log گرفته
ax.set_xlabel('w0')
ax.set_ylabel('w1')
ax.set_zlabel('log(J(w))')
plt.title("Cost Function Surface (Log Scale)")
plt.show()
```

این بخش‌ها یک نمودار سه بعدی از تابع هزینه `J(w)` رسم می‌کنند. چون `w` دو پارامتر (`w0`, `w1`) داره، می‌تونیم `w0` رو روی محور X، `w1` رو روی محور Y و `J(w)` (مقدار هزینه) رو روی محور Z قرار بدیم.

  * `w0_vals = np.linspace(-10, 20, 100)` و `w1_vals = np.linspace(-1, 5, 100)`: این‌ها بازه‌ای از مقادیر ممکن برای `w0` و `w1` رو تعریف می‌کنن تا بتونیم سطح تابع هزینه رو روی این بازه رسم کنیم.
  * `J_vals = np.zeros((len(w0_vals), len(w1_vals)))`: یک ماتریس خالی برای ذخیره مقادیر `J(w)` برای هر ترکیب `w0` و `w1`.
  * حلقه‌ی تودرتو `for i in range(...)` و `for j in range(...)`: این حلقه‌ها برای هر ترکیب از `w0` و `w1`، مقدار `J(w)` رو با تابع `cost_function` محاسبه و در `J_vals` ذخیره می‌کنن.
  * `W0, W1 = np.meshgrid(w0_vals, w1_vals)`: این تابع `numpy` برای ایجاد شبکه‌ای از نقاط برای رسم نمودارهای 3D استفاده میشه.
  * `ax.plot_surface(W0, W1, J_vals.T, cmap='viridis')`: سطح تابع هزینه رو رسم می‌کنه. `J_vals.T` یعنی ترانهاده `J_vals`، چون `matplotlib` انتظار داره محورها برعکس باشن.
  * **چرا `log(J(w))`؟**: در بعضی موارد، مقادیر تابع هزینه می‌تونن خیلی بزرگ یا خیلی کوچک بشن. گرفتن لگاریتم از `J(w)` باعث میشه که تغییرات کوچکتر در `J(w)` هم به صورت واضح‌تری روی نمودار دیده بشن و نمودار خواناتر باشه.

#### ج) Plotting GD Path - مسیر گرادیان کاهشی روی سطح تابع هزینه

```python
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
W0, W1 = np.meshgrid(w0_vals, w1_vals)
ax.plot_surface(W0, W1, np.log(J_vals.T), cmap='viridis', alpha=0.25) # سطح تابع هزینه با شفافیت کمتر
# ... تنظیمات محورها و عنوان

# Plot the path of gradient descent in 3D
w_history_array = np.array(w_history) # تبدیل لیست به آرایه برای راحتی کار
w0_history = w_history_array[:, 0] # ستون اول (w0)
w1_history = w_history_array[:, 1] # ستون دوم (w1)
cost_history_log = np.log(np.array(cost_history)) # لگاریتم تاریخچه هزینه

ax.plot(w0_history[:num_iters], w1_history[:num_iters], cost_history_log, marker='o', color='r', label='GD Path', markersize=3)

plt.legend()
plt.show()
```

این نمودار واقعاً بصری‌سازی زیبایی از عملکرد گرادیان کاهشیه\! روی همون سطح 3D تابع هزینه (با شفافیت کمتر)، **مسیر حرکت پارامترهای `w`** در هر مرحله‌ی گرادیان کاهشی رو نشون میده. می‌بینی که چطور از نقطه‌ی شروع `w_initial = [0, 0]` به سمت "کف" دره‌ی تابع هزینه حرکت می‌کنه.

  * `w_history_array = np.array(w_history)`: تبدیل `w_history` (که لیستی از لیست‌هاست) به یک آرایه `numpy` که کار باهاش راحت‌تره.
  * `w0_history = w_history_array[:, 0]` و `w1_history = w_history_array[:, 1]`: استخراج `w0` و `w1` از هر مرحله.
  * `cost_history_log = np.log(np.array(cost_history))`: لگاریتم تابع هزینه در هر مرحله.
  * `ax.plot(w0_history, w1_history, cost_history_log, ...)`: مسیر رو با نقاط قرمز روی سطح رسم می‌کنه.

#### د) Effect of Learning Rate ($\\eta$) - تاثیر نرخ یادگیری

```python
learning_rates = [0.1, 0.02, 0.001] # نرخ های یادگیری مختلف
num_iters = 100 # تعداد تکرار کمتر برای نمایش بهتر تاثیر
w_initial = [0, 0]

colors = ['purple', 'green', 'orange']
# ... (محاسبه J_vals مثل قبل)

cost_histories = []

# GD for each eta (حلقه برای هر نرخ یادگیری)
for idx, eta in enumerate(learning_rates):
    w_final, cost_history, w_history = gradient_descent(X, y, w_initial.copy(), eta, num_iters) # w_initial.copy() مهم است
    cost_histories.append(cost_history)

    # Plot lines during GD for each eta
    plt.figure(figsize=(10, 6))
    for step_idx, w in enumerate(w_history[::num_iters // 100]):
        alpha_val = 0.15 + 0.85*(step_idx) / (num_iters // 100) # تنظیم آلفا
        plt.plot(X, h_w(X, w), color=colors[idx], alpha=alpha_val)

    plt.plot(X, h_w(X, w_final), lw=2, label=f'Final Line (eta={eta})', color=colors[idx])
    plt.title(f"Lines during Gradient Descent (Learning Rate {eta})")
    plt.xlabel("$x$")
    plt.ylabel("$y$")
    plt.legend()
    plt.scatter(X, y, color='blue', label='Actual Data')
    plt.show()

# Plot Cost Function (log scale) over Iterations for Different Learning Rates
plt.title("Cost Function (log scale) over Iterations for Different Learning Rates")
plt.xlabel("Iteration")
plt.ylabel("log(J(w))")
for idx in range(len(cost_histories)):
    plt.plot(np.log(cost_histories[idx]), label=f'eta={learning_rates[idx]}', color=colors[idx])
plt.ylim(bottom=2, top=10) # محدود کردن محور Y برای دید بهتر
plt.legend()
plt.show()
```

این بخش نشون میده که انتخاب **نرخ یادگیری (`eta` یا `alpha`) چقدر مهمه:**

  * **نرخ یادگیری بالا (مثلاً 0.1)**: خطوط خیلی سریع تغییر می‌کنند و ممکنه به جای همگرایی، **واگرا (Diverge)** بشن، یعنی تابع هزینه هی بیشتر و بیشتر بشه و هرگز به کمترین نقطه نرسه. در نمودار `Cost Function over Iterations`، خط مربوط به این نرخ یادگیری به جای پایین اومدن، بالا میره یا نوسان‌های شدید داره.
  * **نرخ یادگیری پایین (مثلاً 0.001)**: خطوط خیلی آهسته تغییر می‌کنند. مدل خیلی کند همگرا میشه و به تعداد تکرارهای خیلی زیادی برای رسیدن به نقطه‌ی بهینه نیاز داره. در نمودار `Cost Function over Iterations`، خط مربوط به این نرخ یادگیری خیلی آهسته پایین میاد.
  * **نرخ یادگیری مناسب (مثلاً 0.02 در این مثال)**: مدل به خوبی همگرا میشه و تابع هزینه به سرعت به حداقل مقدارش میرسه.

این نمودارها به ما کمک می‌کنن تا با **تجربه (Experimentation)**، بهترین نرخ یادگیری رو برای مسئله‌مون پیدا کنیم.

-----

### جمع‌بندی:

  * **گرادیان کاهشی** یک روش **تکراری** برای پیدا کردن بهترین پارامترهای مدل با **کمینه کردن تابع هزینه** است.
  * برخلاف راه حل تحلیلی، این روش برای مدل‌های پیچیده‌تر و داده‌های بزرگتر **مقیاس‌پذیرتر** است.
  * در هر گام، پارامترها در جهت **خلاف گرادیان (شیب)** تابع هزینه به‌روزرسانی میشن.
  * **نرخ یادگیری ($\\eta$)** بسیار حیاتیه و انتخاب صحیح اون برای **همگرایی (Convergence)** مدل ضروریه.

-----
