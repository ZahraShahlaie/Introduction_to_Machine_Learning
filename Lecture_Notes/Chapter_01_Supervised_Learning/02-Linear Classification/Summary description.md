---

## ۵. توابع هزینه (Cost Functions)

### درک هدف:

در پرسپترون، از $w^T x$ برای پیش‌بینی استفاده می‌شود.
هدف پیدا کردن بردار وزن بهینه $w$ است به طوری که برچسب‌های پیش‌بینی‌شده تا حد امکان با برچسب‌های واقعی مطابقت داشته باشند.

برای رسیدن به این هدف، یک **تابع هزینه** تعریف می‌شود که تفاوت بین برچسب‌های پیش‌بینی شده و واقعی را اندازه‌گیری می‌کند.

یافتن توابع تشخیص $(w^T, w_0)$ به صورت کمینه‌سازی یک تابع هزینه فرمول‌بندی می‌شود.
بر اساس مجموعه آموزشی $D$، یک تابع هزینه $J(w)$ تعریف می‌شود.
مسئله تبدیل می‌شود به یافتن:

$$
\hat{w} = \arg\min_w J(w)
$$

---

### تابع هزینه مجموع مربعات خطا (Sum of Squared Error - SSE) برای طبقه‌بندی:

فرمول:

$$
J(w) = \sum_{i=1}^n \left( y^{(i)} - \hat{y}^{(i)} \right)^2
$$

که در آن:

$$
\hat{y}^{(i)} = w^T x^{(i)} + w_0
$$

---

### محدودیت‌ها برای طبقه‌بندی:

* SSE بزرگی خطا را کمینه می‌کند، که برای رگرسیون ایده‌آل است اما برای طبقه‌بندی مناسب نیست.

* حتی اگر مدل نزدیک به کلاس واقعی پیش‌بینی کند اما دقیقاً 0 یا 1 نباشد، SSE همچنان خطای مثبت نشان می‌دهد.
  مثلاً اگر $y=1$ باشد و مدل $0.9$ پیش‌بینی کند، SSE هنوز خطا دارد، در حالی که برای طبقه‌بندی این پیش‌بینی صحیح است.

* SSE مستعد **بیش‌برازش (Overfitting)** به داده‌های نویزی است، چون تغییرات کوچک می‌توانند تغییرات قابل توجهی در هزینه ایجاد کنند.

---

### جایگزینی برای تابع هزینه SSE: تعداد اشتباه طبقه‌بندی شده

* تعریف: اندازه می‌گیرد چند نمونه توسط مدل اشتباه طبقه‌بندی شده‌اند.

* فرمول:

$$
J(w) = \sum_{i=1}^n \left( 2 y^{(i)} - \operatorname{sign}(\hat{y}^{(i)}) \right)^2
$$

که در آن $\hat{y}^{(i)} = w^T x^{(i)} + w_0$ و $y^{(i)} \in \{-1, +1\}$.
اگر $y$ و $\hat{y}$ برابر باشند، ترم صفر است و در غیر این صورت 1 می‌شود.

---

### محدودیت‌ها:

* این تابع هزینه **ناپیوسته و غیرقابل مشتق‌گیری (Non-differentiable)** است.
  بنابراین تکنیک‌های بهینه‌سازی مانند گرادیان نزولی (Gradient Descent) نمی‌توانند مستقیم روی آن اعمال شوند.

---

### توضیح مهم:

SSE برای طبقه‌بندی مناسب نیست چون به "دقت عددی" پیش‌بینی حساس است نه به "درستی کلاس".
تابع تعداد اشتباه طبقه‌بندی شده خوب است ولی چون پله‌ای و ناپیوسته است، نمی‌توان گرادیان آن را محاسبه کرد.

---

## الگوریتم پرسپترون (Perceptron Algorithm)

### هدف:

یک الگوریتم ساده برای طبقه‌بندی دودویی که دو کلاس را با یک مرز خطی از هم جدا می‌کند.

---

### معیار پرسپترون (Perceptron Criterion - Cost Function):

این تابع هزینه فقط روی نقاط **اشتباه طبقه‌بندی شده** تمرکز دارد:

$$
J_p(w) = - \sum_{i \in M} y^{(i)} w^T x^{(i)}
$$

که $M$ مجموعه نقاط اشتباه طبقه‌بندی شده است و $y^{(i)} \in \{-1, +1\}$.

---

### هدف:

کمینه کردن این خطا با طبقه‌بندی صحیح همه نقاط.
برخلاف تابع تعداد اشتباه طبقه‌بندی شده، این تابع هزینه **محدب** و قابل بهینه‌سازی است.

---

### پرسپترون دسته‌ای (Batch Perceptron):

* در هر تکرار، بردار وزن را با استفاده از **تمام نقاط اشتباه طبقه‌بندی شده** به‌روزرسانی می‌کند.

* به‌روزرسانی وزن با گرادیان نزولی:

$$
w \leftarrow w - \eta \nabla_w J_p(w)
$$

که

$$
\nabla_w J_p(w) = - \sum_{i \in M} y_i x_i
$$

---

### همگرایی:

* پرسپترون دسته‌ای برای داده‌های **جداسازی‌پذیر خطی** در تعداد مراحل متناهی همگرا می‌شود.

---

### پرسپترون تک‌نمونه (Single-sample Perceptron):

* وزن را پس از پردازش هر نمونه به‌روزرسانی می‌کند.

* قاعده به‌روزرسانی (Stochastic Gradient Descent):

$$
w \leftarrow w + \eta y_i x_i
$$

* مزایا: هزینه محاسباتی کمتر و همگرایی سریع‌تر.

* همگرایی: برای داده‌های جداسازی‌پذیر خطی تضمین شده است.

---

### همگرایی پرسپترون (Convergence of the Perceptron):

* اگر داده‌ها **خطی جداسازی‌پذیر نباشند**، پرسپترون همگرا نمی‌شود و وزن‌ها به طور مداوم در حال به‌روزرسانی هستند.

* در نتیجه، الگوریتم هرگز به یک راه‌حل پایدار نمی‌رسد.

---
