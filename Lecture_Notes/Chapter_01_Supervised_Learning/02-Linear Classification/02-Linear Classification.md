

📘 جزوه آموزشی درس یادگیری ماشین (CE 40717)
موضوع: طبقه‌بندی خطی (Linear Classification) و پرسپترون (Perceptron)
تاریخ: ۳۰ سپتامبر ۲۰۲۴
مدرس: دکتر علی شریفی‌زارچی
تهیه و تنظیم: عرفان جعفری (با ویرایش و تکمیل)

۱. مقدمه‌ای بر طبقه‌بندی (Introduction to Classification)
تعریف:
در یادگیری ماشین، هدف طبقه‌بندی این است که:

* داده‌های ورودی (Training Set): یک مجموعه داده آموزشی
  $D$
  با
  $N$
  نمونه برچسب‌دار داریم:
  $D = \{ (x^{(i)}, y^{(i)}) \}_{i=1}^N$

* برچسب‌ها (Labels): هر
  $y^{(i)}$
  به یکی از
  $K$
  کلاس ممکن تعلق دارد:
  $y^{(i)} \in \{1, ..., K\}$

* هدف نهایی: با دریافت یک ورودی جدید
  $x$
  آن را به یکی از
  $K$
  کلاس موجود تخصیص دهیم.

توضیحات اضافه:
تصور کنید یک کمد دارید و می‌خواهید لباس‌ها را بر اساس رنگ (قرمز، آبی، سبز) در قفسه‌های مختلف قرار دهید. اینجا رنگ لباس "برچسب" شماست و هر لباس جدیدی که می‌آید، باید به یکی از این قفسه‌ها (کلاس‌ها) برود. در یادگیری ماشین هم همین‌طور است؛ ما به مدل "آموزش" می‌دهیم که بر اساس ویژگی‌های ورودی، نمونه‌های جدید را به کلاس‌های صحیح اختصاص دهد.

مثال‌های واقعی:

* تشخیص اسپم ایمیل: ایمیل‌ها به دو دسته "اسپم" یا "اینباکس" طبقه‌بندی می‌شوند.
* تشخیص پزشکی: بر اساس علائم و نتایج آزمایش، بیماری‌ها (مانند دیابت) تشخیص داده می‌شوند.
* پیش‌بینی ریزش مشتری (Churn Prediction): پیش‌بینی اینکه آیا یک مشتری، خدمات شرکت را ترک خواهد کرد یا خیر.

مثال: مجموعه داده دیابت پیما ایندیانز (Pima Indians Diabetes Dataset):

* مسئله: پیش‌بینی اینکه آیا یک بیمار بر اساس داده‌های تشخیصی پزشکی (مانند تعداد دفعات بارداری، گلوکز، فشار خون، BMI، سن و...) دیابت دارد یا خیر.
* اهمیت: تشخیص زودهنگام دیابت برای درمان و مدیریت آن حیاتی است.

طبقه‌بندی در مقابل رگرسیون (Classification vs. Regression):

| جنبه          | رگرسیون خطی (Linear Regression)     | طبقه‌بندی خطی (Linear Classification)                    |
| :------------ | :---------------------------------- | :------------------------------------------------------- |
| نوع خروجی     | مقادیر پیوسته (اعداد حقیقی)         | برچسب‌های دودویی یا چندکلاسه (مثلاً -1/+1، A/B/C)        |
| موارد استفاده | پیش‌بینی قیمت خانه، روند بازار سهام | تشخیص اسپم ایمیل، امتیازدهی اعتباری، پیش‌بینی ریزش مشتری |

توضیحات اضافه:
فقط به یاد داشته باش که "رگرسیون" یعنی پیش‌بینی یک عدد (مثلاً قیمت خونه)، ولی "طبقه‌بندی" یعنی پیش‌بینی یک دسته یا گروه (مثلاً اینکه ایمیل اسپمه یا نه). مدل‌های خطی هم می‌تونن برای رگرسیون و هم برای طبقه‌بندی استفاده بشن، فقط نوع خروجی و روش یادگیری‌شون فرق می‌کنه.




البته، این هم متن شما به صورت بازنویسی‌شده و مرتب بدون تغییر معنی:

---

## ۲. توابع تشخیص (Discriminant Functions)

**تعریف:**
تابع تشخیص، تابعی است که به یک بردار ورودی $x$ یک "امتیاز" تخصیص می‌دهد تا آن را به کلاس‌های مختلف طبقه‌بندی کند. این تابع، ورودی $x$ را به یک عدد حقیقی $g(x)$ نگاشت می‌کند که نشان‌دهنده درجه اطمینان مدل در تخصیص $x$ به یک کلاس خاص است.

**توضیحات اضافه:**
تصور کن داری میوه‌ها را دسته‌بندی می‌کنی. یک تابع تشخیص می‌تواند به هر میوه بر اساس رنگ، بو و اندازه، یک "امتیاز" بدهد. مثلاً اگر امتیاز بالای ۰.۷ باشد، می‌گویی این سیب است؛ اگر کمتر بود، ممکن است پرتقال باشد.

---

### چگونگی کارکرد:

**طبقه‌بندی دودویی (Binary Classification):**
برای دو کلاس $C_1$ و $C_2$، دو تابع $g_1(x)$ و $g_2(x)$ محاسبه می‌شوند. کلاس بر اساس مقایسه این دو تابع پیش‌بینی می‌شود:

$$
\hat{y} = \begin{cases} 
C_1 & \text{if } g_1(x) > g_2(x) \\
C_2 & \text{otherwise}
\end{cases}
$$

---

**حالت کلی (K-class Problems):**
برای مسائل با $K$ کلاس، تابع $g_i(x)$ برای هر کلاس $i$ محاسبه می‌شود. نمونه $x$ به کلاسی تخصیص داده می‌شود که بالاترین امتیاز را داشته باشد:

$$
\hat{y} = \arg\max_i g_i(x)
$$

---

### سطح تصمیم (Decision Boundary):

* **تعریف:** یک هایپرپلین جداکننده است که کلاس‌های مختلف را در فضای ویژگی‌ها از هم جدا می‌کند. به آن "سطح تصمیم" (Decision Surface) نیز گفته می‌شود.

* **مثال‌های بصری:**

  * مرز تصمیم خطی دو بعدی (2D Linear Decision Boundary): یک خط صاف که دو دسته داده را از هم جدا می‌کند.
  * مرز تصمیم غیرخطی دو بعدی (2D Non-linear Decision Boundary): یک منحنی که دو دسته داده را از هم جدا می‌کند.
  * مرز تصمیم خطی سه بعدی (3D Linear Decision Boundary): یک صفحه که دو دسته داده را از هم جدا می‌کند.
  * مرز تصمیم غیرخطی سه بعدی (3D Non-linear Decision Boundary): یک سطح منحنی که دو دسته داده را از هم جدا می‌کند.

---

### توابع تشخیص برای دو-دسته (Two-Category):

برای مسائل دو دسته‌ای، می‌توان تنها یک تابع
$g: \mathbb{R}^d \to \mathbb{R}$
پیدا کرد:

$$
g_1(x) = g(x)
$$

$$
g_2(x) = -g(x)
$$

مرز تصمیم:
$g(x) = 0$. (یعنی جایی که $g_1(x)$ و $g_2(x)$ برابر می‌شوند).

---

**توضیحات اضافه:**
وقتی می‌گوییم "مرز تصمیم"، منظورمان یک خط، یک صفحه یا یک سطح پیچیده‌تر است که فضای داده‌ها را به قسمت‌هایی تقسیم می‌کند. هر قسمت مربوط به یک کلاس است. اگر این مرز یک خط راست باشد (در دو بعد) یا یک صفحه صاف (در سه بعد)، به آن "خطی" می‌گوییم. اگر منحنی یا سطح پیچیده‌ای باشد، آن را "غیرخطی" می‌نامیم.

---

اگر نیاز بود متن را در قالب دیگری یا با اضافه کردن بخش‌های دیگر هم آماده کنم، بگوید.



البته، متن شما را به شکل مرتب و با قالب‌بندی مناسب بازنویسی کردم:

---

## ۳. طبقه‌بندی‌کننده‌های خطی (Linear Classifiers)

**تعریف:**
در طبقه‌بندی‌کننده‌های خطی، مرزهای تصمیم به صورت خطی در فضای $d$-بعدی ویژگی‌ها ($x \in \mathbb{R}^d$) یا خطی در یک مجموعه مشخص از توابع $x$ (توابع ویژگی) هستند.

---

### داده‌های جداسازی‌پذیر خطی (Linearly Separable Data):

نقاط داده‌ای که می‌توانند دقیقاً توسط یک مرز تصمیم‌گیری خطی از هم جدا شوند.

**توضیحات اضافه:**
خیلی از اوقات داده‌ها را می‌شود با یک خط راست از هم جدا کرد (مانند دایره‌ها و ضربدرها در صفحه ۱۴ PDF). این داده‌ها "جداسازی‌پذیر خطی" هستند. طبقه‌بندی‌کننده‌های خطی محبوب هستند چون:

* **سادگی (Simplicity):** پیاده‌سازی و درک آسان دارند.
* **کارایی (Efficiency):** از نظر محاسباتی کم‌هزینه هستند.
* **اثربخشی (Effectiveness):** با وجود سادگی، در بسیاری از مسائل واقعی عملکرد خوبی دارند.

---

### طبقه‌بندی دو-دسته (Two Category Classification):

یک تابع تشخیص خطی به صورت زیر تعریف می‌شود:

$$
g(x) = w^T x + w_0 = w_d \cdot x_d + \ldots + w_1 \cdot x_1 + w_0
$$

که در آن:

* $x = [x_1, \ldots, x_d]$ بردار ویژگی‌ها،
* $w = [w_1, \ldots, w_d]$ بردار وزن‌ها،
* و $w_0$ بایاس (bias) است.

---

### قاعده تصمیم:

$$
\hat{y} = 
\begin{cases}
C_1 & \text{if } w^T x + w_0 \geq 0 \\
C_2 & \text{otherwise}
\end{cases}
$$

---

### سطح تصمیم:

معادله سطح تصمیم برابر است با:

$$
w^T x + w_0 = 0
$$

---

### ویژگی‌های مرز تصمیم (هایپرپلین $H$):

* مرز تصمیم یک هایپرپلین $(d-1)$-بعدی $H$ در فضای ویژگی $d$-بعدی است.
* جهت هایپرپلین توسط بردار نرمال $\left[ \frac{w_1}{\|w\|}, \ldots, \frac{w_d}{\|w\|} \right]$ تعیین می‌شود.
* موقعیت هایپرپلین توسط $w_0$ تعیین می‌شود. (این تغییر در نمودارهای صفحه ۱۵ PDF که $w_0$ از 0 به 0.5 تغییر می‌کند، واضح است.)

---

### مرز تصمیم غیرخطی (Non-linear Decision Boundary):

* **تبدیل ویژگی (Feature Transformation):**
  برای ایجاد غیرخطی بودن، ویژگی‌ها به فضای با ابعاد بالاتر تبدیل می‌شوند. این تبدیل با استفاده از یک تابع غیرخطی $\phi(x)$ انجام می‌شود.

* **خطی در فضای تبدیل‌شده:**
  مرز تصمیم در فضای جدید (تبدیل‌شده) خطی می‌شود، اما در فضای اصلی $x$ غیرخطی است.

---

### مثال: ایجاد مرز تصمیم دایره‌ای

مرز تصمیم دایره‌ای مانند $-1 + x_1^2 + x_2^2 = 0$ با مشخصات زیر:

* ویژگی‌های اصلی:

  $$
  x = [x_1, x_2]
  $$

* تبدیل ویژگی:

  $$
  \phi(x) = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]
  $$

* وزن‌ها:

  $$
  w = [w_0, w_1, \ldots, w_m] = [-1, 0, 0, 1, 1, 0]
  $$

* قاعده تصمیم:
  اگر $w^T \phi(x) \geq 0$ باشد، آنگاه $y = 1$؛ در غیر این صورت $y = -1$.

---

**توضیحات اضافه:**
این بخش بسیار مهم است. اگر داده‌ها با یک خط صاف از هم جدا نشوند (مانند مسئله XOR)، می‌توانی ویژگی‌های جدیدی از همان ویژگی‌های قدیمی بسازی. مثلاً اگر ویژگی‌ها $x_1$ و $x_2$ باشند، می‌توانی ویژگی‌های جدید $x_1^2$ و $x_2^2$ را نیز اضافه کنی. اکنون مرز تصمیم‌گیری در این فضای جدید با ابعاد بیشتر، خطی خواهد بود، اما اگر به آن از دید فضای اصلی نگاه کنی، می‌بینی که مرز تصمیم دیگر خطی نیست و ممکن است یک منحنی یا دایره باشد.

---

اگر لازم است متن را به شکل دیگری نیز تنظیم کنم یا بخش‌های بیشتری اضافه کنم، بفرمایید.






۴. پرسپترون (Perceptron)
پرسپترون چیست؟
واحد ساختاری پایه (Basic Building Block):
پرسپترون ساده‌ترین نوع نورون مصنوعی است که در یادگیری ماشین استفاده می‌شود.

طبقه‌بندی‌کننده خطی:
ورودی‌ها را به یک خروجی نگاشت می‌کند، با اعمال یک ترکیب خطی و یک آستانه (threshold).

تصمیم دودویی (Binary Decision):
اگر جمع وزن‌دار ورودی‌ها از آستانه بیشتر شود، خروجی 1 است؛ در غیر این صورت، خروجی 0.

اجزا:
ورودی‌ها، وزن‌ها، بایاس و یک تابع فعال‌سازی (اغلب تابع پله‌ای یا سیگموید).

توضیح:
پرسپترون مانند کوچک‌ترین واحد مغز ما (نورون) عمل می‌کند. ورودی‌ها را می‌گیرد، هرکدام را در وزن مربوط ضرب می‌کند، همه را جمع می‌کند، یک بایاس (مثل آستانه) به آن اضافه می‌کند، و سپس خروجی را از یک تابع فعال‌سازی عبور می‌دهد تا تصمیم نهایی (0 یا 1) را بگیرد.

الهام‌گرفته از زیست‌شناسی
پرسپترون عملکرد اصلی نورون‌های بیولوژیکی مغز را تقلید می‌کند.

نورون منفرد (Single Neuron):
فرمول خروجی یک نورون منفرد به صورت زیر است:

𝑦
=
𝑓
(
𝑤
𝑇
𝑥
+
𝑤
0
)
y=f(w 
T
 x+w 
0
​
 )
که در آن:

𝑥
x بردار ورودی،

𝑤
w بردار وزن‌ها،

𝑤
0
w 
0
​
  بایاس،

𝑓
f تابع فعال‌سازی (مثلاً تابع پله‌ای).

جداسازی خطی:
یک نورون یک مرز تصمیم‌گیری خطی تعریف می‌کند:

𝑤
𝑇
𝑥
+
𝑤
0
=
threshold
w 
T
 x+w 
0
​
 =threshold
(آستانه برای تابع پله‌ای معمولاً ۰ و برای سیگموید ۰.۵ است).

قاعده تصمیم:
𝑦
^
=
{
𝐶
1
اگر 
𝑤
𝑇
𝑥
+
𝑤
0
≥
threshold
𝐶
2
در غیر این صورت
y
^
​
 ={ 
C 
1
​
 
C 
2
​
 
​
  
اگر w 
T
 x+w 
0
​
 ≥threshold
در غیر این صورت
​
 
محدودیت‌های پرسپترون منفرد:
جداسازی خطی:
پرسپترون قادر است مسائل جداسازی‌پذیر خطی مانند AND و OR را حل کند.

شکست در مسائل غیرخطی:
پرسپترون منفرد نمی‌تواند مسائل غیرخطی مثل XOR را حل کند، چون نمی‌توان نقاط داده را با یک خط مستقیم جدا کرد.

توضیح:
پرسپترون بسیار ساده است. اگر داده‌ها قابل جداسازی با یک خط مستقیم باشند، خوب عمل می‌کند؛ ولی در مسائل پیچیده‌تر و غیرخطی مانند XOR، پرسپترون تنها قادر به یادگیری نیست.

به سمت مرزهای تصمیم‌گیری پیچیده‌تر (Multi-Layer Perceptron - MLP):
افزودن لایه‌ها برای پیچیدگی بیشتر:
MLP شامل چندین لایه نورون است که امکان مدل‌سازی توابع پیچیده‌تر نسبت به یک نورون منفرد را فراهم می‌کند.

لایه‌های جدید = مرزهای تصمیم جدید:
هر لایه، مرزهای تصمیم جدیدی ایجاد می‌کند که باعث جداسازی داده‌های غیرخطی می‌شود.

مثال ساختار دو لایه:
لایه ورودی → لایه پنهان (Hidden Layer) → لایه خروجی (Output Layer).

لایه پنهان تبدیل‌های غیرخطی را معرفی می‌کند که مناطق تصمیم‌گیری پیچیده‌تری را امکان‌پذیر می‌سازد.

توضیح:
برای حل مسائل پیچیده‌تر مانند XOR، چند پرسپترون را به صورت لایه‌ای به هم متصل می‌کنیم. به این ساختار پرسپترون چندلایه (MLP) یا شبکه عصبی (Neural Network) گفته می‌شود. ترکیب لایه‌ها و غیرخطی بودن بین آن‌ها باعث می‌شود مدل قادر به یادگیری مرزهای تصمیم‌گیری پیچیده و غیرخطی باشد.


البته، اینجا متن شما را با نگارش روان‌تر و قالب‌بندی منظم بازنویسی کردم، بدون تغییر معنی و محتوا:

---

## ۵. توابع هزینه (Cost Functions)

### درک هدف:

در پرسپترون، از $w^T x$ برای پیش‌بینی استفاده می‌شود.
هدف پیدا کردن بردار وزن بهینه $w$ است به طوری که برچسب‌های پیش‌بینی‌شده تا حد امکان با برچسب‌های واقعی مطابقت داشته باشند.

برای رسیدن به این هدف، یک **تابع هزینه** تعریف می‌شود که تفاوت بین برچسب‌های پیش‌بینی شده و واقعی را اندازه‌گیری می‌کند.

یافتن توابع تشخیص $(w^T, w_0)$ به صورت کمینه‌سازی یک تابع هزینه فرمول‌بندی می‌شود.
بر اساس مجموعه آموزشی $D$، یک تابع هزینه $J(w)$ تعریف می‌شود.
مسئله تبدیل می‌شود به یافتن:

$$
\hat{w} = \arg\min_w J(w)
$$

---

### تابع هزینه مجموع مربعات خطا (Sum of Squared Error - SSE) برای طبقه‌بندی:

فرمول:

$$
J(w) = \sum_{i=1}^n \left( y^{(i)} - \hat{y}^{(i)} \right)^2
$$

که در آن:

$$
\hat{y}^{(i)} = w^T x^{(i)} + w_0
$$

---

### محدودیت‌ها برای طبقه‌بندی:

* SSE بزرگی خطا را کمینه می‌کند، که برای رگرسیون ایده‌آل است اما برای طبقه‌بندی مناسب نیست.

* حتی اگر مدل نزدیک به کلاس واقعی پیش‌بینی کند اما دقیقاً 0 یا 1 نباشد، SSE همچنان خطای مثبت نشان می‌دهد.
  مثلاً اگر $y=1$ باشد و مدل $0.9$ پیش‌بینی کند، SSE هنوز خطا دارد، در حالی که برای طبقه‌بندی این پیش‌بینی صحیح است.

* SSE مستعد **بیش‌برازش (Overfitting)** به داده‌های نویزی است، چون تغییرات کوچک می‌توانند تغییرات قابل توجهی در هزینه ایجاد کنند.

---

### جایگزینی برای تابع هزینه SSE: تعداد اشتباه طبقه‌بندی شده

* تعریف: اندازه می‌گیرد چند نمونه توسط مدل اشتباه طبقه‌بندی شده‌اند.

* فرمول:

$$
J(w) = \sum_{i=1}^n \left( 2 y^{(i)} - \operatorname{sign}(\hat{y}^{(i)}) \right)^2
$$

که در آن $\hat{y}^{(i)} = w^T x^{(i)} + w_0$ و $y^{(i)} \in \{-1, +1\}$.
اگر $y$ و $\hat{y}$ برابر باشند، ترم صفر است و در غیر این صورت 1 می‌شود.

---

### محدودیت‌ها:

* این تابع هزینه **ناپیوسته و غیرقابل مشتق‌گیری (Non-differentiable)** است.
  بنابراین تکنیک‌های بهینه‌سازی مانند گرادیان نزولی (Gradient Descent) نمی‌توانند مستقیم روی آن اعمال شوند.

---

### توضیح مهم:

SSE برای طبقه‌بندی مناسب نیست چون به "دقت عددی" پیش‌بینی حساس است نه به "درستی کلاس".
تابع تعداد اشتباه طبقه‌بندی شده خوب است ولی چون پله‌ای و ناپیوسته است، نمی‌توان گرادیان آن را محاسبه کرد.

---

## الگوریتم پرسپترون (Perceptron Algorithm)

### هدف:

یک الگوریتم ساده برای طبقه‌بندی دودویی که دو کلاس را با یک مرز خطی از هم جدا می‌کند.

---

### معیار پرسپترون (Perceptron Criterion - Cost Function):

این تابع هزینه فقط روی نقاط **اشتباه طبقه‌بندی شده** تمرکز دارد:

$$
J_p(w) = - \sum_{i \in M} y^{(i)} w^T x^{(i)}
$$

که $M$ مجموعه نقاط اشتباه طبقه‌بندی شده است و $y^{(i)} \in \{-1, +1\}$.

---

### هدف:

کمینه کردن این خطا با طبقه‌بندی صحیح همه نقاط.
برخلاف تابع تعداد اشتباه طبقه‌بندی شده، این تابع هزینه **محدب** و قابل بهینه‌سازی است.

---

### پرسپترون دسته‌ای (Batch Perceptron):

* در هر تکرار، بردار وزن را با استفاده از **تمام نقاط اشتباه طبقه‌بندی شده** به‌روزرسانی می‌کند.

* به‌روزرسانی وزن با گرادیان نزولی:

$$
w \leftarrow w - \eta \nabla_w J_p(w)
$$

که

$$
\nabla_w J_p(w) = - \sum_{i \in M} y_i x_i
$$

---

### همگرایی:

* پرسپترون دسته‌ای برای داده‌های **جداسازی‌پذیر خطی** در تعداد مراحل متناهی همگرا می‌شود.

---

### پرسپترون تک‌نمونه (Single-sample Perceptron):

* وزن را پس از پردازش هر نمونه به‌روزرسانی می‌کند.

* قاعده به‌روزرسانی (Stochastic Gradient Descent):

$$
w \leftarrow w + \eta y_i x_i
$$

* مزایا: هزینه محاسباتی کمتر و همگرایی سریع‌تر.

* همگرایی: برای داده‌های جداسازی‌پذیر خطی تضمین شده است.

---

### همگرایی پرسپترون (Convergence of the Perceptron):

* اگر داده‌ها **خطی جداسازی‌پذیر نباشند**، پرسپترون همگرا نمی‌شود و وزن‌ها به طور مداوم در حال به‌روزرسانی هستند.

* در نتیجه، الگوریتم هرگز به یک راه‌حل پایدار نمی‌رسد.

---

### الگوریتم Pocket:

* برای داده‌هایی که به دلیل نویز یا غیرخطی بودن **خطی جداسازی‌پذیر نیستند**، الگوریتم Pocket بهترین وزن $w$ که تا کنون کمترین خطا را داشته، ذخیره می‌کند.

* این الگوریتم وزن‌ها را به‌روزرسانی می‌کند و همزمان بهترین وزن را که کمترین خطا را دارد در "جیب" نگه می‌دارد.

* حتی اگر همگرایی کامل اتفاق نیفتد، بهترین راه‌حل ممکن در دسترس خواهد بود.

---

اگر بخواهید، می‌توانم بخش‌های خاصی را با فرمول‌ها یا توضیحات بیشتری هم توسعه دهم.
البته! متن شما را با ویرایش سبک نگارش و قالب‌بندی روان‌تر اینجا آوردم:

---

## ۶. اعتبارسنجی متقابل (Cross Validation)

### انتخاب مدل از طریق اعتبارسنجی متقابل (Model Selection via Cross Validation):

* **هدف:**
  تکنیکی برای ارزیابی میزان تعمیم‌پذیری مدل روی داده‌های جدید و نادیده (unseen data).

---

### چگونگی کارکرد (K-Fold Cross Validation):

* داده‌ها به $K$ بخش مساوی (fold) تقسیم می‌شوند.

* مدل روی $K-1$ بخش آموزش داده می‌شود و روی بخش باقی‌مانده برای اعتبارسنجی (تست) ارزیابی می‌شود.

* این فرآیند $K$ بار تکرار می‌شود، به طوری که هر بار یک بخش متفاوت به عنوان بخش تست انتخاب می‌شود.

* امتیاز نهایی مدل میانگین امتیازهای به دست آمده در هر بار اعتبارسنجی است.

---

### مزایا:

* کاهش ریسک **بیش‌برازش** (overfitting) مدل.

* ارائه برآورد قابل اعتمادتر و واقعی‌تر از عملکرد مدل روی داده‌های جدید.

---

### توضیح ساده:

کراس‌ولیدیشن مثل این است که شما چند امتحان از یک درس می‌دهید. هر بار بخشی از کتاب را کنار می‌گذارید تا در امتحان از آن سوال بیاید، بقیه را می‌خوانید. به این ترتیب اطمینان حاصل می‌کنید که فقط حفظ نکردید بلکه واقعاً موضوع را یاد گرفته‌اید.

---

## اعتبارسنجی متقابل «ترک یک نمونه» (Leave-One-Out Cross-Validation - LOOCV):

* **چگونگی کارکرد:**
  در هر بار، یک نمونه به عنوان داده اعتبارسنجی انتخاب می‌شود و باقی نمونه‌ها برای آموزش استفاده می‌شوند. این روند برای تمام نمونه‌ها به صورت جداگانه تکرار می‌شود.

* اگر مجموعه داده $N$ نمونه داشته باشد، مدل $N$ بار آموزش داده می‌شود.

---

### خواص LOOCV:

* **عدم اتلاف داده:**
  هر نمونه هم برای آموزش و هم برای اعتبارسنجی استفاده می‌شود.

* **واریانس بالا و بایاس پایین:**
  مدل با تقریباً تمام داده‌ها آموزش می‌بیند (بایاس کم) اما چون مجموعه تست فقط یک نمونه است، ارزیابی می‌تواند واریانس بالایی داشته باشد.

* **هزینه محاسباتی بالا:**
  برای مجموعه‌های بزرگ، آموزش مدل $N$ بار بسیار زمان‌بر است.

* **مناسب برای مجموعه‌های داده کوچک:**
  به دلیل ویژگی‌های بالا، بیشتر برای داده‌های کم‌حجم کاربرد دارد.

---

## اعتبارسنجی متقابل برای انتخاب عبارت رگولاریزاسیون (Regularization Term):

* از CV برای پیدا کردن بهترین مقدار هایپرپارامترهایی مانند $\lambda$ در رگولاریزاسیون استفاده می‌شود که کنترل‌کننده پیچیدگی مدل است.

* تأثیر مقدار $\lambda$:

  * $\lambda$ بزرگ → مدل ساده‌تر و کم‌برازش (underfitting).

  * $\lambda$ متوسط → تعادل بهینه بین برازش و پیچیدگی.

  * $\lambda$ کوچک → مدل پیچیده‌تر و ممکن است بیش‌برازش (overfitting) رخ دهد.

---

## اعتبارسنجی متقابل برای انتخاب پیچیدگی مدل:

* با استفاده از CV می‌توان مدل‌هایی با پیچیدگی‌های متفاوت (مثلاً رگرسیون چندجمله‌ای با درجات مختلف) را مقایسه کرد و بهترین مدل را انتخاب نمود.

* مثال: مقایسه مدل‌های درجه مختلف در صفحه 42 PDF.

---

اگر بخواهید، می‌توانم مثال‌های کد یا توضیحات تکمیلی هم اضافه کنم.





















۷. طبقه‌بندی چندکلاسه (Multi-Category Classification)

### راه‌حل‌ها برای مسئله طبقه‌بندی چندکلاسه:

۱. **گسترش الگوریتم یادگیری برای پشتیبانی از چندکلاسه:**
برای هر کلاس $C_i$، یک تابع تشخیص $g_i$ تعریف می‌شود.
سپس ورودی $x$ به کلاسی $C_i$ تخصیص داده می‌شود که مقدار تابع تشخیص آن بزرگ‌تر از سایر کلاس‌ها باشد، یعنی:

$$
\hat{y} = \arg\max_{i=1,\ldots,c} g_i(x)
$$

۲. **تبدیل مسئله به مجموعه‌ای از مسائل دو-دسته (Two-categorical problems):**

* روش **One-vs-Rest (OvR)** یا **One-vs-All (OvA):**
  برای هر کلاس، یک طبقه‌بندی‌کننده دودویی آموزش داده می‌شود که آن کلاس را از بقیه کلاس‌ها جدا می‌کند (مثلاً کلاس 1 در برابر کلاس‌های 2، 3 و 4).

* روش **One-vs-One (OvO):**
  برای هر جفت کلاس، یک طبقه‌بندی‌کننده دودویی آموزش داده می‌شود (مثلاً کلاس 1 در برابر کلاس 2، کلاس 1 در برابر کلاس 3، و ...).

---

### توضیحات اضافه:

وقتی بیش از دو کلاس دارید، دو راه اصلی برای طبقه‌بندی وجود دارد:

* یا الگوریتم را از ابتدا برای چندکلاسه بودن طراحی کنید (مثل Softmax Regression)،
* یا مسئله چندکلاسه را به چند مسئله دودویی ساده‌تر تقسیم کنید و برای هر کدام طبقه‌بندی‌کننده‌ای آموزش دهید.

---

### ابهام در طبقه‌بندی چندکلاسه (Ambiguity):

روش‌های OvR و OvO ممکن است مناطقی ایجاد کنند که طبقه‌بندی در آن‌ها نامشخص است (مناطق مبهم). (تصاویر صفحه ۴۵ PDF این مناطق مبهم را نشان می‌دهند)

---

### ماشین‌های خطی (Linear Machines) برای طبقه‌بندی چندکلاسه:

جایگزینی برای روش‌های OvR و OvO که در آن هر کلاس با یک تابع تشخیص $g_i(x)$ نمایش داده می‌شود.

* قاعده تصمیم:

$$
\hat{y} = \arg\max_{i=1,\ldots,c} g_i(x)
$$

* مرز تصمیم بین کلاس‌های $i$ و $j$ جایی است که:

$$
g_i(x) = g_j(x) \implies (w_i - w_j)^T x + (w_{0i} - w_{0j}) = 0
$$

* **خواص مناطق تصمیم:** مناطق تصمیم این توابع محدب (convex) و تک‌اتصالی (singly connected) هستند؛ یعنی اگر دو نقطه از یک منطقه انتخاب شوند، خط واصل آن‌ها نیز کاملاً داخل همان منطقه است.

---

### الگوریتم پرسپترون چندکلاسه (Multi-Class Perceptron Algorithm):

* **بردار وزن‌ها:**
  یک ماتریس وزن $W \in \mathbb{R}^{m \times K}$ داریم که $m$ تعداد ویژگی‌ها و $K$ تعداد کلاس‌ها است.
  هر ستون $w_k$ بردار وزن کلاس $k$ است.

* **قاعده پیش‌بینی:**

$$
\hat{y} = \arg\max_{i=1,\ldots,c} w_i^T x
$$

* **تابع هزینه:**
  معیار پرسپترون روی نقاط اشتباه طبقه‌بندی شده تمرکز دارد:

$$
J_p(W) = - \sum_{i \in M} (w_{y^{(i)}} - w_{\hat{y}^{(i)}})^T x^{(i)}
$$

که $M$ مجموعه نقاط اشتباه است.

---

### الگوریتم به‌روزرسانی (صفحه ۴۹ PDF):

اگر نمونه $x^{(i)}$ اشتباه طبقه‌بندی شد (یعنی $y^{(i)} \neq \hat{y}^{(i)}$):

* وزن کلاس اشتباه کاهش می‌یابد:

$$
w_{\hat{y}^{(i)}} \leftarrow w_{\hat{y}^{(i)}} - \eta x^{(i)}
$$

* وزن کلاس واقعی افزایش می‌یابد:

$$
w_{y^{(i)}} \leftarrow w_{y^{(i)}} + \eta x^{(i)}
$$

این به‌روزرسانی‌ها وزن‌ها را به گونه‌ای تغییر می‌دهند که امتیاز کلاس واقعی افزایش و امتیاز کلاس اشتباه کاهش یابد.

---

اگر لازم بود، می‌توانم به تفصیل مثال یا نکات بیشتری هم ارائه کنم.








































عالیه! 🧠 فایل دوم که فرستادی مربوط به جلسه‌ی سوم درس «یادگیری ماشین» با موضوع **طبقه‌بندی خطی (Linear Classification)** هست و توسط دکتر علی شریفی زارعی تدریس شده.

حالا برات یک **جزوه‌ی کامل، آموزش‌محور و مفهومی** آماده کردم، دقیقاً مثل جزوه جلسه‌ی قبل، اما این بار بر اساس مباحث:

> 📚 Introduction to Classification
> 🧮 Discriminant Functions
> ➗ Linear Classifiers
> 🔁 Perceptron & Cost Functions
> ✅ Cross-validation
> 🔢 Multi-class Classification

---

## 📘 جزوه‌ی جلسه سوم: طبقه‌بندی خطی (Linear Classification)

---

### 1. مقدمه‌ای بر طبقه‌بندی (Classification)

**توضیح تکمیلی:**
طبقه‌بندی (Classification) یکی از بنیادی‌ترین مسائل یادگیری ماشین است که هدف آن **تعیین برچسب کلاس برای نمونه‌های ورودی** است. برخلاف رگرسیون که مقدار پیوسته پیش‌بینی می‌کند، خروجی در طبقه‌بندی **دسته‌بندی‌شده و گسسته** است.

**مثال‌های واقعی:**

* تشخیص اسپم بودن ایمیل
* پیش‌بینی بیماری (مثبت/منفی)
* تشخیص ترک شغل (Churn)

---

### 2. تفاوت Classification و Regression

| ویژگی     | رگرسیون (Regression)         | طبقه‌بندی (Classification)       |
| --------- | ---------------------------- | -------------------------------- |
| نوع خروجی | عدد پیوسته (مثلاً قیمت خانه) | برچسب گسسته (مثلاً اسپم / نرمال) |
| هدف       | پیش‌بینی مقدار دقیق          | تعیین کلاس صحیح                  |
| کاربرد    | پیش‌بینی روند قیمت، آب‌وهوا  | تشخیص بیماری، فیلتر اسپم         |

---

### 3. تابع تشخیص (Discriminant Function)

**توضیح تکمیلی:**
تابع تشخیص (Discriminant Function) تابعی است که به یک ورودی x، **یک نمره اختصاص می‌دهد** که بیانگر میزان شباهت آن به هر کلاس است.

**نحوه عملکرد:**

* در حالت دودسته‌ای:

  $$
  \hat{y} = 
  \begin{cases}
  C_1 & \text{اگر } g_1(x) > g_2(x) \\
  C_2 & \text{در غیر این صورت}
  \end{cases}
  $$

* در حالت چند‌کلاسه:

  $$
  \hat{y} = \arg\max_i g_i(x)
  $$

---

### 4. طبقه‌بند خطی (Linear Classifier)

**توضیح تکمیلی:**
در مدل‌های خطی، تصمیم‌گیری از طریق یک **مرز تصمیم خطی** انجام می‌شود.
فرم تابع تشخیص:

$$
g(x) = w^T x + w_0
$$

که w بردار وزن و $w_0$ بایاس است.

**قانون تصمیم‌گیری:**

$$
\hat{y} = 
\begin{cases}
C_1 & \text{اگر } g(x) \geq 0 \\
C_2 & \text{در غیر این صورت}
\end{cases}
$$

---

### 5. مفهوم Decision Boundary

**مرز تصمیم (Decision Boundary)** صفحه‌ای در فضای ویژگی‌هاست که داده‌های متعلق به کلاس‌های مختلف را از هم جدا می‌کند.

در بعد ۲، این یک خط است.
در بعد ۳، یک صفحه.
در بعد d، یک هایپرپلین (Hyperplane) از مرتبه d−1.

---

### 6. تبدیل ویژگی‌ها (Feature Transformation)

**مسئله:** داده‌ها گاهی **خطی قابل تفکیک نیستند.**

**راه‌حل:** نگاشت داده‌ها به فضای با بعد بالاتر با استفاده از تابع φ(x)، به‌طوری‌که در فضای جدید، داده‌ها قابل جداسازی باشند. این کار در مدل‌هایی مثل SVM و MLP مرسوم است.

---

### 7. پرسپترون (Perceptron)

**توضیح تکمیلی:**
مدل پرسپترون یک **نورون مصنوعی ساده** است. یک تابع خطی از ورودی‌ها می‌گیرد و روی آن یک تابع آستانه (مثل sign یا step) اعمال می‌کند.

$$
y = f(w^T x + w_0)
$$

**محدودیت:** تنها قادر به حل مسائل **خطی قابل تفکیک** است، مثل AND و OR، اما نه XOR.

---

### 8. الگوریتم پرسپترون

**نسخه Batch:**

* همزمان از همه نقاط اشتباه استفاده می‌شود.
* به کمک گرادیان کاهشی به‌روزرسانی می‌شود:

  $$
  w \leftarrow w - \eta \sum_{i \in M} y^{(i)} x^{(i)}
  $$

**نسخه Single-sample (SGD):**

* بعد از هر نمونه اشتباه، وزن آپدیت می‌شود:

  $$
  w \leftarrow w + \eta y^{(i)} x^{(i)}
  $$

---

### 9. الگوریتم Pocket

**توضیح تکمیلی:**
اگر داده‌ها **قابل جداسازی خطی نباشند** (مثلاً به دلیل نویز)، الگوریتم پرسپترون هرگز همگرا نمی‌شود.

**راه‌حل:**
الگوریتم Pocket بهترین وزن دیده‌شده را تا آن لحظه نگه می‌دارد و همیشه خروجی نهایی را بر اساس بهترین وزن ارائه می‌دهد.

---

### 10. توابع هزینه در طبقه‌بندی

**1. Sum of Squared Error (SSE):**

$$
J(w) = \sum_{i=1}^n (y^{(i)} - \hat{y}^{(i)})^2
$$

در طبقه‌بندی زیاد مناسب نیست چون به مقادیر نزدیک ولی اشتباه هم امتیاز مثبت می‌دهد.

**2. تعداد نمونه‌های اشتباه:**

$$
J(w) = \sum_{i=1}^n \left(\frac{y^{(i)} - \text{sign}(w^T x^{(i)})}{2}\right)^2
$$

غیر مشتق‌پذیر است → نمی‌توان از گرادیان دیسنت استفاده کرد.

---

### 11. Cross-Validation و ارزیابی مدل

**توضیح تکمیلی:**
برای اینکه بفهمیم مدل روی داده جدید چقدر خوب عمل می‌کند، از Cross-validation استفاده می‌کنیم.

* **K-Fold CV:** داده به K قسمت تقسیم شده، به‌تناوب آموزش و ارزیابی می‌شود.
* **LOOCV (Leave-One-Out):** هر بار فقط یک داده به عنوان تست در نظر گرفته می‌شود (دقت بالا ولی زمان‌بر).

---

### 12. طبقه‌بندی چندکلاسه (Multi-Class Classification)

**روش‌های متداول:**

* One-vs-Rest (OvR): برای هر کلاس یک مدل دوکلاسه ساخته می‌شود.
* One-vs-One (OvO): برای هر زوج کلاس، یک طبقه‌بند ساخته می‌شود.
* Linear Machines: تعریف یک تابع g برای هر کلاس و انتخاب بیشینه:

  $$
  \hat{y} = \arg\max_i g_i(x)
  $$

---

### 13. پرسپترون چندکلاسه

مدلی مشابه پرسپترون ساده، اما با وزن‌های متفاوت برای هر کلاس:

$$
W \in \mathbb{R}^{m \times K}
$$

اگر x به اشتباه به کلاس $\hat{y}$ تخصیص داده شود:

* وزن کلاس درست زیاد می‌شود
* وزن کلاس اشتباه کم می‌شود

---
خیلی عالی، الان برات یک **جزوه‌ی کامل آموزش‌محور** می‌نویسم که هر بخشش شامل حدود **۱۰ خط توضیح مفهومی، آموزشی و یادگیری‌محور** هست. این ساختار طوری طراحی شده که انگار دارم بهت تدریس می‌کنم.

---

## 📘 جزوه‌ی آموزشی جلسه ۳

### موضوع: طبقه‌بندی خطی (Linear Classification)

بر اساس اسلایدهای دکتر علی شریفی زارعی – دانشگاه صنعتی شریف

---

### 1. مقدمه‌ای بر طبقه‌بندی

در یادگیری ماشین، طبقه‌بندی (Classification) یکی از مهم‌ترین مسائل است که در آن هدف ما پیش‌بینی **دسته یا کلاس یک نمونه** است. برای مثال، ما می‌خواهیم یک ایمیل را به عنوان "اسپم" یا "نرمال" تشخیص دهیم. برخلاف رگرسیون که مقدار پیوسته مانند دما یا قیمت پیش‌بینی می‌کند، در طبقه‌بندی ما خروجی گسسته داریم مثل ۰/۱ یا کلاس A/B/C. معمولاً داده‌های ورودی ما x هستند (ویژگی‌ها) و خروجی y یک برچسب از بین چند دسته ممکن است. این مسئله در کاربردهایی مانند پزشکی، فیلتر ایمیل، و تحلیل بازار کاربرد دارد.

---

### 2. تابع تشخیص (Discriminant Function)

برای اینکه بتوانیم بین کلاس‌ها تصمیم‌گیری کنیم، باید به هر نمونه‌ی ورودی یک «نمره‌ی تعلق» بدهیم. این کار با تابع تشخیص انجام می‌شود. در ساده‌ترین حالت، برای دو کلاس، دو تابع g₁(x) و g₂(x) تعریف می‌کنیم و کلاس با نمره بالاتر را انتخاب می‌کنیم. در حالت چندکلاسه، از تابع $\hat{y} = \arg\max_i g_i(x)$ استفاده می‌کنیم. این توابع معمولاً به صورت خطی یا غیرخطی از x هستند. هدف اصلی این است که این توابع بتوانند نمونه‌های هر کلاس را به خوبی جدا کنند.

---

### 3. مرز تصمیم (Decision Boundary)

مرز تصمیم همان سطحی است که فضای ورودی را به ناحیه‌هایی تقسیم می‌کند که هر کدام مربوط به یک کلاس هستند. اگر از تابع g(x) = wᵗx + w₀ استفاده کنیم، مرز تصمیم زمانی است که g(x) = 0 می‌شود. در فضای دو‌بعدی، این مرز یک خط است. در فضای سه‌بعدی یک صفحه و در فضای d-بعدی یک هایپرپلین (صفحه با d−1 بعد). نمونه‌های دو طرف این مرز به دو کلاس متفاوت تعلق دارند. در مدل‌های خطی، مرز تصمیم نیز خطی است.

---

### 4. طبقه‌بند خطی (Linear Classifier)

در طبقه‌بند خطی، تصمیم‌گیری از طریق یک تابع خطی مانند $g(x) = w^T x + w_0$ انجام می‌شود. بردار w وزن‌هایی است که یاد گرفته می‌شود، و w₀ بایاس است که مکان مرز تصمیم را تعیین می‌کند. این مدل ساده، سریع و بسیار پرکاربرد است، به‌خصوص در مسائل با داده‌های زیاد یا ویژگی‌های زیاد. داده‌ها وقتی "linearly separable" باشند، یعنی بشود با یک خط آن‌ها را جدا کرد، این مدل به خوبی کار می‌کند.

---

### 5. تبدیل ویژگی‌ها برای تصمیم‌های غیرخطی

گاهی اوقات داده‌ها به صورت خطی قابل جدا شدن نیستند. در چنین شرایطی ما از **تبدیل ویژگی‌ها** استفاده می‌کنیم: ویژگی‌ها را به فضای جدیدی نگاشت می‌کنیم که در آن، داده‌ها قابل جدا شدن باشند. برای مثال، اگر در فضای اولیه داده‌ها شبیه دایره باشند، با افزودن ویژگی‌هایی مثل $x_1^2 + x_2^2$، آن‌ها را در یک فضای جدید به‌صورت خطی جدا می‌کنیم. این ایده، اساس کار مدل‌هایی مثل Kernel SVM و شبکه‌های عصبی است.

---

### 6. پرسپترون (Perceptron)

پرسپترون یکی از قدیمی‌ترین و ساده‌ترین مدل‌های طبقه‌بندی دودویی است. ایده‌ی اصلی آن ساده است: اگر $w^T x + w_0$ بزرگ‌تر از صفر باشد، خروجی را ۱ می‌گیرد وگرنه ۰. این مدل می‌تواند فقط مسائل خطی قابل تفکیک را حل کند، مثلاً AND یا OR، ولی نمی‌تواند مسئله XOR را حل کند. به همین دلیل بعدها پرسپترون چند‌لایه (MLP) معرفی شد که بتواند تصمیم‌های غیرخطی بگیرد.

---

### 7. الگوریتم پرسپترون

در پرسپترون، وزن‌ها به‌صورت تکراری به‌روزرسانی می‌شوند تا نمونه‌های اشتباه به درستی طبقه‌بندی شوند. اگر نمونه x به اشتباه طبقه‌بندی شده باشد، وزن‌ها به این صورت به‌روزرسانی می‌شوند:

$$
w \leftarrow w + \eta y x
$$

این الگوریتم تا زمانی که داده‌ها قابل تفکیک خطی باشند، همگرا می‌شود. دو نسخه رایج دارد: **Batch** که وزن‌ها را بر اساس تمام نمونه‌های اشتباه آپدیت می‌کند، و **Single Sample (SGD)** که فقط از یک نمونه در هر مرحله استفاده می‌کند.

---

### 8. الگوریتم Pocket

وقتی داده‌ها قابل جداسازی نیستند (مثلاً نویز دارند)، الگوریتم پرسپترون همگرا نمی‌شود. در این حالت از الگوریتم Pocket استفاده می‌کنیم. ایده‌اش این است که همیشه بهترین wای که تاکنون دیده شده را نگه می‌دارد. اگر در طی به‌روزرسانی‌ها، یک وزن جدید عملکرد بهتری روی داده آموزش داشته باشد، جایگزین w قبلی می‌شود. این الگوریتم به ویژه در داده‌های واقعی کاربردی است.

---

### 9. تابع هزینه در طبقه‌بندی

در یادگیری، هدف ما کمینه کردن یک تابع هزینه است. در طبقه‌بندی ساده‌ترین تابع هزینه، شمارش نمونه‌های اشتباه است، ولی این تابع **غیرمشتق‌پذیر** است. بنابراین گاهی از **Sum of Squared Errors (SSE)** استفاده می‌شود، ولی SSE برای طبقه‌بندی ایده‌آل نیست چون خطاها را به‌صورت مربعی می‌سنجد. در پرسپترون از تابع هزینه‌ای استفاده می‌شود که فقط روی نمونه‌های اشتباه تأکید دارد: $J(w) = - \sum y^{(i)} w^T x^{(i)}$

---

### 10. Cross-validation و انتخاب مدل

در یادگیری ماشین، ارزیابی درست مدل بسیار مهم است. Cross-validation روشی است که به ما کمک می‌کند مدل را روی داده‌ای که قبلاً ندیده، آزمایش کنیم. رایج‌ترین روش، K-fold است که داده‌ها را به K بخش تقسیم می‌کند. در هر دور یکی از بخش‌ها تست می‌شود و بقیه آموزش. سپس میانگین عملکرد گزارش می‌شود. این کار باعث می‌شود احتمال overfitting کم‌تر و ارزیابی مدل واقع‌بینانه‌تر باشد.

---

### 11. طبقه‌بندی چندکلاسه (Multi-Class)

بسیاری از مسائل دنیای واقعی بیشتر از دو کلاس دارند. برای حل این نوع مسائل از دو روش استفاده می‌شود:

* One-vs-Rest (OvR): یک مدل برای هر کلاس ساخته می‌شود که آن کلاس را از بقیه جدا کند.
* One-vs-One (OvO): برای هر جفت کلاس، یک طبقه‌بند ساخته می‌شود.

راه دیگر، ساختن تابع تشخیص برای هر کلاس است و انتخاب کلاس با بیشترین خروجی:

$$
\hat{y} = \arg\max_i g_i(x)
$$

---

### 12. پرسپترون چندکلاسه

برای طبقه‌بندی چندکلاسه، از بردار وزن جداگانه برای هر کلاس استفاده می‌کنیم. ماتریس وزن‌ها $W \in \mathbb{R}^{m \times K}$ تعریف می‌شود. برای هر نمونه، خروجی کلاس با بزرگ‌ترین $w_k^T x$ به عنوان پیش‌بینی انتخاب می‌شود. در صورت اشتباه، وزن کلاس اشتباه کاهش و وزن کلاس درست افزایش می‌یابد. این الگوریتم سریع و ساده است و در عمل کارایی خوبی دارد، به‌خصوص وقتی داده‌ها بزرگ اما نسبتاً ساده باشند.

---

اگر تأیید کنی، می‌تونم همین متن رو به صورت یک فایل PDF ساخت‌یافته هم برات بسازم. همچنین می‌تونم سوالات مصاحبه‌ای مختص همین جلسه رو هم بهش اضافه کنم.

✅ آیا ادامه بدم با PDF و سوالات مصاحبه‌ای؟

