تا اینجا ما رگرسیون خطی و چندجمله‌ای رو از پایه (با حل تحلیلی و گرادیان کاهشی) ساختیم. حالا وقتشه که ببینیم چطور می‌تونیم از کتابخانه‌های آماده پایتون برای انجام این کارها استفاده کنیم. این بخش از کد دقیقاً همین رو نشون میده و بسیار رایج‌تر و کاربردی‌تره\!

-----

## کار با مجموعه داده‌های واقعی و Scikit-learn

تا الان ما با داده‌های مصنوعی کار می‌کردیم. حالا می‌خوایم از یک **مجموعه داده (Dataset)** واقعی استفاده کنیم که از پیش در کتابخانه `scikit-learn` (که بهش `sklearn` میگن و یکی از قدرتمندترین کتابخانه‌های یادگیری ماشین در پایتونه) موجوده: **California Housing Dataset**.

### ۱. بارگذاری مجموعه داده (Dataset)

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
X, y = housing.data, housing.target

feature_names = housing.feature_names
print("Feature names:", feature_names)
```

  * **`from sklearn.datasets import fetch_california_housing`**: این خط، تابع `fetch_california_housing` رو از ماژول `datasets` کتابخانه `sklearn` وارد (import) می‌کنه. این تابع مسئول بارگذاری مجموعه داده مسکن کالیفرنیا هست.
  * **`from sklearn.model_selection import train_test_split`**: این رو قبلاً دیدیم. برای تقسیم داده‌ها به بخش‌های آموزش و آزمایش استفاده میشه.
  * **`from sklearn.preprocessing import StandardScaler`**: این یک ابزار خیلی مهمه برای **پیش‌پردازش داده‌ها (Data Preprocessing)** که جلوتر توضیح میدم.
  * **`housing = fetch_california_housing()`**: تابع رو صدا می‌زنیم و مجموعه داده رو داخل متغیر `housing` قرار میدیم. `housing` یک شیء (object) خاص هست که شامل داده‌ها، هدف‌ها (target) و نام ویژگی‌ها (feature names) است.
  * **`X, y = housing.data, housing.target`**:
      * `housing.data`: این بخش شامل **ویژگی‌ها (Features)** یا همون `X`های ماست. مثلاً "متوسط درآمد", "متوسط سن خانه" و ... .
      * `housing.target`: این بخش شامل **هدف (Target)** یا همون `y`های ماست، که در این مجموعه داده **متوسط قیمت خانه** هست.
      * با این خط، `X` و `y` رو از مجموعه داده استخراج می‌کنیم.
  * **`feature_names = housing.feature_names`**: نام هر کدوم از ویژگی‌های `X` رو در `feature_names` ذخیره می‌کنیم. این برای فهمیدن اینکه هر ستون `X` چی رو نشون میده، خیلی مفیده.
  * **`print("Feature names:", feature_names)`**: نام ویژگی‌ها رو چاپ می‌کنه تا بدونیم با چه داده‌هایی سر و کار داریم.

### ۲. پیش‌پردازش داده (Data Preprocessing)

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

**اهمیت پیش‌پردازش**:
تعداد زیادی از الگوریتم‌های یادگیری ماشین، به خصوص اونهایی که از **فاصله (Distance)** بین داده‌ها یا **گرادیان‌ها (Gradients)** استفاده می‌کنن (مثل رگرسیون خطی و گرادیان کاهشی)، خیلی به **مقیاس (Scale)** ویژگی‌های ورودی حساس هستند. اگر یک ویژگی (مثلاً "متوسط درآمد" با مقادیر بالا) دامنه‌ی خیلی بزرگتری نسبت به ویژگی دیگه (مثلاً "تعداد اتاق‌ها" با مقادیر کم) داشته باشه، مدل ممکنه به ویژگی با دامنه‌ی بزرگتر وزن بیشتری بده، حتی اگر از نظر واقعی کم‌اهمیت‌تر باشه.




برای حل این مشکل، از روشی به نام **استانداردسازی (Standardization)** استفاده می‌کنیم. استانداردسازی داده‌ها رو طوری تغییر میده که:

  * میانگین (Mean) هر ویژگی **صفر** بشه.
  * انحراف معیار (Standard Deviation) هر ویژگی **یک** بشه.

این کار باعث میشه همه ویژگی‌ها در یک مقیاس مشابه قرار بگیرن و مدل بهتر عمل کنه.

  * **`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`**: اینجا دقیقاً مثل قبل، داده‌ها رو به 80 درصد **آموزش (Train)** و 20 درصد **آزمایش (Test)** تقسیم می‌کنیم. این مرحله حیاتیه تا بتونیم عملکرد مدل رو روی داده‌های "دیده نشده" ارزیابی کنیم.
  * **`scaler = StandardScaler()`**: یک نمونه (object) از `StandardScaler` می‌سازیم.
  * **`X_train_scaled = scaler.fit_transform(X_train)`**:
      * `scaler.fit(X_train)`: این متد `fit`، مقادیر میانگین و انحراف معیار رو از **فقط داده‌های آموزشی (`X_train`)** یاد می‌گیره. **مهمه که این مقادیر رو فقط از داده‌های آموزش یاد بگیریم تا از "نشت داده (Data Leakage)" از مجموعه آزمایش جلوگیری کنیم.**
      * `scaler.transform(X_train)`: با استفاده از میانگین و انحراف معیاری که یاد گرفته، داده‌های `X_train` رو **استانداردسازی (scaling)** می‌کنه و نتیجه رو در `X_train_scaled` قرار میده.
  * **`X_test_scaled = scaler.transform(X_test)`**:
      * برای داده‌های آزمایشی (`X_test`)، فقط از متد `transform` استفاده می‌کنیم. یعنی از **همون میانگین و انحراف معیاری که از `X_train` یاد گرفته بودیم**، برای استانداردسازی `X_test` استفاده می‌کنیم. **هرگز نباید `fit` رو روی `X_test` اجرا کرد\!**





## 🎯 اول تعریف دقیق:

| عمل               | توضیح                                                            |
| ----------------- | ---------------------------------------------------------------- |
| `fit()`           | **یادگیری آماری** از داده‌ها (مثلاً میانگین و انحراف معیار)      |
| `transform()`     | **تغییر دادن داده‌ها** بر اساس آماری که در `fit()` یاد گرفته شده |
| `fit_transform()` | ترکیبی از هر دو: اول `fit()`، بعد `transform()` روی همان داده    |

---

## 🧪 مثال ساده: نرمال‌سازی با StandardScaler

فرض کن داده‌ی خام ما فقط یک ویژگی داره:

```python
X_train = [[10], [20], [30]]
X_test = [[40], [50]]
```

### ✅ مرحله ۱: fit روی X\_train

```python
scaler = StandardScaler()
scaler.fit(X_train)
```

اینجا `scaler` میاد و از `X_train`:

* میانگین $\mu = 20$
* انحراف معیار $\sigma = 8.16$

رو **یاد می‌گیره** و **ذخیره** می‌کنه.

---

### ✅ مرحله ۲: transform روی X\_train

```python
X_train_scaled = scaler.transform(X_train)
```

الان با فرمول:

$$
x' = \frac{x - \mu}{\sigma}
$$

برای ۱۰ داریم:

$$
\frac{10 - 20}{8.16} \approx -1.22
$$

و به همین شکل برای بقیه.

---

### ⚠️ حالا بریم سراغ X\_test

```python
X_test_scaled = scaler.transform(X_test)
```

**اینجا نباید fit کنیم!** چون `X_test` رو در زمان آموزش نباید ببینیم (یعنی مدل نباید از آینده خبر داشته باشه!)

اگه اشتباهی `fit(X_test)` بزنی، میانگین و انحراف معیار جدیدی از `X_test` یاد گرفته می‌شه، و این یعنی **مدل تقلب کرده**!

---

## ✅ چرا جدا کردن fit و transform مهمه؟

| کار                  | چرا انجام می‌دیم؟                                          |
| -------------------- | ---------------------------------------------------------- |
| `fit(X_train)`       | مدل باید فقط از آموزش یاد بگیره                            |
| `transform(X_train)` | داده رو برای آموزش آماده می‌کنیم                           |
| `transform(X_test)`  | همون آماده‌سازی رو روی تست اجرا می‌کنیم، بدون یادگیری جدید |

---

## 🔁 یادآوری:

```python
# آموزش:
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

# تست:
X_test_scaled = scaler.transform(X_test)
```

اگر بخوای هم `fit` و هم `transform` رو با یک خط انجام بدی فقط برای آموزش:

```python
X_train_scaled = scaler.fit_transform(X_train)
```

---

اگر بخوایم تفاوت رو با جمله‌ای ساده بگیم:

> 🔹 `fit` = یادگیری از داده‌ها
> 🔹 `transform` = اعمال آنچه یاد گرفته‌ایم به داده‌ها

---

خیلی خوبه که دقیق دنبال می‌کنی! 🧠
حالا بیایم همون مثالی که داشتیم رو **تا انتها ادامه بدیم** و دقیق ببینیم وقتی این خط اجرا می‌شه:

```python
X_test_scaled = scaler.transform(X_test)
```

چه اتفاقی برای `X_test` می‌افته 👇

---

## 🧪 داده‌ها:

```python
X_train = [[10], [20], [30]]
X_test = [[40], [50]]
```

### 📌 مرحله ۱: `fit` روی X\_train

```python
scaler = StandardScaler()
scaler.fit(X_train)
```

محاسبه‌ی میانگین و انحراف معیار:

* میانگین:

  $$
  \mu = \frac{10 + 20 + 30}{3} = 20
  $$
* انحراف معیار:

  $$
  \sigma = \sqrt{\frac{(10 - 20)^2 + (20 - 20)^2 + (30 - 20)^2}{3}} = \sqrt{\frac{100 + 0 + 100}{3}} \approx 8.16
  $$

---

### 📌 مرحله ۲: `transform` روی X\_test

```python
X_test_scaled = scaler.transform(X_test)
```

یعنی حالا با همون **μ = 20** و **σ = 8.16** که از `X_train` یاد گرفته، می‌خوایم `X_test` رو نرمال کنیم:

---

### 📐 محاسبه مقادیر نرمال‌شده:

#### برای 40:

$$
z = \frac{40 - 20}{8.16} \approx \frac{20}{8.16} \approx 2.45
$$

#### برای 50:

$$
z = \frac{50 - 20}{8.16} \approx \frac{30}{8.16} \approx 3.67
$$

---

### ✅ نتیجه نهایی:

```python
X_test_scaled ≈ [[2.45], [3.67]]
```

یعنی:

| مقدار اصلی (X\_test) | مقدار نرمال‌شده (X\_test\_scaled) |
| -------------------- | --------------------------------- |
| 40                   | 2.45                              |
| 50                   | 3.67                              |

---

## 🔍 جمع‌بندی نهایی:

* **فقط از X\_train** برای یادگیری استفاده شد.
* X\_test هم فقط **بر اساس همون آمار قبلی نرمال شد**.
* این یعنی مدل مثل یک آدم "واقعی" فقط با داده‌های آموزش تمرین کرده و در زمان تست **تقلب نکرده**.

---

توضیحتون کاملاً درسته و نشون میده که مفهوم رو خیلی خوب متوجه شدی!

---

## چرا فقط `transform` برای `X_test`؟

این یک اصل بسیار مهم در یادگیری ماشین به نام **"جلوگیری از نشت داده (Data Leakage)"** هست. بیاید با جزئیات بیشتری این موضوع رو بررسی کنیم.

### ۱. **داده‌های آموزش (`X_train`, `y_train`)**: دنیای واقعی که مدل می‌بیند

* وقتی مدل رو **آموزش (Train)** میدیم، در واقع داریم بهش اجازه میدیم که از **داده‌های آموزشی** یاد بگیره. این مثل اینه که شما برای کنکور درس می‌خونی. کتاب‌های درسی و تست‌های سال‌های قبل، داده‌های آموزش شما هستند.
* **`scaler.fit_transform(X_train)`**: تابع `fit()` در `StandardScaler` (یا هر `scaler` دیگه مثل `MinMaxScaler`)، مقادیر آماری (مثل میانگین و انحراف معیار در `StandardScaler` یا حداقل و حداکثر در `MinMaxScaler`) رو از **فقط داده‌های `X_train`** محاسبه می‌کنه.
    * یعنی `scaler` می‌فهمه که میانگین "متوسط درآمد" تو داده‌های آموزش چنده و انحراف معیار این ویژگی چقدره.
    * بعد `transform()` با استفاده از این مقادیر یاد گرفته شده، داده‌های `X_train` رو **تغییر مقیاس** میده.

### ۲. **داده‌های آزمایش (`X_test`, `y_test`)**: دنیای واقعی که مدل برای اولین بار می‌بیند

* **هدف** ما اینه که ببینیم مدلی که آموزش دادیم، چقدر خوب روی **داده‌هایی که هرگز ندیده** عمل می‌کنه. این مثل اینه که شما سر جلسه‌ی کنکور میشینی و با سوالات جدیدی روبرو میشی. اگر سوالات کنکور رو از قبل دیده باشی، نمره‌ات واقعی نیست!
* **`scaler.transform(X_test)`**: حالا که `scaler` مقادیر آماری رو از `X_train` یاد گرفته، از **همون مقادیر** برای تغییر مقیاس `X_test` استفاده می‌کنه.
    * یعنی اگر میانگین "متوسط درآمد" در `X_train` مثلاً 60000 دلار بوده، حتی اگر در `X_test` میانگین "متوسط درآمد" 58000 دلار باشه، `scaler` از همون 60000 دلار و انحراف معیار `X_train` برای استانداردسازی `X_test` استفاده می‌کنه.

### چرا `fit` رو روی `X_test` اجرا نمی‌کنیم؟ (نشت داده)

اگر `fit()` رو روی `X_test` هم اجرا کنیم، یعنی `scaler` میاد و میانگین و انحراف معیار `X_test` رو هم محاسبه می‌کنه. این یعنی اطلاعاتی از **آینده** (داده‌های تستی که قرار بود دیده نشده باشن) به **حال** (فرایند آماده‌سازی داده برای آموزش) نشت پیدا می‌کنه.

**پیامدهای نشت داده:**
1.  **ارزیابی غیرواقعی:** اگر مدل شما اطلاعاتی از داده‌های تست داشته باشه (حتی در حد میانگین و انحراف معیار اونها)، عملکردی که روی داده‌های تست گزارش میده، **بهتر از عملکرد واقعی** مدل در دنیای واقعی خواهد بود. یعنی وقتی مدل رو روی داده‌های جدید و کاملاً ناشناخته اجرا کنی، شوکه میشی چون نتایجش به خوبی نتایج تست نیست.
2.  **Overfitting پنهان:** نشت داده می‌تونه باعث بشه که فکر کنیم مدل داریم که خوب "تعمیم‌پذیره" (Generalizes well)، در حالی که در واقع مدل به داده‌های تست هم تا حدی "اورفیت" شده (overfit).

---

**پس، خلاصه‌اش اینه:**

ما می‌خوایم شبیه‌سازی کنیم که مدل ما وقتی در دنیای واقعی با داده‌های جدید روبرو میشه (که میانگین و انحراف معیار واقعی‌شون رو از قبل نمی‌دونیم)، چطور رفتار می‌کنه. برای همین، **تمام محاسبات آماری مورد نیاز برای پیش‌پردازش (مثل میانگین، انحراف معیار، min، max و...) فقط و فقط باید از مجموعه داده‌های آموزشی (Training Data) به دست بیان.** بعد از اینکه این مقادیر رو یاد گرفتیم، برای تبدیل (transform) هم داده‌های آموزش و هم داده‌های آزمایش از **همون مقادیر یاد گرفته شده** استفاده می‌کنیم.

---

امیدوارم این توضیح به درک عمیق‌تر این مفهوم مهم کمک کرده باشه! سوال دیگه ای داری؟
-----

### ۳. پیاده‌سازی رگرسیون خطی با Scikit-learn

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model = LinearRegression()
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Test RMSE: {rmse:.2f}")
```

این بخش نشون میده که چقدر آسونه با `sklearn` یک مدل رگرسیون خطی ساخت و آموزش داد:

  * **`from sklearn.linear_model import LinearRegression`**: کلاس `LinearRegression` رو وارد می‌کنه.
  * **`from sklearn.metrics import mean_squared_error`**: تابعی برای محاسبه خطای میانگین مربعات (Mean Squared Error - MSE) رو وارد می‌کنه. (قبلاً ما خودمون `compute_rms_error` رو نوشتیم).
  * **`model = LinearRegression()`**: یک نمونه از مدل رگرسیون خطی رو ایجاد می‌کنه.
  * **`model.fit(X_train_scaled, y_train)`**: این خط جادویی\! اینجا مدل رو **آموزش (Train)** میدیم. متد `fit` همون کاری رو می‌کنه که ما با `polynomial_regression` و `gradient_descent` انجام می‌دادیم: پارامترهای `w` (ضرایب) مدل رو با استفاده از داده‌های آموزش `X_train_scaled` و `y_train` پیدا می‌کنه. `sklearn` به صورت پیش‌فرض از روش‌های بهینه شده برای حل این مسئله استفاده می‌کنه (معمولاً حل تحلیلی یا نسخه‌های بهینه‌شده گرادیان کاهشی).
  * **`y_pred = model.predict(X_test_scaled)`**: بعد از آموزش، حالا مدل رو روی داده‌های **آزمایشی (`X_test_scaled`)** اجرا می‌کنیم تا **پیش‌بینی (Prediction)** انجام بده. نتیجه (قیمت‌های پیش‌بینی شده) در `y_pred` ذخیره میشه.
  * **`rmse = np.sqrt(mean_squared_error(y_test, y_pred))`**: RMSE رو محاسبه می‌کنه. `mean_squared_error` همون `(np.mean((y_true - y_pred) ** 2))` ماست، فقط `sklearn` آماده‌اش رو داره. ما خودمون `np.sqrt` رو بهش اضافه می‌کنیم تا RMSE بشه.
  * **`print(f"Test RMSE: {rmse:.2f}")`**: RMSE نهایی مدل رو روی داده‌های تست چاپ می‌کنه. هر چه این عدد کمتر باشه، مدل ما دقیق‌تره.

-----

### ۴. اهمیت ویژگی‌ها (Feature Importance)

```python
coefficients = model.coef_
for name, coef in zip(feature_names, coefficients):
    print(f"{name}: {coef:.3f}")
```

بعد از آموزش یک مدل رگرسیون خطی، می‌تونیم به **ضرایب (Coefficients)** اون مدل نگاه کنیم. این ضرایب به ما میگن که هر ویژگی چقدر در پیش‌بینی متغیر هدف (قیمت خانه) تأثیر داره.

  * **`coefficients = model.coef_`**: بعد از `fit` کردن مدل، ضرایب مدل در صفت `coef_` (توجه: بعد از `coef` یک `_` هست که نشون میده این یک صفت هست که بعد از آموزش مدل مقداردهی شده) ذخیره میشن. این‌ها همون `w`های ما (به جز `w0` یا عرض از مبدأ) هستند. (عرض از مبدأ در `model.intercept_` ذخیره میشه.)
  * **`for name, coef in zip(feature_names, coefficients):`**: با استفاده از تابع `zip`، نام هر ویژگی رو با ضریب مربوط به اون جفت می‌کنه و در یک حلقه چاپ می‌کنه. این به ما کمک می‌کنه بفهمیم مثلاً "MedInc" (متوسط درآمد) چه ضریبی داره.
      * ضریب مثبت یعنی با افزایش اون ویژگی، قیمت خانه هم افزایش پیدا می‌کنه.
      * ضریب منفی یعنی با افزایش اون ویژگی، قیمت خانه کاهش پیدا می‌کنه.
      * قدر مطلق ضریب (اندازه‌اش) نشون دهنده اهمیت نسبی اون ویژگی در مدل هست (البته فقط وقتی که ویژگی‌ها `scaled` شده باشن).


      البته! این نتایج خیلی مهم و جالبه. بذار دقیق برات تفسیر کنم هر کدوم از این ضرایب (coefficients) یعنی چی:

---

## مفهوم ضرایب (coefficients) در رگرسیون خطی

مدل رگرسیون خطی ما فرمول پیش‌بینی رو داره:

$$
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_8 x_8 + b
$$

که هر $w_i$ ضریب متناظر ویژگی $x_i$ است.

---

## بررسی ضرایب چاپ شده:

| ویژگی      | ضریب مدل (Coefficient) | تفسیر                                                                                                                         |
| ---------- | ---------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| MedInc     | 0.854                  | **هر واحد افزایش متوسط درآمد** (MedInc) باعث افزایش تقریبی 0.854 واحد در قیمت خانه می‌شود. یعنی درآمد مهم‌ترین عامل مثبت است. |
| HouseAge   | 0.123                  | سن خانه کمی تأثیر مثبت دارد؛ خانه‌های قدیمی‌تر کمی قیمت بالاتری دارند (یا حداقل رابطه مثبت جزئی وجود دارد).                   |
| AveRooms   | -0.294                 | هرچه میانگین تعداد اتاق‌ها بیشتر باشد، قیمت کاهش می‌یابد؛ شاید به دلیل خانه‌های بزرگ‌تر با اتاق‌های زیاد و کمتر لوکس.         |
| AveBedrms  | 0.339                  | هرچه اتاق خواب‌ها بیشتر باشد، قیمت افزایش می‌یابد؛ اثر مثبت و قابل توجه.                                                      |
| Population | -0.002                 | جمعیت منطقه تقریباً تأثیر منفی خیلی کوچکی دارد (تقریبا بی‌اثر).                                                               |
| AveOccup   | -0.041                 | میانگین تعداد افراد در هر خانه تأثیر منفی کمی دارد.                                                                           |
| Latitude   | -0.897                 | افزایش مقدار Latitude (عرض جغرافیایی) باعث کاهش قیمت می‌شود (مثلا مناطق شمالی قیمت کمتر).                                     |
| Longitude  | -0.870                 | افزایش مقدار Longitude (طول جغرافیایی) نیز باعث کاهش قیمت می‌شود.                                                             |

---

## نکات مهم:

* ضرایب مثبت یعنی **با افزایش آن ویژگی، قیمت پیش‌بینی شده افزایش می‌یابد**.
* ضرایب منفی یعنی **با افزایش آن ویژگی، قیمت پیش‌بینی شده کاهش می‌یابد**.
* ضرایب کوچک یا نزدیک صفر یعنی آن ویژگی تأثیر کمتری دارد.

---

## یک مثال ساده:

اگر متوسط درآمد (MedInc) یک منطقه ۱ واحد افزایش پیدا کند (مثلاً از ۳ به ۴)، قیمت متوسط خانه حدود ۰.۸۵۴ واحد افزایش می‌یابد، البته با فرض ثابت بودن سایر ویژگی‌ها.

---

## جمع‌بندی

* **MedInc** مهم‌ترین عامل مثبت در تعیین قیمت خانه است.
* مختصات جغرافیایی (Latitude و Longitude) تأثیر منفی دارند که می‌تواند به دلایل منطقه‌ای باشد.
* ویژگی‌های مربوط به اتاق‌ها (AveRooms, AveBedrms) اثر متفاوتی دارند که می‌تواند به ساختار خانه‌ها برگردد.

---



-----

### ۵. بصری‌سازی قیمت‌های واقعی در برابر پیش‌بینی شده

```python
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted House Prices")
plt.show()
```

این نمودار یک روش رایج برای ارزیابی بصری عملکرد مدل‌های رگرسیون هست:

  * **`plt.scatter(y_test, y_pred, alpha=0.5)`**: نقاط رو رسم می‌کنه. محور X **قیمت‌های واقعی (`y_test`)** و محور Y **قیمت‌های پیش‌بینی شده (`y_pred`)** توسط مدل هستند. `alpha=0.5` باعث میشه نقاط کمی شفاف باشن تا اگر روی هم افتادن، بشه تراکم رو دید.
  * **`plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)`**: این خط یک خط **مرجع (Reference Line)** رو رسم می‌کنه. این خط از نقطه‌ی `(min_price, min_price)` تا `(max_price, max_price)` میره و به صورت خط چین سیاه هست (`k--`).
      * **تفسیر**: اگر مدل ما **کامل** عمل می‌کرد، یعنی همیشه `y_pred` دقیقاً برابر با `y_test` بود، همه‌ی نقاط روی همین خط مرجع قرار می‌گرفتند. هر چه نقاط به این خط نزدیک‌تر باشند، مدل ما دقیق‌تر عمل کرده. اگر نقاط خیلی پراکنده باشند، یعنی مدل خیلی دقیق نیست.

-----

## جمع‌بندی:

این بخش از کد نشون میده که:

1.  چطور میشه از **مجموعه داده‌های واقعی** موجود در `sklearn` استفاده کرد.
2.  اهمیت و نحوه‌ی انجام **پیش‌پردازش داده‌ها (Data Preprocessing)**، به خصوص **استانداردسازی (Standardization)**، برای بهبود عملکرد مدل‌های یادگیری ماشین.
3.  نحوه‌ی استفاده‌ی آسان از کلاس **`LinearRegression` در `sklearn`** برای ساخت، آموزش و پیش‌بینی با مدل رگرسیون خطی.
4.  چطور میشه **ضرایب مدل (Coefficients)** رو بررسی کرد تا اهمیت ویژگی‌ها رو فهمید.
5.  یک روش **بصری‌سازی** برای مقایسه قیمت‌های واقعی و پیش‌بینی شده.

-----

**حالا سوالاتی که می‌تونی از خودت بپرسی و اگر دوست داشتی جواب بدی:**
خیلی خوبه! 👏
الان تو با این دو خط:

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
```

یک مدل رگرسیون خطی ساختی. بذار دقیق برات توضیح بدم که چه اتفاقی افتاده و مرحله بعد چیه:

---

## ✅ خط 1:

```python
from sklearn.linear_model import LinearRegression
```

* این خط **کلاس LinearRegression** رو از ماژول `sklearn.linear_model` وارد می‌کنه.
* این مدل یکی از ساده‌ترین و پایه‌ای‌ترین مدل‌های رگرسیونه که سعی می‌کنه رابطه‌ی خطی بین ویژگی‌ها و هدف یاد بگیره.

---

## ✅ خط 2:

```python
model = LinearRegression()
```

* اینجا شما یک **شیء (instance)** از مدل ساختی.
* الان `model` آماده است برای یادگیری.

اما هنوز هیچ چیزی یاد نگرفته! یعنی **مدل هنوز آموزش ندیده**.

---

## ❗ مرحله بعدی چیه؟

ما الان داده‌های نرمال‌شده‌مون رو داریم (`X_train_scaled` و `y_train`)
باید با این داده‌ها مدل رو آموزش بدیم:

```python
model.fit(X_train_scaled, y_train)
```

---

## ❓چی میشه وقتی `.fit()` اجرا می‌کنی؟

مدل میاد و:

* بهترین ضرایب (weights) یا همون **ضریب‌های خط رگرسیون** رو پیدا می‌کنه.
* فرمولی به صورت زیر می‌سازه:

$$
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_8 x_8 + b
$$

که این $\hat{y}$ پیش‌بینی مدل برای قیمت خانه است.

---

## ✅ بعدش می‌تونی:

### 1. پیش‌بینی کنی:

```python
y_pred = model.predict(X_test_scaled)
```

### 2. عملکرد مدل رو ارزیابی کنی:

مثلاً با **MSE** یا **R²**:

```python
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("MSE:", mse)
print("R²:", r2)
```

---

اگه بخوای همین‌ها رو برات اجرا کنیم با خروجی‌ها، یا نمودار خط پیش‌بینی بکشیم، فقط بگو.
آماده‌ای برای `fit()`؟ 😎

  * **چرا قبل از آموزش مدل، داده‌ها رو استانداردسازی می‌کنیم؟ چه مشکلی ممکنه پیش بیاد اگه این کار رو نکنیم؟**
  * **فرق `scaler.fit_transform(X_train)` و `scaler.transform(X_test)` چیه؟ چرا برای `X_test` فقط `transform` رو استفاده می‌کنیم؟**
  * **با توجه به نمودار "Actual vs Predicted Prices"، آیا مدل خوبی داریم؟ نقاط باید روی چه خطی قرار بگیرند تا بگیم مدل عالیه؟**

---


سؤال خیلی خوبی پرسیدی: **چرا مدل LinearRegression رو استفاده کردیم؟ چرا "خطی"؟**
بذار با توضیح ساده و دقیق برات باز کنم:

---

## 🧠 مدل رگرسیون خطی یعنی چی؟

رگرسیون خطی سعی می‌کنه یک **رابطه‌ی خطی** بین ویژگی‌ها (X) و هدف (y) یاد بگیره.
یعنی فرض می‌کنه که خروجی، ترکیبی خطی از ورودی‌هاست:

$$
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$

---

## 🤔 چرا از مدل خطی استفاده می‌کنیم؟

### 1. **مدل پایه و قابل فهمه**

* خیلی سریع و ساده اجرا می‌شه
* راحت می‌تونیم وزن‌ها (ضرایب) رو تفسیر کنیم:
  مثلاً بگیم اگر `MedInc` یک واحد زیاد شه، `y` چقدر تغییر می‌کنه.

### 2. **برای شروع خیلی خوبه**

وقتی با داده‌ها آشنا نیستیم، مدل خطی مثل یه «خط مبنا» (baseline) عمل می‌کنه که بعداً می‌تونیم ببینیم مدل‌های پیچیده‌تر (مثل RandomForest یا Neural Net) چقدر بهتر از اون عمل می‌کنن.

### 3. **خیلی از روابط واقعی، تقریباً خطی هستن**

مثلاً در دیتاست California Housing، فرض رایجه که درآمد یا جمعیت ممکنه اثر خطی یا تقریباً خطی روی قیمت خانه بذاره.

---

## ⚠️ اما محدودیت‌هاش چیه؟

* فقط می‌تونه **رابطه‌های خطی** رو یاد بگیره.
* اگه رابطه‌ها غیرخطی باشن (مثلاً قیمت خونه با `Latitude` رفتار پیچیده‌ای داره)، عملکردش ضعیف می‌شه.
* به **نقاط پرت (outliers)** حساسه.
* فرض می‌کنه همه ویژگی‌ها **همبستگی خطی با هدف** دارن.

---

## 🔄 پس چرا استفاده کردیم؟

چون الان در مراحل ابتدایی کار هستیم و هدف:

* آشنایی با داده‌ست
* ساختن یک مدل baseline قابل ارزیابی
* ساده نگه داشتن محاسبات برای تحلیل بهتر

---

اگر بخوای بعدش بریم سراغ مدل‌های **غیرخطی** مثل:

* Polynomial Regression (برای یادگیری روابط منحنی‌وار)
* Random Forest
* Gradient Boosting
* Neural Network



---

---

دقیقاً! این سوال خیلی مهمیه و نشون میده که داری عمیق فکر می‌کنی. اینکه چرا برای این داده‌ها از **رگرسیون خطی (Linear Regression)** استفاده شده، دلایل مختلفی می‌تونه داشته باشه.

---

## چرا برای این دیتاست خاص (California Housing) از مدل خطی استفاده شد؟

در کدی که دیدیم، هیچ **تجزیه و تحلیل اولیه (Exploratory Data Analysis - EDA)** یا **استدلال صریحی** برای انتخاب مدل خطی وجود نداشت. در واقع، این کد بیشتر برای **معرفی نحوه استفاده از Scikit-learn** برای حل یک مسئله رگرسیون با داده‌های واقعی نوشته شده بود. مدل خطی ساده‌ترین نقطه شروع برای یک مسئله رگرسیون است و اغلب به عنوان **"مدل بیس‌لاین (Baseline Model)"** استفاده می‌شود. یعنی: "اول با ساده‌ترین مدل شروع می‌کنیم، ببینیم چطور عمل می‌کنه."

اما اگر بخواهیم دقیق‌تر باشیم و این انتخاب رو توجیه کنیم، چند دلیل احتمالی وجود داره:

### ۱. سادگی و قابلیت تفسیر (Simplicity and Interpretability)

* **مدل‌های خطی ساده هستند:** رگرسیون خطی از نظر مفهومی بسیار ساده است. فرض می‌کند که یک رابطه مستقیم و خطی بین ویژگی‌ها و متغیر هدف (قیمت خانه) وجود دارد.
* **قابلیت تفسیر بالا:** ضریب‌های (coefficients) مدل خطی به راحتی قابل تفسیر هستند. وقتی دیدیم که هر `feature_name` یک `coef` داره، اون ضریب به ما میگه که با افزایش یک واحد در اون ویژگی (مثلاً "MedInc" یا متوسط درآمد)، متغیر هدف (قیمت خانه) چقدر تغییر می‌کنه، با فرض ثابت بودن بقیه ویژگی‌ها. این شفافیت در فهم تأثیر هر عامل، در بسیاری از کاربردها بسیار ارزشمنده.

### ۲. عملکرد اولیه خوب (Reasonable Baseline Performance)

* **اولین قدم منطقی:** در پروژه‌های واقعی یادگیری ماشین، معمولاً با یک مدل ساده (مثل رگرسیون خطی) شروع می‌کنند تا یک **عملکرد پایه (Baseline Performance)** داشته باشند. اگر مدل خطی عملکرد قابل قبولی ارائه دهد، شاید نیازی به مدل‌های پیچیده‌تر نباشد.
* **بسیاری از روابط تقریباً خطی هستند:** با وجود اینکه روابط در دنیای واقعی پیچیده هستند، اما بسیاری از آن‌ها را می‌توان به صورت **تقریباً خطی** مدل‌سازی کرد، یا اینکه در یک بازه‌ی خاص، رفتار خطی از خود نشان می‌دهند.

### ۳. پیش‌فرض‌های اولیه (Initial Assumptions)

* بدون دانش قبلی از مجموعه داده، انتخاب یک مدل خطی یک **پیش‌فرض اولیه معقول** است. اگر بعداً مشاهده کنیم که مدل خطی عملکرد خوبی ندارد (مثلاً RMSE بالا باشد یا نمودار Actual vs Predicted نقاط زیادی خارج از خط مرجع داشته باشد)، آنگاه به سراغ مدل‌های پیچیده‌تر مانند:
    * **رگرسیون چندجمله‌ای (Polynomial Regression):** که قبلاً دیدیم و روابط غیرخطی را مدل می‌کند.
    * **رگرسیون درختی (Tree-based Regression):** مثل Decision Tree Regressor یا Random Forest Regressor.
    * **شبکه‌های عصبی (Neural Networks):** برای روابط بسیار پیچیده.

### ۴. ویژگی‌های عددی (Numerical Features)

* تمام ویژگی‌هایی که نام بردی (`MedInc`, `HouseAge`, `AveRooms` و ...) **عددی (Numerical)** هستند. رگرسیون خطی به خوبی با ویژگی‌های عددی کار می‌کند.

---

## از کجا فهمید خطی باشه؟

در این مثال خاص، کد **به صورت صریح به مدل نگفته که "خطی" باش**. بلکه ما یک کلاس به نام `LinearRegression` از کتابخانه `sklearn` را **انتخاب کرده‌ایم** و به مدل گفته‌ایم "از این نوع الگوریتم استفاده کن". انتخاب الگوریتم بر عهده برنامه‌نویس یا تحلیل‌گر داده است، نه خود مدل.

در واقع، این پروسه به این شکل است:

1.  **شناخت مسئله:** مسئله ما **رگرسیون** است (پیش‌بینی یک مقدار عددی پیوسته).
2.  **انتخاب اولیه مدل:** با توجه به سادگی و قابلیت تفسیر، `LinearRegression` به عنوان یک گزینه اولیه انتخاب می‌شود.
3.  **آموزش مدل:** `model.fit(X_train_scaled, y_train)` مدل خطی را روی داده‌ها آموزش می‌دهد.
4.  **ارزیابی عملکرد:** `rmse` و نمودار Actual vs Predicted را بررسی می‌کنیم.

---

## در یک سناریوی واقعی، چطور انتخاب می‌کنیم؟

در دنیای واقعی، انتخاب نوع مدل (خطی یا غیرخطی) یک فرآیند تکراری و مبتنی بر شواهد است:

1.  **تحلیل اکتشافی داده (EDA):** قبل از انتخاب هر مدلی، معمولاً نمودارهایی از هر ویژگی در برابر `y` (متغیر هدف) رسم می‌کنیم. اگر نقاط روی نمودار شبیه یک خط مستقیم باشند، مدل خطی مناسب است. اگر منحنی یا پراکندگی پیچیده‌تری وجود داشته باشد، به مدل‌های غیرخطی فکر می‌کنیم.
2.  **فرضیه‌های دامنه‌ای (Domain Knowledge):** گاهی اوقات، متخصصان آن حوزه می‌دانند که روابط بین متغیرها خطی هستند یا خیر. مثلاً می‌دانند که قیمت خانه در ابتدا با متراژ خطی بالا می‌رود اما بعد از یک حدی، افزایش متراژ تأثیر کمتری دارد که یک رابطه غیرخطی است.
3.  **تست و مقایسه مدل‌های مختلف:** اغلب چندین مدل (هم خطی و هم غیرخطی) را امتحان می‌کنیم و با استفاده از معیارهایی مانند RMSE یا $R^2$ (ضریب تعیین)، بهترین مدل را انتخاب می‌کنیم.
4.  **پیچیدگی در برابر عملکرد:** همیشه باید تعادلی بین پیچیدگی مدل و عملکرد آن برقرار کرد. یک مدل ساده‌تر که عملکرد تقریباً مشابه مدل پیچیده‌تر دارد، اغلب ترجیح داده می‌شود، زیرا تفسیر و نگهداری آن آسان‌تر است.

---

پس در این مورد خاص، انتخاب مدل خطی بیشتر به دلیل **معرفی یک گام پایه‌ای و رایج در کار با Scikit-learn** بوده است. اما در پروژه‌های واقعی، این انتخاب باید بر اساس **تحلیل داده‌ها، فرضیه‌ها، و مقایسه عملکرد مدل‌های مختلف** صورت بگیرد.

اگر سوالی در مورد **EDA** یا **انواع دیگر مدل‌ها** داری، بپرس تا بیشتر توضیح بدم!


---

## روش یادگیری در `LinearRegression` از sklearn

### 🔹 مدل رگرسیون خطی در `sklearn` با **حل تحلیلی (Closed-form solution)** وزن‌ها را محاسبه می‌کند، نه با گرادیان‌دی‌سنت!

---

### چرا؟

* رگرسیون خطی یک مسأله‌ی بهینه‌سازی **حداقل مربعات (Ordinary Least Squares - OLS)** است که:

$$
\min_w \sum_{i=1}^N (y_i - \hat{y}_i)^2 = \min_w \| y - Xw \|^2
$$

* این مسأله به صورت تحلیلی و دقیق قابل حل است با فرمول:

$$
w = (X^T X)^{-1} X^T y
$$

* یعنی می‌تونیم وزن‌ها رو با محاسبات جبری مستقیم به دست بیاریم.

---

### مزایای روش بسته:

* **سریع‌تر** و **دقیق‌تر** (تا وقتی که داده‌ها خیلی بزرگ یا ماتریس $X^T X$ معکوس‌پذیر باشه).
* نیازی به تنظیم پارامترهای بهینه‌سازی مثل نرخ یادگیری یا تعداد اپوک (epochs) نیست.

---

### چرا ما تعداد ایپاک نمی‌دیم؟

* چون **هیچ به‌روزرسانی تکراری لازم نیست**.
* وزن‌ها فقط یک بار با فرمول بسته محاسبه می‌شن و تمام.

---

### نکته‌ها:

* وقتی داده‌ها خیلی بزرگ باشن یا $X^T X$ عددی سخت بشه، ممکنه از روش‌های عددی مثل گرادیان‌دی‌سنت استفاده بشه (مثلاً در مدل‌های پیچیده‌تر یا کتابخانه‌های دیگر).
* اما در `sklearn.linear_model.LinearRegression`، روش پیش‌فرض **حل تحلیلی** است.

---

### جمع‌بندی:

| روش آموزش               | توضیح                                               | نیاز به ایپاک یا نرخ یادگیری؟ |
| ----------------------- | --------------------------------------------------- | ----------------------------- |
| حل تحلیلی (Closed-form) | با فرمول جبری مستقیم وزن‌ها را محاسبه می‌کند        | خیر                           |
| گرادیان دیسنت           | وزن‌ها را با گام‌های کوچک تکراری به‌روزرسانی می‌کند | بله                           |

---






