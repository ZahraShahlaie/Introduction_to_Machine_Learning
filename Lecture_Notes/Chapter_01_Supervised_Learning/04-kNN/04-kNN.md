**K-نزدیکترین همسایه (K-Nearest Neighbors - KNN): الگوریتمی غیرپارامتریک و نمونه‌محور**

در این جلسه، قصد داریم درباره یک الگوریتم جدید به نام K-نزدیکترین همسایه (KNN) صحبت کنیم. این الگوریتم هم در رگرسیون و هم در دسته‌بندی کاربرد دارد.

**روش‌های پارامتریک در مقابل روش‌های ناپارامتریک:**
در یادگیری ماشین، به طور کلی دو گروه از روش‌ها وجود دارد:
* **روش‌های پارامتریک (Parametric Methods):** این روش‌ها شامل پارامترهایی هستند که در طول فرآیند یادگیری (training) مدل، آن‌ها را "یاد می‌گیریم" یا "بهینه‌سازی می‌کنیم". مانند وزن‌ها (weights) در رگرسیون خطی یا پرسپترون. فرآیند تصمیم‌گیری (چه رگرسیون باشد چه دسته‌بندی) بر اساس این پارامترهای یادگرفته شده انجام می‌شود.
* **روش‌های ناپارامتریک (Non-Parametric Methods):** این روش‌ها، برخلاف روش‌های پارامتریک، دارای پارامترهایی مانند وزن‌ها نیستند که نیاز به یادگیری در مرحله آموزش داشته باشند.
    * **مزایای روش‌های ناپارامتریک:**
        * **سادگی و سرعت:** فرآیند یادگیری یا آموزش (training) ندارند. این باعث می‌شود که بتوانیم خیلی سریع‌تر کار با آن‌ها را شروع کنیم.
        * **کاربرد در داده‌های کم:** زمانی که حجم داده‌های آموزشی (training data) کم است و نمی‌توان پارامترهای زیادی را به طور قابل اعتماد یاد گرفت، روش‌های ناپارامتریک مفید هستند.
    * **مثال در آمار:** در آمار نیز آزمون‌هایی مانند T-test وجود دارند که فرض می‌کنند داده‌ها از یک توزیع خاص (مثلاً توزیع نرمال) آمده‌اند (این‌ها آزمون‌های پارامتریک هستند). در مقابل، آزمون‌های ناپارامتریک چنین فرضیاتی درباره توزیع داده‌ها ندارند و فرض می‌کنند که داده‌ها می‌توانند هر توزیعی داشته باشند. KNN یکی از جدی‌ترین الگوریتم‌های ناپارامتریک در یادگیری ماشین است که کاربرد فراوانی دارد.

**روش‌های مبتنی بر حافظه (Memory-Based) در مقابل روش‌های مبتنی بر نمونه (Instance-Based):**
همچنین می‌توان الگوریتم‌ها را از منظر دیگری دسته‌بندی کرد:
* **روش‌های مبتنی بر حافظه (Memory-Based Methods):** این روش‌ها نیاز دارند که "الگوهایی" را از داده‌های قبلی یا تجربیات پیشین به خاطر بسپارند. این خاطرات یا الگوها، همان پارامترهای مدل (مانند وزن‌ها) هستند که بر اساس بهترین عملکرد روی داده‌های آموزشی تنظیم شده‌اند.
* **روش‌های مبتنی بر نمونه (Instance-Based Methods):** این روش‌ها، نمونه‌محور هستند. یعنی در زمان تصمیم‌گیری، مدل به جای استفاده از پارامترهای یادگرفته شده، مستقیماً از "نمونه‌های آموزشی" (که برچسب‌های آن‌ها مشخص است) استفاده می‌کند.

**الگوریتم K-نزدیکترین همسایه (K-Nearest Neighbors - KNN):**
الگوریتم KNN بسیار شبیه به این شعر سعدی است: "تو اول بگو با کیان زیستی، من آنگه بگویم که تو کیستی." به عبارت دیگر، "دوستان نزدیکت را به من نشان بده تا بتوانم حدس بزنم تو کی هستی."

**نحوه عملکرد KNN:**
وقتی یک نمونه جدید داریم که می‌خواهیم وضعیت آن را مشخص کنیم (مثلاً سالم است یا سرطانی، قیمت خانه چقدر است، سگ است یا گربه، رقم تصویر 0 است یا 1، ایمیل اسپم است یا خیر، یا فیلمی را به کاربر پیشنهاد دهیم یا خیر)، KNN به صورت زیر عمل می‌کند:

1.  **انتخاب K:** ابتدا یک عدد صحیح K را مشخص می‌کنیم (مثلاً K=5).
2.  **یافتن نزدیکترین همسایه‌ها:** برای نمونه جدید (که به آن "نمونه تست" یا "نمونه ارزیابی" می‌گوییم)، K تا از "نزدیکترین" نمونه‌ها را از بین تمام داده‌های آموزشی (training data) پیدا می‌کنیم. نزدیکی بین نمونه‌ها بر اساس یک معیار فاصله (مثلاً فاصله اقلیدسی) تعیین می‌شود.
    * به عنوان مثال، برای توصیه فیلم، می‌توان نزدیکی دو کاربر را بر اساس درصد فیلم‌های مشترکی که دیده‌اند، تعیین کرد. اگر 90% فیلم‌های دیده شده توسط دو کاربر مشترک باشد، یعنی سلیقه فیلمی مشابهی دارند.
3.  **تصمیم‌گیری (برای دسته‌بندی):** بر اساس وضعیت این K همسایه نزدیک:
   
    * **رأی اکثریت (Majority Vote):** در مسائل دسته‌بندی، کلاس نمونه جدید بر اساس رأی اکثریت کلاس‌های K همسایه نزدیک تعیین می‌شود.
    * **مثال:** اگر 4 تا از 5 همسایه نزدیک نمونه جدید، "بیمار" باشند و فقط 1 همسایه "سالم" باشد، مدل پیش‌بینی می‌کند که نمونه جدید "بیمار" است.
    *
    * ![image](https://github.com/user-attachments/assets/91e439e7-c41f-4034-b2ae-965908be20eb)

    * **انتخاب K فرد:** معمولاً K را به صورت یک عدد فرد انتخاب می‌کنند تا از حالت تساوی در رأی‌گیری جلوگیری شود.

**خلاصه کلی KNN:**
KNN یک الگوریتم بسیار ساده و در عین حال قدرتمند است که پیچیدگی ریاضیاتی زیادی ندارد. تنها کاری که انجام می‌دهد این است که:
1.  برای هر نمونه جدیدی که می‌خواهیم پیش‌بینی روی آن انجام دهیم (بدون نیاز به فاز آموزش اولیه و پیدا کردن پارامترهایی مانند وزن‌ها).
2.  K تا از نزدیکترین نمونه‌ها را از مجموعه داده آموزشی (که تجربیات قبلی ما هستند و برچسب‌هایشان معلوم است) پیدا می‌کند.
3.  سپس، بر اساس کلاس‌هایی که در این K همسایه نزدیک بیشتر تکرار شده‌اند، کلاس نمونه جدید را پیش‌بینی می‌کند.
    * **مثال:** اگر از بین K همسایه، دو نمونه از کلاس 1، یک نمونه از کلاس 2، یک نمونه از کلاس 5 و یک نمونه از کلاس 4 باشند، نمونه جدید به کلاس 1 نسبت داده می‌شود، زیرا کلاس 1 بیشترین تکرار را در بین همسایگان نزدیک دارد.
  
**K-نزدیکترین همسایه (KNN): ویژگی‌ها و ملاحظات**

یکی از مزیت‌های مهم الگوریتم K-نزدیکترین همسایه (KNN) این است که می‌تواند مرزهای تصمیم‌گیری "غیرخطی" (non-linear) را ایجاد کند. این برخلاف الگوریتم‌هایی است که تا کنون بررسی شده‌اند، مانند رگرسیون خطی، پرسپترون، و رگرسیون لجستیک، که همگی با مدل‌های خطی کار می‌کنند.

**مرز تصمیم‌گیری غیرخطی در KNN:**
مدل‌های خطی نمی‌توانند به خوبی داده‌هایی که به صورت غیرخطی تفکیک‌پذیر هستند را جدا کنند، مگر با افزودن ویژگی‌های جدید. اما KNN می‌تواند به راحتی این کلاس‌ها را تفکیک کند. مرز تصمیم‌گیری در KNN می‌تواند بسیار پیچیده و غیرخطی باشد؛ این الگوریتم شکل هر یک از کلاس‌ها را در فضا شناسایی کرده و در اطراف آن‌ها مرزهایی را ترسیم می‌کند.

**اثر K بر مرز تصمیم‌گیری (Effect of K):**
تغییر مقدار K (تعداد همسایگان نزدیک) تأثیر قابل توجهی بر شکل مرز تصمیم‌گیری و عملکرد مدل دارد.

* **K=1 (1-Nearest Neighbor):**
    * در این حالت، کلاس یک نقطه جدید تنها بر اساس نزدیکترین همسایه آن تعیین می‌شود.
    * مرزهای تصمیم‌گیری بسیار "پیچیده" و "بریده‌بریده" می‌شوند.
    * این حالت به "Voronoi Tessellation" (تکه‌بندی ورونوی) منجر می‌شود. در تکه‌بندی ورونوی، فضا به سلول‌هایی تقسیم می‌شود که هر سلول شامل تمام نقاطی است که به یک نقطه آموزشی خاص (نسبت به سایر نقاط آموزشی) نزدیک‌ترند.
    * ![image](https://github.com/user-attachments/assets/57ded919-fb41-4243-bdd0-d53a42dbe6c6)

    * K=1 به شدت مستعد "بیش‌برازش" (Overfitting) و حساس به "نویز" (noise) یا داده‌های پرت (outliers) است. یک نقطه نویز می‌تواند بخش بزرگی از فضا را به اشتباه طبقه‌بندی کند.

* **افزایش K (K > 1):**
    * با افزایش K، مدل "هموارتر" (smoother) می‌شود و حساسیت آن به نویز و نقاط پرت کاهش می‌یابد.
    * با افزایش K، مدل از "بیش‌برازش" فاصله گرفته و به سمت "کم‌برازش" (Underfitting) میل می‌کند.
    * **کاهش واریانس و افزایش بایاس:** افزایش K باعث "کاهش واریانس" (variance) مدل می‌شود (مدل کمتر به نویزها حساس است)، اما در عین حال می‌تواند "بایاس" (bias) آن را "افزایش" دهد (مدل ممکن است پیچیدگی واقعی داده‌ها را نادیده بگیرد و دچار خطا شود). این پدیده به "موازنه بایاس-واریانس" (Bias-Variance Trade-off) معروف است.
        * **K خیلی کم (واریانس بالا):** مدل بیش‌برازش می‌کند، به نویز حساس است، و عملکرد روی داده‌های دیده نشده ممکن است ضعیف باشد.
        * **K خیلی زیاد (بایاس بالا):** مدل کم‌برازش می‌کند، پیچیدگی واقعی داده‌ها را نادیده می‌گیرد و ممکن است نتواند الگوهای مهم را تشخیص دهد.







![image](https://github.com/user-attachments/assets/36705d4f-2854-4a12-811b-5957332e1c29)

![image](https://github.com/user-attachments/assets/01c1452b-e38b-40c5-b7ca-b0c1de03ae8b)
![image](https://github.com/user-attachments/assets/ba8f13ef-2a46-46a6-a944-7b7562729084)


























































**ساختار اصلی یک یادگیرنده مبتنی بر نمونه (Instance-based Learner):**
برای ساخت یک یادگیرنده مبتنی بر نمونه مانند KNN، سه عنصر اصلی مورد نیاز است:
1.  **معیار فاصله (Distance Metric):** برای اندازه‌گیری نزدیکی بین نمونه‌ها.
    * **فاصله اقلیدسی (Euclidean Distance):** رایج‌ترین معیار فاصله. $d(x, x') = \sqrt[2]{||x-x'||_2^2} = \sqrt[2]{(x_1-x_1')^2 + \dots + (x_d-x_d')^2}$.
    * **فاصله اقلیدسی وزن‌دار (Weighted Euclidean Distance):** به ویژگی‌های خاص وزن می‌دهد. $d_w(x, x') = \sqrt[2]{w_1(x_1-x_1')^2 + \dots + w_d(x_d-x_d')^2}$.
    * **فاصله مینکوفسکی (Minkowski Distance):** یک حالت کلی از فواصل اقلیدسی و منهتن.
        * $d(x, x') = (\sum_{i=1}^{n}|x_i-x_i'|^p)^{1/p}$ برای $p \ge 1$.
        * وقتی $p=2$， فاصله اقلیدسی است.
        * این فاصله با نرم $L^p$ بردار $(x-x')$ برابر است.
    * **فاصله کسینوسی (Cosine Distance):** برای اندازه‌گیری زاویه بین بردارها استفاده می‌شود و به جای اندازه بردارها، به جهت آن‌ها اهمیت می‌دهد.
        * $d(x,x')=1 - \text{cosine similarity}(x,x')$.
        * $\text{cosine similarity}(x,x') = \frac{x \cdot x'}{||x||_2 ||x'||_2} = \frac{\sum_{i=1}^{d}x_i x_i'}{\sqrt{\sum_{i=1}^{d}x_i^2}\sqrt{\sum_{i=1}^{d}{x_i'^2}}}$.
2.  **تعداد همسایگان نزدیک (K):** تعداد نزدیکترین همسایگانی که برای تصمیم‌گیری در نظر گرفته می‌شوند.
3.  **تابع وزن‌دهی (اختیاری) (Weighting Function):** (اختیاری) برای اختصاص وزن‌های متفاوت به همسایگان بر اساس فاصله‌شان (همسایگان نزدیک‌تر وزن بیشتری دارند).
4.  **نحوه تعیین خروجی (Output Determination):** چگونه خروجی نهایی بر اساس همسایگان تعیین می‌شود.

**KNN برای رگرسیون (KNN Regression):**
در مسائل رگرسیون، KNN برای پیش‌بینی یک مقدار پیوسته استفاده می‌شود.
* **روش پیش‌بینی:** اگر $x'^{(1)}, \dots, x'^{(k)}$ K نزدیکترین همسایه به نمونه جدید $x$ باشند و $y'^{(1)}, \dots, y'^{(k)}$ مقادیر برچسب آن‌ها باشند، مقدار پیش‌بینی شده $\hat{y}$ به صورت میانگین مقادیر برچسب همسایگان محاسبه می‌شود:
    * $\hat{y} = \frac{1}{k} \sum_{j=1}^{k} y'^{(j)}$.
* **مشکلات KNN در رگرسیون:**
    * **ناپیوستگی (Discontinuities):** تابع تخمین زده شده توسط KNN می‌تواند دارای ناپیوستگی باشد.
    * **برازش نویز (Noise-fitting) در K=1:** مانند دسته‌بندی، در K=1، مدل به شدت به نویز حساس است و نویزهای داده را برازش می‌دهد.
    * **هموارسازی نویز و مشکلات دیگر در K > 1:** در K > 1، KNN نویز را هموار می‌کند، اما ممکن است مشکلات دیگری مانند "تخت کردن انتهای منحنی" (flats the ends) را ایجاد کند، به خصوص اگر K خیلی بزرگ باشد.
    * **مثال:** در یک مدل رگرسیون برای داده‌های سینوسی با نویز، با افزایش K، منحنی پیش‌بینی شده هموارتر می‌شود، اما اگر K خیلی بزرگ باشد، ممکن است از شکل واقعی منحنی منحرف شود و به سمت یک خط مستقیم میل کند.
