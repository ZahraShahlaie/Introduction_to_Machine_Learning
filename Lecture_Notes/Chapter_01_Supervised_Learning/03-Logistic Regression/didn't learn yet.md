



----

**2.6. رگرسیون لجستیک چندکلاسه (Multi-class Logistic Regression)**

* **مسئله:** زمانی که بیش از دو کلاس (K کلاس) وجود دارد و هر نمونه فقط به یک کلاس تعلق دارد.
* **تابع Softmax (Normalized Exponential):**
    * برای هر کلاس $k$، $\sigma_k(x; W)$ احتمال $P(y=k|x, W)$ را پیش‌بینی می‌کند.
    * مجموع احتمالات برای همه کلاس‌ها باید برابر با 1 باشد: $\sum_{k=1}^{K} P(y=k|x_0, W) = 1$.
    * **فرمول:** $\sigma_k(x, W) = P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_{j=1}^{K} \exp(w_j^T x)}$.
    * $W$ یک ماتریس وزن است که شامل بردارهای وزن $w_k$ برای هر کلاس است.
    * **ویژگی‌ها:**
        * مقادیر منفی را نیز به خوبی مدیریت می‌کند (به دلیل تابع نمایی).
        * مانند سیگموئید، تابع Softmax نیز هموار (smooth) و مشتق‌پذیر است.
        * مقدار بیشینه را به صورت هموار برجسته می‌کند (برخلاف تابع $\max(.)$ که گسسته و نامشتق‌پذیر است).
* **قانون پیش‌بینی:** برای یک ورودی جدید $x$， کلاسی را انتخاب می‌کنیم که $\sigma_k(x; W)$ آن را حداکثر کند:
    * $\alpha(x) = \arg \max_{k=1,...,K} \sigma_k(x; W)$
* **تابع هزینه (Cross-Entropy Loss برای Multi-class):**
    * تابع هزینه همچنان منفی لگاریتم درستنمایی است.
    * $J(W) = -\sum_{i=1}^{n} \sum_{k=1}^{K} y_k^{(i)} \log(\sigma_k(x^{(i)}; W))$
    * در اینجا $y_k^{(i)}$ نشان‌دهنده یک "کدگذاری یک از K" (1-of-K encoding) برای برچسب $y^{(i)}$ است (اگر نمونه $i$ به کلاس $k$ تعلق داشته باشد، $y_k^{(i)}=1$ و در غیر این صورت $0$).
* **گرادیان کاهشی برای Multi-class Logistic Regression:**
    * همچنان راه حل فرم بسته وجود ندارد.
    * قانون به‌روزرسانی گرادیان کاهشی: $w_j^{t+1} = w_j^t - \eta \nabla_{w_j} J(W^t)$
    * گرادیان نسبت به بردار وزن $w_j$ برای کلاس $j$:
        * $\nabla_{w_j} J(W) = \sum_{i=1}^{n} (\sigma_j(x^{(i)}; W) - y_j^{(i)}) x^{(i)}$
    * که در آن $w_j^t$ بردار وزن برای کلاس $j$ در تکرار $t$-ام است.

---

**3. جمع‌بندی رگرسیون لجستیک (Logistic Regression Summary)**

* **دسته‌بند خطی (Linear Classifier):** رگرسیون لجستیک یک دسته‌بند خطی است (مگر اینکه از ویژگی‌های غیرخطی - Feature Engineering - استفاده شود).
* **بهینه‌سازی با حداکثر درستنمایی (Maximum Likelihood Optimization):** مسئله بهینه‌سازی آن از طریق حداکثر درستنمایی به دست می‌آید.
* **عدم وجود فرم بسته (No Closed-Form Solution):** برای حل مسئله بهینه‌سازی آن راه حل فرم بسته وجود ندارد.
* **تابع هزینه محدب (Convex Cost Function):** تابع هزینه آن محدب است و می‌توان با استفاده از گرادیان صعودی (برای حداکثر کردن درستنمایی) یا گرادیان کاهشی (برای حداقل کردن تابع هزینه) به بهینه سراسری دست یافت.

---

**4. مطالعه بیشتر: دیدگاه احتمالی در دسته‌بندی (Probabilistic View in Classification)**

* **متغیرهای تصادفی:** در مسائل دسته‌بندی، هم ویژگی‌ها (مثل قد یک فرد) و هم برچسب کلاس (مثل اضافه وزن داشتن یا نداشتن) به عنوان متغیرهای تصادفی در نظر گرفته می‌شوند.
* **هدف:** مشاهده ویژگی‌ها ($x$) و پیدا کردن برچسب کلاس ($y$).

**4.1. تعاریف (Definitions)**

* **احتمال پسین (Posterior Probability):** احتمال یک برچسب کلاس $C_k$ با توجه به یک نمونه $x$.
    * $P(C_k|x)$
* **درستنمایی (Likelihood) یا احتمال شرطی کلاس (Class Conditional Probability):** تابع چگالی احتمال (PDF) بردار ویژگی $x$ برای نمونه‌های کلاس $C_k$.
    * $P(x|C_k)$
* **احتمال پیشین (Prior Probability):** احتمال وقوع کلاس $C_k$.
    * $P(C_k)$
* **تابع چگالی احتمال (PDF) بردار ویژگی $x$:**
    * $P(x) = \sum_{k=1}^{K} P(x|C_k) P(C_k)$

**4.2. دسته‌بندهای احتمالی (Probabilistic Classifiers)**

* **رویکردهای اصلی:**
    * **مدل‌های مولد (Generative Models):**
        * یادگیری توزیع احتمال توأم $P(x, y)$.
        * سپس از آن برای پیدا کردن $P(C_k|x)$ (با استفاده از قضیه بیز) استفاده می‌شود.
        * می‌توانند برای تولید جفت‌های $(x, y)$ نیز استفاده شوند.
    * **مدل‌های تمایزدهنده (Discriminative Models):**
        * مستقیماً توزیع احتمال شرطی $P(y|x)$ را تخمین می‌زنند.
        * **رگرسیون لجستیک یک رویکرد تمایزدهنده است.**
        * در رگرسیون لجستیک، ما مستقیماً می‌خواهیم برچسب کلاس را با $\sigma(w^T x)$ مشخص کنیم.

---
