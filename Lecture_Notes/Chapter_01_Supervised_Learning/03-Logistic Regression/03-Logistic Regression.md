✅ جزوه کامل و ساخت‌یافته‌ی جلسه چهارم با موضوع **رگرسیون لجستیک (Logistic Regression)** آماده شد.

📄 دانلود PDF جزوه:
[📥 دریافت فایل PDF](sandbox:/mnt/data/ML_Session4_Logistic_Regression.pdf)

ساختار جزوه:

* آموزش مفهومی رگرسیون لجستیک
* تابع سیگموید، مرز تصمیم، MLE، تابع هزینه
* مدل چندکلاسه (Softmax)
* دیدگاه احتمالاتی تفکیک‌گر vs. گشتاورزا

اگر تأیید کنی، مجموعه ۱۰ الی ۲۰ سؤال مصاحبه‌ای تخصصی هم براساس همین جلسه برات آماده می‌کنم. ✅ آیا ادامه بدم؟


با کمال میل! در ادامه، **متن کامل جزوه‌ی آموزشی جلسه ۴** با موضوع **رگرسیون لجستیک (Logistic Regression)** رو در قالبی آموزشی، منظم و یادگیری‌محور برات می‌نویسم—درست همون‌طور که در PDF بود، فقط اینجا به‌صورت متنی کامل برای کپی یا ویرایش راحت:

---

## 📘 جزوه آموزشی جلسه ۴

### موضوع: رگرسیون لجستیک (Logistic Regression)

---

### ۱. مقدمه: چرا رگرسیون لجستیک؟

رگرسیون لجستیک مدلی برای حل مسائل **طبقه‌بندی دودویی** است. برخلاف رگرسیون خطی که مقدار پیوسته پیش‌بینی می‌کند، رگرسیون لجستیک می‌خواهد خروجی بین ۰ و ۱ باشد (یعنی احتمال تعلق به کلاس مثبت). بنابراین به تابعی نیاز داریم که خروجی‌اش در بازه \[0,1] باشد.

---

### ۲. چرا نمی‌توان از رگرسیون خطی برای طبقه‌بندی استفاده کرد؟

رگرسیون خطی ممکن است خروجی‌هایی خارج از بازه \[0,1] تولید کند، مثل 1.2 یا -0.3 که نمی‌توانند احتمال باشند. در رگرسیون لجستیک، تابع خروجی به‌گونه‌ای طراحی شده که خروجی همیشه بین صفر و یک باشد.

---

### ۳. تابع سیگموید (Sigmoid Function)

تابع سیگموید به شکل زیر تعریف می‌شود:

$$
σ(z) = \frac{1}{1 + e^{-z}}
$$

این تابع برای نگاشت مقادیر به بازه \[0,1] مناسب است. وقتی ورودی مثبت باشد، خروجی نزدیک به ۱ و وقتی منفی باشد، خروجی نزدیک به صفر خواهد بود. بنابراین مقدار خروجی آن را می‌توان به‌عنوان احتمال تفسیر کرد.

---

### ۴. تابع تصمیم در رگرسیون لجستیک

در رگرسیون لجستیک، تابع تصمیم ترکیب تابع سیگموید با ترکیب خطی ویژگی‌هاست:

$$
h(x) = σ(w^T x)
$$

اگر $h(x) ≥ 0.5$، نمونه به کلاس ۱ تعلق دارد و در غیر این‌صورت به کلاس ۰. بنابراین مرز تصمیم همانند مدل‌های خطی یک هایپرپلین خواهد بود.

---

### ۵. سطح تصمیم (Decision Boundary)

سطح تصمیم همان مکانی است که احتمال تعلق به دو کلاس برابر است، یعنی:

$$
σ(w^T x) = 0.5 \Rightarrow w^T x = 0
$$

بنابراین مرز تصمیم در فضای ویژگی‌ها یک **مرز خطی** است. اگر از نگاشت ویژگی‌های غیرخطی استفاده کنیم، مرز تصمیم می‌تواند غیرخطی نیز باشد.

---

### ۶. برآورد بیشینه درست‌نمایی (Maximum Likelihood Estimation)

برای یادگیری وزن‌ها در رگرسیون لجستیک از روش MLE استفاده می‌شود. احتمال هر نمونه به صورت زیر تعریف می‌شود:

$$
P(y|x,w) = σ(w^T x)^y \cdot (1 - σ(w^T x))^{1-y}
$$

با گرفتن لگاریتم از احتمال کل داده‌ها، تابع **log-likelihood** به‌دست می‌آید که راحت‌تر برای بهینه‌سازی استفاده می‌شود.

---

### ۷. تابع هزینه و مشتق آن

تابع هزینه همان منفی log-likelihood است و به شکل زیر نوشته می‌شود:

$$
J(w) = -\sum_i [ y^{(i)} \log(σ(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - σ(w^T x^{(i)})) ]
$$

این تابع **محدب (Convex)** است و گرادیان آن نیز به‌سادگی قابل محاسبه است، بنابراین می‌توان از گرادیان دیسنت برای بهینه‌سازی آن استفاده کرد.

---

### ۸. مقایسه با گرادیان رگرسیون خطی

در رگرسیون خطی، گرادیان تابع هزینه به صورت:

$$
\nabla J(w) = \sum (w^T x - y) x
$$

اما در رگرسیون لجستیک، تفاوت فقط در جایگزینی $w^T x$ با تابع سیگموید است.

$$
\nabla J(w) = \sum (σ(w^T x) - y) x
$$

بنابراین ساختار مشابه است اما رفتار یادگیری متفاوت، چون لجستیک خروجی را به‌عنوان احتمال مدل می‌کند.

---

### ۹. رگرسیون لجستیک چندکلاسه (Softmax Regression)

برای طبقه‌بندی چندکلاسه، از تعمیم رگرسیون لجستیک به **Softmax Regression** استفاده می‌شود. تابع احتمال برای کلاس k به صورت زیر است:

$$
P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_j \exp(w_j^T x)}
$$

در این مدل برای هر کلاس یک بردار وزن مجزا $w_k$ در نظر گرفته می‌شود و مجموع خروجی‌ها برابر ۱ خواهد بود.

---

### ۱۰. دیدگاه احتمالاتی: Generative vs Discriminative

مدل‌های **Generative** مانند Naive Bayes ابتدا توزیع $P(x|y)$ و $P(y)$ را مدل می‌کنند و با استفاده از قانون بیز، احتمال $P(y|x)$ را محاسبه می‌کنند. اما مدل‌های **Discriminative** مانند Logistic Regression مستقیماً $P(y|x)$ را مدل می‌کنند. این باعث می‌شود دقت طبقه‌بندی بالاتر ولی قدرت تولید داده کمتر باشد.

---

اگر بخوای می‌تونم برات:

* ۱۰ یا ۲۰ سوال مصاحبه‌ای با پاسخ مفهومی از همین مبحث بنویسم
* یا ادامه بدم به جلسه ۵ (اگه فایلش رو برام بفرستی)

📩 آماده‌ای برای سوالات مصاحبه‌ای Logistic Regression؟


فوق‌العاده! 🧠 حالا براساس فایل جلسه سوم که موضوعش **رگرسیون لجستیک (Logistic Regression)** هست، برات یک **جزوه‌ی کامل، آموزشی و مفهومی** تهیه می‌کنم، با ساختاری دقیق مثل جلسات قبل، و هر بخشش شامل حدود ۸ تا ۱۰ خط توضیح یادگیری‌محور.

---

## 📘 جزوه آموزشی جلسه ۴

### موضوع: رگرسیون لجستیک (Logistic Regression)

برگرفته از اسلایدهای دکتر علی شریفی زارعی – دانشگاه صنعتی شریف

---

### 1. مقدمه: چرا رگرسیون لجستیک؟

رگرسیون لجستیک مدلی برای حل مسائل **طبقه‌بندی دودویی** است. مثلاً تشخیص اسپم بودن ایمیل، خوش‌خیم یا بدخیم بودن تومور، یا پیش‌بینی ترک کار. برخلاف رگرسیون خطی که خروجی را به‌صورت مقدار پیوسته پیش‌بینی می‌کند، در رگرسیون لجستیک می‌خواهیم خروجی بین ۰ و ۱ باشد (یعنی احتمال تعلق به کلاس مثبت). بنابراین به یک تابع نیاز داریم که خروجی‌اش در بازه \[0,1] باشد.

---

### 2. چرا نمی‌توان از رگرسیون خطی برای طبقه‌بندی استفاده کرد؟

اگر از رگرسیون خطی استفاده کنیم، ممکن است خروجی مدل (hθ(x)) مقادیری خارج از بازه \[0,1] تولید کند (مثلاً 1.5 یا -0.3). این مقادیر را نمی‌توان به‌عنوان احتمال تفسیر کرد. در رگرسیون لجستیک، ما تابعی می‌خواهیم که خروجی‌اش **همیشه بین صفر و یک** باشد تا بتواند احتمال تعلق به کلاس ۱ را بیان کند.

---

### 3. تابع سیگموید (Sigmoid Function)

برای رسیدن به خروجی بین ۰ و ۱، از تابع **سیگموید** استفاده می‌کنیم:

$$
σ(z) = \frac{1}{1 + e^{-z}}
$$

این تابع نرم، مشتق‌پذیر و به‌خوبی در محدوده \[0,1] تعریف شده است. وقتی ورودی بزرگ باشد، خروجی نزدیک به ۱ و وقتی ورودی خیلی منفی باشد، خروجی نزدیک به ۰ می‌شود. بنابراین خروجی را به‌صورت احتمال تفسیر می‌کنیم.

---

### 4. تابع تصمیم در رگرسیون لجستیک

در رگرسیون لجستیک، تابع تصمیم همان تابع سیگموید از ترکیب خطی ویژگی‌هاست:

$$
h(x) = σ(w^T x)
$$

برای پیش‌بینی کلاس، از آستانه ۰.۵ استفاده می‌شود:

* اگر $h(x) ≥ 0.5$ → کلاس ۱
* اگر $h(x) < 0.5$ → کلاس ۰
  این مرز تصمیم در فضای ویژگی‌ها یک **هایپرپلین خطی** خواهد بود.

---

### 5. سطح تصمیم (Decision Boundary)

سطح تصمیم محل‌هایی است که احتمال پیش‌بینی کلاس ۰ و ۱ برابر است. یعنی زمانی که:

$$
σ(w^T x) = 0.5
\Rightarrow w^T x = 0
$$

این یعنی مرز تصمیم در رگرسیون لجستیک **همانند مدل خطی، یک مرز خطی (یا غیرخطی با تبدیل ویژگی‌ها)** است. البته در صورت استفاده از ویژگی‌های غیرخطی، این مرز می‌تواند پیچیده‌تر شود.

---

### 6. برآورد بیشینه درست‌نمایی (Maximum Likelihood Estimation)

برای یادگیری پارامترهای w، از روش **بیشینه‌سازی تابع درست‌نمایی (MLE)** استفاده می‌کنیم. تابع احتمال به شکل زیر است:

$$
P(y|x,w) = σ(w^T x)^y (1 - σ(w^T x))^{1-y}
$$

و تابع لگاریتم درست‌نمایی:

$$
\log L = \sum_i y^{(i)} \log(σ(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - σ(w^T x^{(i)}))
$$

هدف یادگیری، بیشینه کردن این مقدار است.

---

### 7. تابع هزینه و مشتق آن

تابع هزینه رگرسیون لجستیک همان منفی لگاریتم درست‌نمایی است:

$$
J(w) = -\sum_i y^{(i)} \log(σ(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - σ(w^T x^{(i)}))
$$

این تابع **کووِکس (Convex)** است، بنابراین گرادیان دیسنت می‌تواند به‌راحتی مینیمم آن را پیدا کند. مشتق آن برای هر نمونه:

$$
\nabla J(w) = \sum_i (σ(w^T x^{(i)}) - y^{(i)}) x^{(i)}
$$

---

### 8. مقایسه با گرادیان رگرسیون خطی

در رگرسیون خطی گرادیان به صورت:

$$
\nabla J(w) = \sum (w^T x - y) x
$$

در حالی که در رگرسیون لجستیک، تنها تفاوت در وجود تابع غیرخطی سیگموید است. این تفاوت باعث می‌شود یادگیری بر اساس **احتمال تعلق به کلاس‌ها** صورت گیرد، نه فقط فاصله از مقدار عددی.

---

### 9. رگرسیون لجستیک چندکلاسه (Softmax Regression)

در مسائل چندکلاسه، خروجی فقط بین ۰ و ۱ نیست. در اینجا، به جای تابع سیگموید از **تابع softmax** استفاده می‌شود:

$$
P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_j \exp(w_j^T x)}
$$

برای هر کلاس یک بردار وزن مجزا تعریف می‌شود. مجموع خروجی‌های softmax برابر با ۱ است، بنابراین خروجی‌ها قابل تفسیر به‌عنوان احتمال هستند.

---

### 10. دیدگاه احتمالاتی: مدل‌های گشتاورزای (Generative) و تفکیک‌گر (Discriminative)

مدل‌های **Generative** مثل Naive Bayes ابتدا $P(x|C_k)$ و $P(C_k)$ را مدل می‌کنند و با قانون بیز $P(C_k|x)$ را می‌سازند. ولی مدل‌های **Discriminative** مثل Logistic Regression مستقیماً $P(C_k|x)$ را مدل می‌کنند. این باعث می‌شود مدل‌های تفکیک‌گر معمولاً در دقت طبقه‌بندی عملکرد بهتری داشته باشند، ولی توانایی تولید داده ندارند.

---

اگر تأیید کنی، در مرحله‌ی بعد:

✅ این جزوه رو به‌صورت فایل PDF حرفه‌ای و فارسی طراحی می‌کنم
✅ و مجموعه سؤالات مصاحبه‌ای مرتبط با رگرسیون لجستیک رو هم برات آماده می‌کنم (در دو سطح: مفهومی و تکنیکی)

🟢 ادامه بدم؟

البته! در ادامه بازنویسی و بازآرایی متن جزوه آموزشی شما با حفظ جامعیت و ساختار تخصصی، ولی با زبانی روان‌تر و روان‌تر ارائه شده است:

---

📘 **جزوه آموزشی درس یادگیری ماشین (CE 40717)**
**موضوع: رگرسیون لجستیک (Logistic Regression)**
**تاریخ: ۵ اکتبر ۲۰۲۴**
**مدرس: دکتر علی شریفی‌زارچی**
**تهیه و تنظیم: دانیال غریب (ویرایش و تکمیل)**

---

### ۱. مقدمه: چرا رگرسیون لجستیک؟

رگرسیون لجستیک (Logistic Regression - LR) یکی از الگوریتم‌های پایه و مهم یادگیری ماشین است که به‌خصوص برای مسائل طبقه‌بندی دودویی کاربرد دارد. بر خلاف رگرسیون خطی که خروجی پیوسته پیش‌بینی می‌کند، هدف رگرسیون لجستیک این است که احتمال تعلق نمونه به یک کلاس خاص (مثلاً کلاس مثبت) را به صورت عددی بین ۰ و ۱ برگرداند.

**مثال‌های رایج در طبقه‌بندی دودویی:**

* تشخیص ایمیل اسپم یا غیر اسپم
* تشخیص تراکنش‌های کلاهبرداری یا واقعی
* تشخیص تومور بدخیم یا خوش‌خیم

در این مسائل، معمولاً برچسب‌ها به صورت ۰ و ۱ تعریف می‌شوند؛
۰ نشان‌دهنده کلاس منفی (مثلاً تومور خوش‌خیم) و ۱ نشان‌دهنده کلاس مثبت (مثلاً تومور بدخیم).

---

### ۲. چرا رگرسیون خطی برای طبقه‌بندی مناسب نیست؟

اگر بخواهیم از رگرسیون خطی برای طبقه‌بندی استفاده کنیم، چند مشکل اساسی پیش می‌آید:

* **خروجی‌های خارج از بازه \[0,1]:** رگرسیون خطی ممکن است مقادیری کمتر از صفر یا بزرگ‌تر از یک پیش‌بینی کند که معنای احتمال ندارند.
* **حساسیت به داده‌های پرت:** داده‌های پرت می‌توانند تاثیر نامطلوبی بر مدل گذاشته و مرز تصمیم‌گیری نامناسبی ایجاد کنند.
  بنابراین، نیازمند تابعی هستیم که خروجی‌اش به طور طبیعی محدود به بازه \[0,1] بوده و به عنوان احتمال قابل تفسیر باشد.

---

### ۳. تابع سیگموید (Sigmoid Function)

تابع سیگموید یا لجستیک، به عنوان یک تابع فعال‌سازی در رگرسیون لجستیک به کار می‌رود تا خروجی مدل را در بازه‌ی \[0,1] نگه دارد. فرمول آن به صورت زیر است:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

ویژگی‌های مهم تابع سیگموید:

* خروجی همیشه بین ۰ و ۱ است.
* تابعی هموار و قابل مشتق است، که برای بهینه‌سازی ضروری است.
* خروجی را می‌توان به عنوان احتمال شرطی تعلق نمونه به کلاس مثبت تعبیر کرد:

$$
P(y=1|x,w) = \sigma(w^T x)
$$

و احتمال کلاس منفی:

$$
P(y=0|x,w) = 1 - \sigma(w^T x)
$$

مثلاً اگر خروجی برابر ۰.۷ باشد، یعنی ۷۰٪ احتمال تعلق نمونه به کلاس مثبت وجود دارد.

---

### ۴. تابع تصمیم در رگرسیون لجستیک

مدل رگرسیون لجستیک خروجی خود را با ترکیب خطی ویژگی‌ها و اعمال تابع سیگموید می‌سازد:

$$
h(x) = \sigma(w^T x)
$$

قاعده تصمیم‌گیری به شکل زیر است:

* اگر $h(x) \geq 0.5$ پیش‌بینی کلاس ۱
* اگر $h(x) < 0.5$ پیش‌بینی کلاس ۰

---

### ۵. مرز تصمیم (Decision Boundary)

مرز تصمیم ناحیه‌ای است که مدل بین دو کلاس مردد است؛ یعنی احتمال تعلق نمونه به هر دو کلاس برابر است. این نقطه در رگرسیون لجستیک هنگامی رخ می‌دهد که:

$$
\sigma(w^T x) = 0.5 \quad \Rightarrow \quad w^T x = 0
$$

معادله فوق یک **هایپرپلین خطی** در فضای ویژگی‌ها تعریف می‌کند، که نشان‌دهنده مرز تصمیم است.

**نکته:**
برای یادگیری مرزهای غیرخطی، می‌توان ویژگی‌های جدیدی شامل توابع درجه بالاتر (مثلاً توان‌های $x_1^2$، ترکیب‌های چند متغیره و...) به مدل افزود.

---

### ۶. برآورد بیشینه درست‌نمایی (MLE)

هدف اصلی رگرسیون لجستیک یافتن پارامتر $w$ است که بیشترین احتمال مشاهدات داده‌های آموزشی را ایجاد کند. برای یک نمونه $i$، احتمال مشاهده آن با توجه به مدل به صورت زیر است:

$$
P(y^{(i)}|x^{(i)},w) = \sigma(w^T x^{(i)})^{y^{(i)}} (1 - \sigma(w^T x^{(i)}))^{1 - y^{(i)}}
$$

تابع درست‌نمایی کل داده (فرض استقلال نمونه‌ها):

$$
L(w) = \prod_{i=1}^{n} P(y^{(i)}|x^{(i)},w)
$$

برای ساده‌سازی بهینه‌سازی، معمولاً از لگاریتم درست‌نمایی استفاده می‌کنیم:

$$
\log L(w) = \sum_{i=1}^n \left[ y^{(i)} \log \sigma(w^T x^{(i)}) + (1 - y^{(i)}) \log (1 - \sigma(w^T x^{(i)})) \right]
$$

---

### ۷. تابع هزینه و مشتق آن

تابع هزینه را به صورت منفی لگاریتم درست‌نمایی تعریف می‌کنیم تا مسئله را به صورت کمینه‌سازی درآوریم:

$$
J(w) = - \sum_{i=1}^n \left[ y^{(i)} \log \sigma(w^T x^{(i)}) + (1 - y^{(i)}) \log (1 - \sigma(w^T x^{(i)})) \right]
$$

این تابع که به نام **Binary Cross-Entropy Loss** شناخته می‌شود، ویژگی محدب بودن دارد و به این معنی است که الگوریتم‌هایی مانند گرادیان دیسنت تضمین همگرایی به بهینه سراسری را دارند.

---

### ۸. گرادیان دیسنت برای رگرسیون لجستیک

چون راه حل تحلیلی برای کمینه کردن $J(w)$ وجود ندارد، از الگوریتم‌های تکراری مانند گرادیان دیسنت استفاده می‌شود:

قاعده به‌روزرسانی پارامترها:

$$
w^{t+1} = w^t - \eta \nabla_w J(w^t)
$$

گرادیان تابع هزینه نسبت به $w$ برابر است با:

$$
\nabla_w J(w) = \sum_{i=1}^n (\sigma(w^T x^{(i)}) - y^{(i)}) x^{(i)}
$$

توجه کنید که این گرادیان شباهت زیادی به گرادیان رگرسیون خطی دارد، با این تفاوت که به جای $w^T x^{(i)}$ از $\sigma(w^T x^{(i)})$ استفاده می‌شود.

---

### ۹. رگرسیون لجستیک چندکلاسه (Softmax Regression)

وقتی تعداد کلاس‌ها بیش از دو تا باشد، از تعمیم رگرسیون لجستیک به نام Softmax Regression استفاده می‌کنیم.

تابع Softmax احتمال تعلق نمونه به کلاس $k$ را به صورت زیر مدل می‌کند:

$$
P(y=k|x,W) = \frac{e^{w_k^T x}}{\sum_{j=1}^K e^{w_j^T x}}
$$

که $w_k$ وزن مربوط به کلاس $k$ است.

ویژگی‌های Softmax:

* مجموع خروجی‌ها برای همه کلاس‌ها برابر ۱ است.
* تابعی هموار و قابل مشتق است.
* احتمال بیشترین کلاس را برجسته می‌کند.
* می‌تواند ورودی‌های منفی را به خوبی پردازش کند.

قاعده تصمیم‌گیری: انتخاب کلاسی که بیشترین احتمال را داشته باشد.

تابع هزینه Cross-Entropy چندکلاسه به شکل زیر است:

$$
J(W) = - \sum_{i=1}^n \sum_{k=1}^K y_k^{(i)} \log P(y=k|x^{(i)}, W)
$$

که $y_k^{(i)}$ مقدار باینری نشان‌دهنده تعلق نمونه $i$ به کلاس $k$ است.

گرادیان تابع هزینه نسبت به وزن کلاس $j$:

$$
\nabla_{w_j} J(W) = \sum_{i=1}^n \left( P(y=j|x^{(i)}, W) - y_j^{(i)} \right) x^{(i)}
$$

---

### ۱۰. دیدگاه احتمالاتی: مدل‌های مولد (Generative) و تمایزی (Discriminative)

مدل‌های یادگیری ماشین را می‌توان بر اساس نحوه مدل‌سازی توزیع‌های احتمالاتی به دو دسته تقسیم کرد:

* **مدل‌های مولد (Generative):**

  * توزیع توام $P(x,y)$ را یاد می‌گیرند.
  * برای هر کلاس توزیع $P(x|C_k)$ و احتمال پیشین $P(C_k)$ را برآورد می‌کنند.
  * سپس با قانون بیز احتمال پسین $P(C_k|x)$ را محاسبه و پیش‌بینی می‌کنند.
  * علاوه بر طبقه‌بندی، می‌توانند داده‌های جدید تولید کنند.
  * مثال: Naive Bayes.

* **مدل‌های تمایزی (Discriminative):**

  * مستقیماً توزیع شرطی $P(y|x)$ را مدل می‌کنند.
  * به جای مدل‌سازی داده، مرز تصمیم را می‌یابند.
  * تمرکز اصلی بر دقت طبقه‌بندی است و معمولاً داده جدید تولید نمی‌کنند.
  * مثال: رگرسیون لجستیک.

| ویژگی         | مدل مولد (Generative)       | مدل تمایزی (Discriminative) |                      |     |
| ------------- | --------------------------- | --------------------------- | -------------------- | --- |
| هدف یادگیری   | مدل‌سازی $P(x,y)$           | مدل‌سازی مستقیم (P(y        | x))                  |     |
| نحوه پیش‌بینی | استفاده از قانون بیز و (P(x | y), P(y))                   | پیش‌بینی مستقیم (P(y | x)) |
| کاربرد دیگر   | امکان تولید داده‌های جدید   | تمرکز روی طبقه‌بندی دقیق    |                      |     |
| مثال‌ها       | Naive Bayes                 | Logistic Regression, SVM    |                      |     |

---

### خلاصه رگرسیون لجستیک

* رگرسیون لجستیک یک طبقه‌بندی‌کننده خطی است (مگر آنکه نگاشت ویژگی‌های غیرخطی اعمال شود).
* بهینه‌سازی آن با روش بیشینه درست‌نمایی (MLE) انجام می‌شود.
* راه‌حل تحلیلی وجود ندارد، ولی تابع هزینه آن محدب است.
* این محدب بودن تضمین می‌کند که الگوریتم‌های گرادیان دیسنت به بهینه سراسری همگرا شوند.

---

**منابع:**

1. M. Soleymani Baghshah, "Machine learning." Lecture slides.
2. A. Ng, "Ml-005, lecture 6." Lecture slides.
3. C. M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.
4. S. Fidler, "Csc411." Lecture slides.
5. A. Ng and T. Ma, CS229 Lecture Notes. Updated June 11, 2023.
   ... (و سایر منابع)

---

اگر بخواهید می‌توانم کمک کنم قسمت‌های کد یا مثال‌های عملی را هم اضافه کنم، یا برای هر بخش سوال تمرینی طراحی کنم. نظر شما چیست؟
البته! این متن کامل‌تر و اصلاح‌شده جزوه آموزش رگرسیون لجستیک بدون ذکر منابع (سایت‌ها) و به صورت مرتب و روان آماده شده است:

---

📘 **جزوه آموزشی درس یادگیری ماشین (CE 40717)**
**موضوع: رگرسیون لجستیک (Logistic Regression)**
**تاریخ: ۵ اکتبر ۲۰۲۴**
**مدرس: دکتر علی شریفی‌زارچی**
**تهیه و تنظیم: دانیال غریب (با ویرایش و تکمیل)**

---

### ۱. مقدمه: چرا رگرسیون لجستیک؟

رگرسیون لجستیک یک الگوریتم یادگیری ماشین است که عمدتاً برای حل مسائل طبقه‌بندی، به‌ویژه طبقه‌بندی دودویی، کاربرد دارد. برخلاف رگرسیون خطی که یک مقدار پیوسته را پیش‌بینی می‌کند، رگرسیون لجستیک هدف دارد تا خروجی‌ای را در بازه‌ی \[0, 1] تولید کند که می‌تواند به عنوان احتمال تعلق یک نمونه به کلاس "مثبت" تفسیر شود.

**مثال‌هایی از مسائل طبقه‌بندی دودویی:**

* ایمیل: اسپم یا غیر اسپم؟
* تراکنش‌های آنلاین: کلاهبرداری یا واقعی؟
* تومور: بدخیم یا خوش‌خیم؟

در این مسائل، برچسب خروجی معمولاً به صورت 0 و 1 تعریف می‌شود:

* ۰: کلاس منفی (مثلاً تومور خوش‌خیم)
* ۱: کلاس مثبت (مثلاً تومور بدخیم)

---

### ۲. چرا نمی‌توان از رگرسیون خطی برای طبقه‌بندی استفاده کرد؟

رگرسیون خطی برای مسائل طبقه‌بندی مناسب نیست زیرا:

* خروجی‌های ممکن است خارج از بازه \[0,1] باشند؛ مثلاً 1.2 یا -0.3 که نمی‌توان آن‌ها را به عنوان احتمال تفسیر کرد.
* حساسیت زیاد به داده‌های پرت وجود دارد که می‌تواند مرز تصمیم‌گیری را به شدت تغییر دهد و باعث عملکرد نامناسب شود.

بنابراین، به تابعی نیاز داریم که خروجی آن به طور ذاتی در بازه‌ی \[0,1] قرار گیرد و قابل تفسیر به عنوان احتمال باشد.

---

### ۳. تابع سیگموید (Sigmoid Function)

برای اطمینان از خروجی در بازه \[0,1] از تابع سیگموید استفاده می‌کنیم که به صورت زیر تعریف می‌شود:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

ویژگی‌های تابع سیگموید:

* خروجی در بازه \[0,1] است.
* تابعی هموار و مشتق‌پذیر است، که برای بهینه‌سازی لازم است.
* خروجی آن را می‌توان به عنوان احتمال شرطی \$P(y=1|x,w)\$ تفسیر کرد، یعنی احتمال تعلق نمونه به کلاس مثبت.

مثلاً اگر خروجی 0.7 باشد، یعنی ۷۰ درصد احتمال تعلق نمونه به کلاس مثبت وجود دارد.

---

### ۴. تابع تصمیم در رگرسیون لجستیک

در رگرسیون لجستیک، تابع تصمیم به صورت ترکیب خطی ویژگی‌ها و تابع سیگموید تعریف می‌شود:

$$
h(x) = \sigma(w^T x)
$$

که \$x\$ بردار ویژگی‌ها و \$w\$ بردار وزن‌ها است.

قاعده تصمیم‌گیری:

* اگر \$h(x) \geq 0.5\$ پیش‌بینی کلاس ۱ (مثبت) است.
* اگر \$h(x) < 0.5\$ پیش‌بینی کلاس ۰ (منفی) است.

---

### ۵. سطح تصمیم (Decision Boundary)

مرز تصمیم ناحیه‌ای است که مدل بین دو کلاس به صورت مساوی تردید دارد، یعنی احتمال تعلق به هر دو کلاس برابر است:

$$
\sigma(w^T x) = 0.5 \implies w^T x = 0
$$

این معادله یک هایپرپلین (صفحه خطی) در فضای ویژگی‌ها است که مرز تصمیم را تشکیل می‌دهد.

* ابعاد هایپرپلین همیشه یک بعد کمتر از فضای ویژگی‌ها است.
* برای مرزهای تصمیم غیرخطی، می‌توان با افزودن ترم‌های مرتبه بالاتر به ویژگی‌ها (مثلاً مربع و ضرب ویژگی‌ها) از نگاشت ویژگی استفاده کرد.

---

### ۶. برآورد بیشینه درست‌نمایی (Maximum Likelihood Estimation)

هدف یافتن بردار وزن \$w\$ است که احتمال پیش‌بینی صحیح داده‌های آموزشی را حداکثر کند.

احتمال هر نمونه در طبقه‌بندی دودویی به شکل زیر است:

$$
P(y^{(i)}|x^{(i)},w) = \sigma(w^T x^{(i)})^{y^{(i)}} (1 - \sigma(w^T x^{(i)}))^{1 - y^{(i)}}
$$

تابع درست‌نمایی کل داده‌ها (با فرض استقلال نمونه‌ها):

$$
L(w) = \prod_{i=1}^{n} P(y^{(i)}|x^{(i)},w)
$$

برای ساده‌تر کردن محاسبات، لگاریتم تابع درست‌نمایی گرفته می‌شود:

$$
\log L(w) = \sum_{i=1}^n \left[ y^{(i)} \log(\sigma(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(w^T x^{(i)})) \right]
$$

---

### ۷. تابع هزینه و مشتق آن

تابع هزینه به عنوان منفی لگاریتم درست‌نمایی تعریف می‌شود (برای کمینه‌سازی):

$$
J(w) = - \sum_{i=1}^n \left[ y^{(i)} \log(\sigma(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(w^T x^{(i)})) \right]
$$

این تابع که به نام **Binary Cross-Entropy Loss** شناخته می‌شود، محدب است که تضمین‌کننده همگرایی به بهینه سراسری توسط الگوریتم‌های گرادیان دیسنت است.

---

### ۸. گرادیان دیسنت برای رگرسیون لجستیک

از آنجا که برای بهینه‌سازی \$J(w)\$ راه‌حل تحلیلی وجود ندارد، از گرادیان دیسنت استفاده می‌کنیم.

قاعده به‌روزرسانی وزن‌ها:

$$
w^{t+1} = w^t - \eta \nabla_w J(w^t)
$$

که \$\eta\$ نرخ یادگیری است.

گرادیان تابع هزینه:

$$
\nabla_w J(w) = \sum_{i=1}^n (\sigma(w^T x^{(i)}) - y^{(i)}) x^{(i)}
$$

این گرادیان مشابه گرادیان رگرسیون خطی است، با این تفاوت که در رگرسیون لجستیک از خروجی تابع سیگموید به جای مقدار خطی استفاده می‌شود.

---

### ۹. رگرسیون لجستیک چندکلاسه (Softmax Regression)

وقتی تعداد کلاس‌ها بیشتر از دو باشد (کلاس‌های چندگانه)، از تعمیم رگرسیون لجستیک به نام Softmax Regression استفاده می‌شود.

تابع Softmax برای هر کلاس \$k\$ تعریف می‌شود:

$$
P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_{j=1}^K \exp(w_j^T x)}
$$

که \$w\_k\$ وزن‌های مربوط به کلاس \$k\$ است.

ویژگی‌ها:

* مجموع احتمالات برای همه کلاس‌ها برابر ۱ است.
* هموار و مشتق‌پذیر است.
* خروجی‌ها احتمال تعلق به هر کلاس را به صورت نرم و پیوسته ارائه می‌کنند.

قاعده تصمیم‌گیری: کلاس با بیشترین احتمال انتخاب می‌شود.

تابع هزینه (Cross-Entropy چندکلاسه):

$$
J(W) = - \sum_{i=1}^n \sum_{k=1}^K y_k^{(i)} \log P(y=k|x^{(i)})
$$

که \$y\_k^{(i)}\$ برابر ۱ است اگر نمونه \$i\$ به کلاس \$k\$ تعلق داشته باشد و صفر در غیر این صورت.

گرادیان تابع هزینه برای کلاس \$j\$:

$$
\nabla_{w_j} J(W) = \sum_{i=1}^n (P(y=j|x^{(i)}) - y_j^{(i)}) x^{(i)}
$$

---

### ۱۰. دیدگاه احتمالاتی در طبقه‌بندی: مدل‌های مولد و تمایزی

در طبقه‌بندی می‌توان دو نوع مدل کلی داشت:

**الف) مدل‌های مولد (Generative):**

* یادگیری توزیع توام \$P(x,y)\$
* برآورد توزیع شرطی \$P(x|C\_k)\$ و احتمال پیشین \$P(C\_k)\$
* استفاده از قانون بیز برای محاسبه احتمال پسین \$P(C\_k|x)\$
* امکان تولید داده جدید
* مثال: Naive Bayes

**ب) مدل‌های تمایزی (Discriminative):**

* یادگیری مستقیم توزیع شرطی \$P(y|x)\$
* یادگیری مرز تصمیم
* تمرکز روی دقت طبقه‌بندی
* مثال: رگرسیون لجستیک، SVM

---

### خلاصه رگرسیون لجستیک

* رگرسیون لجستیک یک طبقه‌بندی‌کننده خطی است (مگر با نگاشت ویژگی‌های غیرخطی).
* بهینه‌سازی آن از طریق بیشینه‌سازی درست‌نمایی صورت می‌گیرد.
* هیچ راه‌حل تحلیلی بسته‌ای ندارد، اما تابع هزینه محدب است و بهینه‌سازی گرادیان دیسنت قابل اعتماد است.

---

اگر لازم باشد می‌توانم به شکل کد، فرمول‌ها یا مثال‌های بیشتر هم اضافه کنم. آیا می‌خواهید؟

----
حتما! در ادامه فرمول‌های مهم متن را به صورت درست و با قالب‌بندی ریاضی استاندارد (LaTeX) آورده‌ام:

---

### تابع سیگموید (Sigmoid Function)

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

---

### تابع تصمیم در رگرسیون لجستیک

$$
h(x) = \sigma(w^T x)
$$

---

### مرز تصمیم (Decision Boundary)

$$
\sigma(w^T x) = 0.5 \implies w^T x = 0
$$

---

### احتمال نمونه در طبقه‌بندی دودویی

$$
P(y^{(i)} | x^{(i)}, w) = \sigma(w^T x^{(i)})^{y^{(i)}} \cdot \big(1 - \sigma(w^T x^{(i)})\big)^{1 - y^{(i)}}
$$

---

### تابع درست‌نمایی (Likelihood Function)

$$
L(w) = \prod_{i=1}^n P(y^{(i)} | x^{(i)}, w)
$$

---

### لگاریتم تابع درست‌نمایی (Log-Likelihood)

$$
\log L(w) = \sum_{i=1}^n \left[ y^{(i)} \log \big( \sigma(w^T x^{(i)}) \big) + (1 - y^{(i)}) \log \big( 1 - \sigma(w^T x^{(i)}) \big) \right]
$$

---

### تابع هزینه (Negative Log-Likelihood)

$$
J(w) = - \sum_{i=1}^n \left[ y^{(i)} \log \big( \sigma(w^T x^{(i)}) \big) + (1 - y^{(i)}) \log \big( 1 - \sigma(w^T x^{(i)}) \big) \right]
$$

---

### قاعده به‌روزرسانی وزن‌ها در گرادیان دیسنت

$$
w^{t+1} = w^t - \eta \nabla_w J(w^t)
$$

---

### گرادیان تابع هزینه نسبت به $w$

$$
\nabla_w J(w) = \sum_{i=1}^n \left( \sigma(w^T x^{(i)}) - y^{(i)} \right) x^{(i)}
$$

---

### تابع Softmax برای کلاس $k$

$$
P(y = k | x) = \frac{\exp(w_k^T x)}{\sum_{j=1}^K \exp(w_j^T x)}
$$

---

### تابع هزینه (Cross-Entropy) برای Softmax Regression

$$
J(W) = - \sum_{i=1}^n \sum_{k=1}^K y_k^{(i)} \log P(y = k | x^{(i)})
$$

---

### گرادیان تابع هزینه نسبت به وزن کلاس $j$

$$
\nabla_{w_j} J(W) = \sum_{i=1}^n \left( P(y = j | x^{(i)}) - y_j^{(i)} \right) x^{(i)}
$$

---

اگر بخواهید، می‌توانم فرمول‌ها را داخل متن اصلی هم جایگزین کنم یا هر فرمولی که مد نظرتان است را با توضیح کامل‌تر ارائه دهم.


--

