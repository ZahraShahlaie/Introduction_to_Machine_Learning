
### ۱. مقدمه: چرا رگرسیون لجستیک؟

رگرسیون لجستیک یک الگوریتم یادگیری ماشین است که عمدتاً برای حل مسائل طبقه‌بندی، به‌ویژه طبقه‌بندی دودویی، کاربرد دارد. برخلاف رگرسیون خطی که یک مقدار پیوسته را پیش‌بینی می‌کند، رگرسیون لجستیک هدف دارد تا خروجی‌ای را در بازه‌ی \[0, 1] تولید کند که می‌تواند به عنوان احتمال تعلق یک نمونه به کلاس "مثبت" تفسیر شود.

**مثال‌هایی از مسائل طبقه‌بندی دودویی:**

* ایمیل: اسپم یا غیر اسپم؟
* تراکنش‌های آنلاین: کلاهبرداری یا واقعی؟
* تومور: بدخیم یا خوش‌خیم؟

در این مسائل، برچسب خروجی معمولاً به صورت 0 و 1 تعریف می‌شود:

* ۰: کلاس منفی (مثلاً تومور خوش‌خیم)
* ۱: کلاس مثبت (مثلاً تومور بدخیم)

---

### ۲. چرا نمی‌توان از رگرسیون خطی برای طبقه‌بندی استفاده کرد؟

رگرسیون خطی برای مسائل طبقه‌بندی مناسب نیست زیرا:

* خروجی‌های ممکن است خارج از بازه \[0,1] باشند؛ مثلاً 1.2 یا -0.3 که نمی‌توان آن‌ها را به عنوان احتمال تفسیر کرد.
* حساسیت زیاد به داده‌های پرت وجود دارد که می‌تواند مرز تصمیم‌گیری را به شدت تغییر دهد و باعث عملکرد نامناسب شود.

بنابراین، به تابعی نیاز داریم که خروجی آن به طور ذاتی در بازه‌ی \[0,1] قرار گیرد و قابل تفسیر به عنوان احتمال باشد.

---

### ۳. تابع سیگموید (Sigmoid Function)

برای اطمینان از خروجی در بازه \[0,1] از تابع سیگموید استفاده می‌کنیم که به صورت زیر تعریف می‌شود:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

ویژگی‌های تابع سیگموید:

* خروجی در بازه \[0,1] است.
* تابعی هموار و مشتق‌پذیر است، که برای بهینه‌سازی لازم است.
* خروجی آن را می‌توان به عنوان احتمال شرطی \$P(y=1|x,w)\$ تفسیر کرد، یعنی احتمال تعلق نمونه به کلاس مثبت.

مثلاً اگر خروجی 0.7 باشد، یعنی ۷۰ درصد احتمال تعلق نمونه به کلاس مثبت وجود دارد.

---

### ۴. تابع تصمیم در رگرسیون لجستیک

در رگرسیون لجستیک، تابع تصمیم به صورت ترکیب خطی ویژگی‌ها و تابع سیگموید تعریف می‌شود:

$$
h(x) = \sigma(w^T x)
$$

که \$x\$ بردار ویژگی‌ها و \$w\$ بردار وزن‌ها است.

قاعده تصمیم‌گیری:

* اگر \$h(x) \geq 0.5\$ پیش‌بینی کلاس ۱ (مثبت) است.
* اگر \$h(x) < 0.5\$ پیش‌بینی کلاس ۰ (منفی) است.

---

### ۵. سطح تصمیم (Decision Boundary)

مرز تصمیم ناحیه‌ای است که مدل بین دو کلاس به صورت مساوی تردید دارد، یعنی احتمال تعلق به هر دو کلاس برابر است:

$$
\sigma(w^T x) = 0.5 \implies w^T x = 0
$$

این معادله یک هایپرپلین (صفحه خطی) در فضای ویژگی‌ها است که مرز تصمیم را تشکیل می‌دهد.

* ابعاد هایپرپلین همیشه یک بعد کمتر از فضای ویژگی‌ها است.
* برای مرزهای تصمیم غیرخطی، می‌توان با افزودن ترم‌های مرتبه بالاتر به ویژگی‌ها (مثلاً مربع و ضرب ویژگی‌ها) از نگاشت ویژگی استفاده کرد.

---

### ۶. برآورد بیشینه درست‌نمایی (Maximum Likelihood Estimation)

هدف یافتن بردار وزن \$w\$ است که احتمال پیش‌بینی صحیح داده‌های آموزشی را حداکثر کند.

احتمال هر نمونه در طبقه‌بندی دودویی به شکل زیر است:

$$
P(y^{(i)}|x^{(i)},w) = \sigma(w^T x^{(i)})^{y^{(i)}} (1 - \sigma(w^T x^{(i)}))^{1 - y^{(i)}}
$$

تابع درست‌نمایی کل داده‌ها (با فرض استقلال نمونه‌ها):

$$
L(w) = \prod_{i=1}^{n} P(y^{(i)}|x^{(i)},w)
$$

برای ساده‌تر کردن محاسبات، لگاریتم تابع درست‌نمایی گرفته می‌شود:

$$
\log L(w) = \sum_{i=1}^n \left[ y^{(i)} \log(\sigma(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(w^T x^{(i)})) \right]
$$

---

### ۷. تابع هزینه و مشتق آن

تابع هزینه به عنوان منفی لگاریتم درست‌نمایی تعریف می‌شود (برای کمینه‌سازی):

$$
J(w) = - \sum_{i=1}^n \left[ y^{(i)} \log(\sigma(w^T x^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(w^T x^{(i)})) \right]
$$

این تابع که به نام **Binary Cross-Entropy Loss** شناخته می‌شود، محدب است که تضمین‌کننده همگرایی به بهینه سراسری توسط الگوریتم‌های گرادیان دیسنت است.

---

### ۸. گرادیان دیسنت برای رگرسیون لجستیک

از آنجا که برای بهینه‌سازی \$J(w)\$ راه‌حل تحلیلی وجود ندارد، از گرادیان دیسنت استفاده می‌کنیم.

قاعده به‌روزرسانی وزن‌ها:

$$
w^{t+1} = w^t - \eta \nabla_w J(w^t)
$$

که \$\eta\$ نرخ یادگیری است.

گرادیان تابع هزینه:

$$
\nabla_w J(w) = \sum_{i=1}^n (\sigma(w^T x^{(i)}) - y^{(i)}) x^{(i)}
$$

این گرادیان مشابه گرادیان رگرسیون خطی است، با این تفاوت که در رگرسیون لجستیک از خروجی تابع سیگموید به جای مقدار خطی استفاده می‌شود.

---

### ۹. رگرسیون لجستیک چندکلاسه (Softmax Regression)

وقتی تعداد کلاس‌ها بیشتر از دو باشد (کلاس‌های چندگانه)، از تعمیم رگرسیون لجستیک به نام Softmax Regression استفاده می‌شود.

تابع Softmax برای هر کلاس \$k\$ تعریف می‌شود:

$$
P(y=k|x) = \frac{\exp(w_k^T x)}{\sum_{j=1}^K \exp(w_j^T x)}
$$

که \$w\_k\$ وزن‌های مربوط به کلاس \$k\$ است.

ویژگی‌ها:

* مجموع احتمالات برای همه کلاس‌ها برابر ۱ است.
* هموار و مشتق‌پذیر است.
* خروجی‌ها احتمال تعلق به هر کلاس را به صورت نرم و پیوسته ارائه می‌کنند.

قاعده تصمیم‌گیری: کلاس با بیشترین احتمال انتخاب می‌شود.

تابع هزینه (Cross-Entropy چندکلاسه):

$$
J(W) = - \sum_{i=1}^n \sum_{k=1}^K y_k^{(i)} \log P(y=k|x^{(i)})
$$

که \$y\_k^{(i)}\$ برابر ۱ است اگر نمونه \$i\$ به کلاس \$k\$ تعلق داشته باشد و صفر در غیر این صورت.

گرادیان تابع هزینه برای کلاس \$j\$:

$$
\nabla_{w_j} J(W) = \sum_{i=1}^n (P(y=j|x^{(i)}) - y_j^{(i)}) x^{(i)}
$$

---

### ۱۰. دیدگاه احتمالاتی در طبقه‌بندی: مدل‌های مولد و تمایزی

در طبقه‌بندی می‌توان دو نوع مدل کلی داشت:

**الف) مدل‌های مولد (Generative):**

* یادگیری توزیع توام \$P(x,y)\$
* برآورد توزیع شرطی \$P(x|C\_k)\$ و احتمال پیشین \$P(C\_k)\$
* استفاده از قانون بیز برای محاسبه احتمال پسین \$P(C\_k|x)\$
* امکان تولید داده جدید
* مثال: Naive Bayes

**ب) مدل‌های تمایزی (Discriminative):**

* یادگیری مستقیم توزیع شرطی \$P(y|x)\$
* یادگیری مرز تصمیم
* تمرکز روی دقت طبقه‌بندی
* مثال: رگرسیون لجستیک، SVM

---

### خلاصه رگرسیون لجستیک

* رگرسیون لجستیک یک طبقه‌بندی‌کننده خطی است (مگر با نگاشت ویژگی‌های غیرخطی).
* بهینه‌سازی آن از طریق بیشینه‌سازی درست‌نمایی صورت می‌گیرد.
* هیچ راه‌حل تحلیلی بسته‌ای ندارد، اما تابع هزینه محدب است و بهینه‌سازی گرادیان دیسنت قابل اعتماد است.

---




### تابع سیگموید (Sigmoid Function)

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

---

### تابع تصمیم در رگرسیون لجستیک

$$
h(x) = \sigma(w^T x)
$$

---

### مرز تصمیم (Decision Boundary)

$$
\sigma(w^T x) = 0.5 \implies w^T x = 0
$$

---

### احتمال نمونه در طبقه‌بندی دودویی

$$
P(y^{(i)} | x^{(i)}, w) = \sigma(w^T x^{(i)})^{y^{(i)}} \cdot \big(1 - \sigma(w^T x^{(i)})\big)^{1 - y^{(i)}}
$$

---

### تابع درست‌نمایی (Likelihood Function)

$$
L(w) = \prod_{i=1}^n P(y^{(i)} | x^{(i)}, w)
$$

---

### لگاریتم تابع درست‌نمایی (Log-Likelihood)

$$
\log L(w) = \sum_{i=1}^n \left[ y^{(i)} \log \big( \sigma(w^T x^{(i)}) \big) + (1 - y^{(i)}) \log \big( 1 - \sigma(w^T x^{(i)}) \big) \right]
$$

---

### تابع هزینه (Negative Log-Likelihood)

$$
J(w) = - \sum_{i=1}^n \left[ y^{(i)} \log \big( \sigma(w^T x^{(i)}) \big) + (1 - y^{(i)}) \log \big( 1 - \sigma(w^T x^{(i)}) \big) \right]
$$

---

### قاعده به‌روزرسانی وزن‌ها در گرادیان دیسنت

$$
w^{t+1} = w^t - \eta \nabla_w J(w^t)
$$

---

### گرادیان تابع هزینه نسبت به $w$

$$
\nabla_w J(w) = \sum_{i=1}^n \left( \sigma(w^T x^{(i)}) - y^{(i)} \right) x^{(i)}
$$

---

### تابع Softmax برای کلاس $k$

$$
P(y = k | x) = \frac{\exp(w_k^T x)}{\sum_{j=1}^K \exp(w_j^T x)}
$$

---

### تابع هزینه (Cross-Entropy) برای Softmax Regression

$$
J(W) = - \sum_{i=1}^n \sum_{k=1}^K y_k^{(i)} \log P(y = k | x^{(i)})
$$

---

### گرادیان تابع هزینه نسبت به وزن کلاس $j$

$$
\nabla_{w_j} J(W) = \sum_{i=1}^n \left( P(y = j | x^{(i)}) - y_j^{(i)} \right) x^{(i)}
$$



